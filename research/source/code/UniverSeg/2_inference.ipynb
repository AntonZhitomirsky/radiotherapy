{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# define argparser for anatomy, and axis\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Inference for UniverSeg')\n",
    "\n",
    "parser.add_argument('anatomy', type=str, help='The anatomy to infer on')\n",
    "parser.add_argument('axis', type=str, help='The axis to infer on (0,1,2)')\n",
    "\n",
    "args = parser.parse_args(\n",
    "    # ['Anorectum', '0']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy = args.anatomy # 'Anorectum'\n",
    "support_size = 80\n",
    "axis = int(args.axis) # 0\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infering on Anorectum along axis 0\n",
      "Using support size 80 and batch size 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Infering on {anatomy} along axis {axis}')\n",
    "print(f'Using support size {support_size} and batch size {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "class UniverSegDataSet(Dataset):\n",
    "    def __init__(self, support_size, anatomy, axis):\n",
    "\n",
    "        self.medsam_gts = os.path.join(os.environ.get('MedSAM_preprocessed_lowres'), 'gts', anatomy, f'axis{axis}')\n",
    "        self.medsam_imgs = os.path.join(os.environ.get('MedSAM_preprocessed_lowres'), 'imgs', f'axis{axis}')\n",
    "\n",
    "        self.support_size = support_size\n",
    "        self.anatomy = anatomy\n",
    "        self.axis = axis\n",
    "\n",
    "        self.gts_samples = [f for f in os.listdir(self.medsam_gts) if f.endswith('.npy')]\n",
    "        random.shuffle(self.gts_samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.gts_samples)\n",
    "        return 1000\n",
    "    \n",
    "    def _read_image_and_gt(self, img_id, img_slice):\n",
    "        img = np.load(os.path.join(self.medsam_imgs, f'CT_zzAMLART_{img_id:03d}-{img_slice:03d}.npy'))\n",
    "        gt = np.load(os.path.join(self.medsam_gts, f'CT_{self.anatomy}_zzAMLART_{img_id:03d}-{img_slice:03d}.npy'))\n",
    "        return img, gt\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ith_example = self.gts_samples[idx]\n",
    "\n",
    "        # get a support set that doesn't contain the same id as the ith example\n",
    "        get_id_from_img = lambda img_name: int(img_name.split('_')[3].split('-')[0])\n",
    "        get_slice_from_img = lambda img_name: int(img_name.split('_')[3].split('-')[1].split('.')[0])\n",
    "\n",
    "        ith_id = get_id_from_img(ith_example)\n",
    "        ith_slice = get_slice_from_img(ith_example)\n",
    "\n",
    "        support_set = random.sample([f for f in self.gts_samples if get_id_from_img(f) != ith_id], self.support_size)\n",
    "\n",
    "        # read in the images and gts for the ith example and the support set and resize them appropriately\n",
    "        ith_img, ith_gt = self._read_image_and_gt(ith_id, ith_slice)\n",
    "\n",
    "        support_imgs = []\n",
    "        support_gts = []\n",
    "\n",
    "        for support_example in support_set:\n",
    "            support_img, support_gt = self._read_image_and_gt(get_id_from_img(support_example), get_slice_from_img(support_example))\n",
    "\n",
    "            support_imgs.append(support_img)\n",
    "            support_gts.append(support_gt)\n",
    "\n",
    "        # resize the images and gts to 128x128 we need for universeg\n",
    "\n",
    "        ith_img = cv2.resize(ith_img, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        ith_gt = cv2.resize(ith_gt, (128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        support_imgs = [cv2.resize(support_img, (128, 128), interpolation=cv2.INTER_LINEAR) for support_img in support_imgs]\n",
    "        support_gts = [cv2.resize(support_gt, (128, 128), interpolation=cv2.INTER_NEAREST) for support_gt in support_gts]\n",
    "\n",
    "        # convert to torch tensors\n",
    "\n",
    "        ith_img = torch.from_numpy(ith_img).float().unsqueeze(0)\n",
    "        ith_gt = torch.from_numpy(ith_gt).float().unsqueeze(0)\n",
    "\n",
    "        support_imgs = [torch.from_numpy(support_img).float().unsqueeze(0) for support_img in support_imgs]\n",
    "        support_gts = [torch.from_numpy(support_gt).float().unsqueeze(0) for support_gt in support_gts]\n",
    "\n",
    "        # stack the support images and gts\n",
    "        support_imgs = torch.stack(support_imgs) # (S x 128 x 128)\n",
    "        support_gts = torch.stack(support_gts) # (S x 128 x 128)\n",
    "\n",
    "        assert support_imgs.shape == (self.support_size, 1, 128, 128), support_imgs.shape\n",
    "        assert support_gts.shape == (self.support_size, 1, 128, 128), support_gts.shape\n",
    "        assert ith_img.shape == (1, 128, 128), ith_img.shape\n",
    "        assert ith_gt.shape == (1, 128, 128), ith_gt.shape\n",
    "\n",
    "        return {\n",
    "            'query_name': ith_example,\n",
    "            'query': ith_img,\n",
    "            'query_gt': ith_gt,\n",
    "            'support_imgs': support_imgs,\n",
    "            'support_gts': support_gts\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 80, 1, 128, 128])\n",
      "torch.Size([1, 80, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "my_dataset = UniverSegDataSet(support_size=support_size, anatomy=anatomy, axis=axis)\n",
    "my_dataloder = DataLoader(my_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# get a batch of data\n",
    "\n",
    "for i, batch in enumerate(my_dataloder):\n",
    "    print(batch['query'].shape)\n",
    "    print(batch['query_gt'].shape)\n",
    "    print(batch['support_imgs'].shape)\n",
    "    print(batch['support_gts'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universeg import universeg\n",
    "\n",
    "dataset = UniverSegDataSet(support_size=support_size, anatomy=anatomy, axis=axis)\n",
    "dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platipy.imaging.label.comparison import compute_metric_total_apl, compute_surface_dsc, compute_metric_hd\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'dice', 'volume_similarity', 'apl', 'surface_distance', 'hausdorff_distance'])\n",
    "\n",
    "save_dir = os.path.join('results_finetuned', anatomy, f'axis{axis}')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_3429703/2010511328.py:58: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_record], ignore_index=True)\n",
      "334it [07:55,  1.42s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "overlap_measures_filter = sitk.LabelOverlapMeasuresImageFilter()\n",
    "\n",
    "# Run the inference\n",
    "model = universeg(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# load in parameters from the trained model\n",
    "checkpoint = torch.load('results_finetuned/finetuning/model_checkpoint_best.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "for i, batch in tqdm(enumerate(dataloader)):\n",
    "    try:\n",
    "        names = batch['query_name']\n",
    "        image = batch['query'].to(device)\n",
    "        label = batch['query_gt']\n",
    "        support_images = batch['support_imgs'].to(device)\n",
    "        support_labels = batch['support_gts'].to(device)\n",
    "        \n",
    "        image = image.to(device)\n",
    "        # label = label.to(device)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "\n",
    "        prediction_logits = model(image, support_images, support_labels)\n",
    "        prediction_soft = torch.sigmoid(prediction_logits)\n",
    "\n",
    "        # threshold probabilities to get hard predictions\n",
    "        prediction_hard = (prediction_soft > 0.5).float()\n",
    "\n",
    "        # delete the variables from the gpu\n",
    "        del image, support_images, support_labels, prediction_logits, prediction_soft\n",
    "\n",
    "        # for each batch of predictions, compute the metrics\n",
    "        for j in range(prediction_hard.shape[0]):\n",
    "            # compute the metrics\n",
    "            prediction = prediction_hard[j].cpu().detach().numpy()\n",
    "            prediction = sitk.GetImageFromArray(prediction)\n",
    "            label_sitk = sitk.GetImageFromArray(label[j])\n",
    "\n",
    "            prediction = sitk.Cast(prediction, sitk.sitkUInt8)\n",
    "            label_sitk = sitk.Cast(label_sitk, sitk.sitkUInt8)\n",
    "\n",
    "            overlap_measures_filter.Execute(label_sitk, prediction)\n",
    "\n",
    "            dice = overlap_measures_filter.GetDiceCoefficient()\n",
    "            hd = compute_metric_hd(label_sitk, prediction)\n",
    "            volume_similarity = overlap_measures_filter.GetVolumeSimilarity()\n",
    "            surface_dsc = compute_surface_dsc(label_sitk, prediction)\n",
    "            apl = compute_metric_total_apl(label_sitk, prediction)\n",
    "\n",
    "            new_record = pd.DataFrame([\n",
    "                {'name': names[j],  'dice': dice, 'volume_similarity': volume_similarity, 'apl': apl, 'surface_distance': surface_dsc, 'hausdorff_distance': hd}\n",
    "            ])\n",
    "\n",
    "            df = pd.concat([df, new_record], ignore_index=True)\n",
    "\n",
    "        df.to_csv(f'{save_dir}/validation.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
