{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(2147483648)\n",
    "random.seed(2147483648)\n",
    "np.random.seed(2147483648)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_size = 80\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using support size 80 and batch size 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Using support size {support_size} and batch size {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "class UniverSegDataSet(Dataset):\n",
    "    def __init__(self, support_size, training=True):\n",
    "\n",
    "        self.medsam_gts = os.path.join(os.environ.get('MedSAM_preprocessed_lowres'), 'gts')\n",
    "        self.medsam_imgs = os.path.join(os.environ.get('MedSAM_preprocessed_lowres'), 'imgs')\n",
    "\n",
    "        self.support_size = support_size\n",
    "\n",
    "        self.training = training\n",
    "        self.anatomies = dict([(f, dict()) for f in os.listdir(self.medsam_gts) if f != 'TotalBinary'])\n",
    "\n",
    "        min_samples = float('inf')\n",
    "\n",
    "        for anatomy in self.anatomies.keys():\n",
    "            for axis in [f for f in os.listdir(os.path.join(self.medsam_gts, anatomy)) if 'axis' in f]:\n",
    "                subdir = os.path.join(anatomy, axis)\n",
    "\n",
    "                samples = [f for f in os.listdir(os.path.join(self.medsam_gts, subdir)) if f.endswith('.npy')]\n",
    "                random.shuffle(samples)\n",
    "\n",
    "                if len(samples) < min_samples:\n",
    "                    min_samples = len(samples)\n",
    "\n",
    "                self.anatomies[anatomy][axis] = samples\n",
    "\n",
    "        self.anatomy_keys = sorted(list(self.anatomies.keys()))\n",
    "        self.list_of_axis = sorted(list(self.anatomies[self.anatomy_keys[0]].keys()))\n",
    "\n",
    "        self.setup_sampler()\n",
    "\n",
    "        self.training_length = min_samples * len(self.anatomy_keys) * len(self.list_of_axis)\n",
    "\n",
    "    def set_training(self):\n",
    "        self.training = True\n",
    "        self.setup_sampler()\n",
    "\n",
    "    def set_validation(self):\n",
    "        self.training = False\n",
    "        self.setup_sampler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.training_length\n",
    "    \n",
    "    def setup_sampler(self):\n",
    "        if self.training:\n",
    "            self._sample_to_consider = lambda idx: (idx // (len(self.list_of_axis) * len(self.anatomy_keys)))\n",
    "        else:\n",
    "            self._sample_to_consider = lambda idx: -(idx // (len(self.list_of_axis) * len(self.anatomy_keys))) - 1\n",
    "\n",
    "    def _read_image_and_gt(self, img_id, img_slice, anatomy, axis):\n",
    "        img = np.load(os.path.join(self.medsam_imgs, f'axis{axis}', f'CT_zzAMLART_{img_id:03d}-{img_slice:03d}.npy'))\n",
    "        gt = np.load(os.path.join(self.medsam_gts, anatomy, f'axis{axis}', f'CT_{anatomy}_zzAMLART_{img_id:03d}-{img_slice:03d}.npy'))\n",
    "        return img, gt\n",
    "\n",
    "    def _anatomy_to_consider(self, idx):\n",
    "        return self.anatomy_keys[(idx // len(self.list_of_axis)) % len(self.anatomy_keys)]\n",
    "    \n",
    "    def _axis_to_consider(self, idx):\n",
    "        return self.list_of_axis[idx % len(self.list_of_axis)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anatomy_to_consider = self._anatomy_to_consider(idx)\n",
    "        axis_to_consider = self._axis_to_consider(idx)\n",
    "        sample_to_consider = self._sample_to_consider(idx)\n",
    "\n",
    "        ith_example = self.anatomies[anatomy_to_consider][axis_to_consider][sample_to_consider]\n",
    "\n",
    "        get_id_from_img = lambda img_name: int(img_name.split('_')[3].split('-')[0])\n",
    "        get_slice_from_img = lambda img_name: int(img_name.split('_')[3].split('-')[1].split('.')[0])\n",
    "\n",
    "        ith_id = get_id_from_img(ith_example)\n",
    "        ith_slice = get_slice_from_img(ith_example)\n",
    "\n",
    "        # get a support set that doesn't contain the same id as the ith example\n",
    "        support_set = random.sample([f for f in self.anatomies[anatomy_to_consider][axis_to_consider] if get_id_from_img(f) != ith_id], self.support_size)\n",
    "\n",
    "        # read in the images and gts for the ith example and the support set and resize them appropriately\n",
    "        ith_img, ith_gt = self._read_image_and_gt(ith_id, ith_slice, anatomy_to_consider, int(axis_to_consider[-1]))\n",
    "\n",
    "        support_imgs = []\n",
    "        support_gts = []\n",
    "\n",
    "        for support_example in support_set:\n",
    "            support_img, support_gt = self._read_image_and_gt(get_id_from_img(support_example), get_slice_from_img(support_example), anatomy_to_consider, int(axis_to_consider[-1]))\n",
    "\n",
    "            support_imgs.append(support_img)\n",
    "            support_gts.append(support_gt)\n",
    "\n",
    "        # resize the images and gts to 128x128 we need for universeg\n",
    "\n",
    "        ith_img = cv2.resize(ith_img, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        ith_gt = cv2.resize(ith_gt, (128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        support_imgs = [cv2.resize(support_img, (128, 128), interpolation=cv2.INTER_LINEAR) for support_img in support_imgs]\n",
    "        support_gts = [cv2.resize(support_gt, (128, 128), interpolation=cv2.INTER_NEAREST) for support_gt in support_gts]\n",
    "\n",
    "        # convert to torch tensors\n",
    "\n",
    "        ith_img = torch.from_numpy(ith_img).float().unsqueeze(0)\n",
    "        ith_gt = torch.from_numpy(ith_gt).float().unsqueeze(0)\n",
    "\n",
    "        support_imgs = [torch.from_numpy(support_img).float().unsqueeze(0) for support_img in support_imgs]\n",
    "        support_gts = [torch.from_numpy(support_gt).float().unsqueeze(0) for support_gt in support_gts]\n",
    "\n",
    "        # stack the support images and gts\n",
    "        support_imgs = torch.stack(support_imgs) # (S x 128 x 128)\n",
    "        support_gts = torch.stack(support_gts) # (S x 128 x 128)\n",
    "\n",
    "        assert support_imgs.shape == (self.support_size, 1, 128, 128), support_imgs.shape\n",
    "        assert support_gts.shape == (self.support_size, 1, 128, 128), support_gts.shape\n",
    "        assert ith_img.shape == (1, 128, 128), ith_img.shape\n",
    "        assert ith_gt.shape == (1, 128, 128), ith_gt.shape\n",
    "\n",
    "        return {\n",
    "            'query_name': ith_example,\n",
    "            'query_anatomy_axis': f'{anatomy_to_consider}_{axis_to_consider}', \n",
    "            'query': ith_img,\n",
    "            'query_gt': ith_gt,\n",
    "            'support_name': support_set,\n",
    "            'support_imgs': support_imgs,\n",
    "            'support_gts': support_gts\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class UniversegDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super(UniversegDataLoader, self).__init__(dataset, **kwargs)\n",
    "\n",
    "    def set_training(self):\n",
    "        self.dataset.set_training()\n",
    "\n",
    "    def set_validation(self):\n",
    "        self.dataset.set_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UniverSegDataSet(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 1, 128, 128])\n",
      "torch.Size([1, 20, 1, 128, 128])\n",
      "torch.Size([1, 20, 1, 128, 128])\n",
      "query name: ['CT_Uterus_zzAMLART_079-301.npy']\n",
      "support_name [('CT_Uterus_zzAMLART_064-286.npy',), ('CT_Uterus_zzAMLART_056-308.npy',), ('CT_Uterus_zzAMLART_014-219.npy',), ('CT_Uterus_zzAMLART_025-182.npy',), ('CT_Uterus_zzAMLART_059-302.npy',), ('CT_Uterus_zzAMLART_084-235.npy',), ('CT_Uterus_zzAMLART_015-235.npy',), ('CT_Uterus_zzAMLART_065-242.npy',), ('CT_Uterus_zzAMLART_082-281.npy',), ('CT_Uterus_zzAMLART_090-211.npy',), ('CT_Uterus_zzAMLART_012-300.npy',), ('CT_Uterus_zzAMLART_029-285.npy',), ('CT_Uterus_zzAMLART_012-260.npy',), ('CT_Uterus_zzAMLART_050-307.npy',), ('CT_Uterus_zzAMLART_009-286.npy',), ('CT_Uterus_zzAMLART_059-317.npy',), ('CT_Uterus_zzAMLART_038-277.npy',), ('CT_Uterus_zzAMLART_076-278.npy',), ('CT_Uterus_zzAMLART_062-254.npy',), ('CT_Uterus_zzAMLART_064-246.npy',)]\n"
     ]
    }
   ],
   "source": [
    "# fetch a batch\n",
    "dataloader = UniversegDataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "dataloader.set_validation()\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch['query'].shape)\n",
    "    print(batch['query_gt'].shape)\n",
    "    print(batch['support_imgs'].shape)\n",
    "    print(batch['support_gts'].shape)\n",
    "    print('query name:', batch['query_name'])\n",
    "    print('support_name', batch['support_name'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universeg import universeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platipy.imaging.label.comparison import compute_metric_total_apl, compute_surface_dsc, compute_metric_hd\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import monai\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame(columns=['epoch', 'train_or_val', 'batch', 'anatomy_axis', 'loss', 'time'])\n",
    "\n",
    "save_dir = os.path.join('results_finetuned', 'finetuning')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the existing training csv\n",
    "\n",
    "if os.path.exists(os.path.join(save_dir, 'training.csv')):\n",
    "    df = pd.read_csv(os.path.join(save_dir, 'training.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_or_val</th>\n",
       "      <th>batch</th>\n",
       "      <th>anatomy_axis</th>\n",
       "      <th>loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>['Uterus_axis2', 'Vagina_axis1', 'Anorectum_ax...</td>\n",
       "      <td>0.323409</td>\n",
       "      <td>0.023994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>['Uterus_axis2', 'Bladder_axis2', 'Uterus_axis2']</td>\n",
       "      <td>0.221870</td>\n",
       "      <td>0.020527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>['Vagina_axis1', 'Vagina_axis2', 'Uterus_axis1']</td>\n",
       "      <td>0.392631</td>\n",
       "      <td>0.022901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>['CTVp_axis0', 'Uterus_axis1', 'Uterus_axis2']</td>\n",
       "      <td>0.212383</td>\n",
       "      <td>0.027104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>['Bladder_axis2', 'Parametrium_axis0', 'CTVp_a...</td>\n",
       "      <td>0.398783</td>\n",
       "      <td>0.020440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9035</th>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.264425</td>\n",
       "      <td>0.159355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9036</th>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120418</td>\n",
       "      <td>0.157920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068474</td>\n",
       "      <td>0.158706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9038</th>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.372611</td>\n",
       "      <td>0.158550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.175169</td>\n",
       "      <td>0.160240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      epoch train_or_val  batch  \\\n",
       "0         0        train      0   \n",
       "1         0        train      1   \n",
       "2         0        train      2   \n",
       "3         0        train      3   \n",
       "4         0        train      4   \n",
       "...     ...          ...    ...   \n",
       "9035      0          val     95   \n",
       "9036      0          val     96   \n",
       "9037      0          val     97   \n",
       "9038      0          val     98   \n",
       "9039      0          val     99   \n",
       "\n",
       "                                           anatomy_axis      loss      time  \n",
       "0     ['Uterus_axis2', 'Vagina_axis1', 'Anorectum_ax...  0.323409  0.023994  \n",
       "1     ['Uterus_axis2', 'Bladder_axis2', 'Uterus_axis2']  0.221870  0.020527  \n",
       "2      ['Vagina_axis1', 'Vagina_axis2', 'Uterus_axis1']  0.392631  0.022901  \n",
       "3        ['CTVp_axis0', 'Uterus_axis1', 'Uterus_axis2']  0.212383  0.027104  \n",
       "4     ['Bladder_axis2', 'Parametrium_axis0', 'CTVp_a...  0.398783  0.020440  \n",
       "...                                                 ...       ...       ...  \n",
       "9035                                                NaN  0.264425  0.159355  \n",
       "9036                                                NaN  0.120418  0.157920  \n",
       "9037                                                NaN  0.068474  0.158706  \n",
       "9038                                                NaN  0.372611  0.158550  \n",
       "9039                                                NaN  0.175169  0.160240  \n",
       "\n",
       "[9040 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 7/8869 [00:17<6:05:29,  2.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     35\u001b[0m dataloader\u001b[38;5;241m.\u001b[39mset_training()\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)):\n\u001b[1;32m     37\u001b[0m     new_record \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m     38\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_or_val\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m: i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manatomy_axis\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_anatomy_axis\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m     39\u001b[0m     ])\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# forward inference \u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m, in \u001b[0;36mUniverSegDataSet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     88\u001b[0m support_gts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m support_example \u001b[38;5;129;01min\u001b[39;00m support_set:\n\u001b[0;32m---> 91\u001b[0m     support_img, support_gt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_image_and_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_id_from_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_example\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_slice_from_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupport_example\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manatomy_to_consider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxis_to_consider\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     support_imgs\u001b[38;5;241m.\u001b[39mappend(support_img)\n\u001b[1;32m     94\u001b[0m     support_gts\u001b[38;5;241m.\u001b[39mappend(support_gt)\n",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m, in \u001b[0;36mUniverSegDataSet._read_image_and_gt\u001b[0;34m(self, img_id, img_slice, anatomy, axis)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_image_and_gt\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_id, img_slice, anatomy, axis):\n\u001b[0;32m---> 58\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedsam_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maxis\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43maxis\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCT_zzAMLART_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimg_id\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m03d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimg_slice\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m03d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     gt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedsam_gts, anatomy, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCT_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manatomy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_zzAMLART_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_slice\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, gt\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/numpy/lib/npyio.py:434\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    432\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    433\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 434\u001b[0m magic \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set up model\n",
    "model = universeg(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# set up optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.00005,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# load the checkpoint\n",
    "checkpoint = torch.load('results_finetuned/finetuning/model_checkpoint_latest.pth')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# monitor best loss\n",
    "best_loss = float('inf')\n",
    "best_loss = checkpoint['best_loss']\n",
    "\n",
    "# set up loss function\n",
    "seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "dataset = UniverSegDataSet(support_size=support_size, training=True)\n",
    "dataloader = UniversegDataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set up training loop\n",
    "for epoch in range(checkpoint['epoch'] + 1, 10):\n",
    "    model.train()\n",
    "    dataloader.set_training()\n",
    "    for i, batch in tqdm(enumerate(dataloader), desc='Training', total=len(dataloader)):\n",
    "        new_record = pd.DataFrame([\n",
    "            {'epoch': epoch, 'train_or_val': 'train', 'batch': i, 'anatomy_axis': batch['query_anatomy_axis'], 'loss': 0, 'time': 0}\n",
    "        ])\n",
    "\n",
    "        # forward inference \n",
    "        start_time = time()\n",
    "        \n",
    "        names = batch['query_name']\n",
    "        image = batch['query'].to(device)\n",
    "        label = batch['query_gt']\n",
    "        support_images = batch['support_imgs'].to(device)\n",
    "        support_labels = batch['support_gts'].to(device)\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction_logits = model(image, support_images, support_labels)\n",
    "\n",
    "        loss = seg_loss(prediction_logits, label) + ce_loss(prediction_logits, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        new_record['loss'] = loss.item()\n",
    "        new_record['time'] = end_time - start_time\n",
    "\n",
    "        df = pd.concat([df, new_record], ignore_index=True)\n",
    "        df.to_csv(f'{save_dir}/training.csv', index=False)\n",
    "\n",
    "    epoch_loss_reduced = df[df['epoch'] == epoch]['loss'].mean()\n",
    "    \n",
    "    # save a checkpoint of the model if the loss is lower than the previous best\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"epoch\": epoch, \n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"loss\": epoch_loss_reduced,\n",
    "        \"best_loss\": best_loss\n",
    "    }\n",
    "\n",
    "    if epoch_loss_reduced < best_loss:\n",
    "        best_loss = epoch_loss_reduced\n",
    "        checkpoint['best_loss'] = best_loss\n",
    "        torch.save(checkpoint, f'{save_dir}/model_checkpoint_best.pth')\n",
    "    torch.save(checkpoint, f'{save_dir}/model_checkpoint_latest.pth')\n",
    "\n",
    "    model.eval()\n",
    "    dataloader.set_validation()\n",
    "\n",
    "    validation_iterator = iter(dataloader)\n",
    "    how_many_batches_validation = 100 # len(dataloader)\n",
    "    for i in tqdm(range(how_many_batches_validation), desc='Validation', total=how_many_batches_validation):\n",
    "        batch = next(validation_iterator)\n",
    "        \n",
    "        new_record = pd.DataFrame([\n",
    "            {'epoch': epoch, 'train_or_val': 'val', 'batch': i, 'anatomy_axis': batch['query_anatomy_axis'], 'loss': 0, 'time': 0}\n",
    "        ])\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # calculate the validation loss\n",
    "        names = batch['query_name']\n",
    "        image = batch['query'].to(device)\n",
    "        label = batch['query_gt']\n",
    "        support_images = batch['support_imgs'].to(device)\n",
    "        support_labels = batch['support_gts'].to(device)\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction_logits = model(image, support_images, support_labels)\n",
    "            loss = seg_loss(prediction_logits, label) + ce_loss(prediction_logits, label)\n",
    "\n",
    "        new_record['loss'] = loss.item()\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        new_record['time'] = end_time - start_time\n",
    "\n",
    "        df = pd.concat([df, new_record], ignore_index=True)\n",
    "        df.to_csv(f'{save_dir}/training.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
