{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_size = 80\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using support size 80 and batch size 3\n"
     ]
    }
   ],
   "source": [
    "print(f'Using support size {support_size} and batch size {batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "class UniverSegDataSet(Dataset):\n",
    "    def __init__(self, support_size, training=True):\n",
    "\n",
    "        self.medsam_gts = os.path.join(os.environ.get('MedSAM_preprocessed_lowres'), 'gts')\n",
    "        self.medsam_imgs = os.path.join(os.environ.get('MedSAM_preprocessed_lowres'), 'imgs')\n",
    "\n",
    "        self.support_size = support_size\n",
    "\n",
    "        self.anatomies = {}\n",
    "        self.training = training\n",
    "\n",
    "        min_samples = math.inf\n",
    "\n",
    "        for root, dirs, files in os.walk(self.medsam_gts):\n",
    "            if root == self.medsam_gts:\n",
    "                for d in dirs:\n",
    "                    if d == 'TotalBinary':\n",
    "                        continue\n",
    "                    self.anatomies[d] = dict()\n",
    "                    for root2, dirs2, _ in os.walk(os.path.join(root, d)):\n",
    "                        if root2 == os.path.join(root, d):\n",
    "                            for d2 in dirs2:\n",
    "                                samples = [f for f in os.listdir(os.path.join(root2, d2)) if f.endswith('.npy')]\n",
    "                                random.shuffle(samples)\n",
    "\n",
    "                                if len(samples) < min_samples:\n",
    "                                    min_samples = len(samples)\n",
    "\n",
    "                                self.anatomies[d][d2] = samples\n",
    "\n",
    "        self.anatomy_keys = sorted(list(self.anatomies.keys()))\n",
    "        self.list_of_axis = sorted(list(self.anatomies[self.anatomy_keys[0]].keys()))\n",
    "\n",
    "        self.setup_sampler()\n",
    "\n",
    "        self.training_length = min_samples * len(self.anatomy_keys) * len(self.list_of_axis)\n",
    "\n",
    "    def set_training(self):\n",
    "        self.training = True\n",
    "        self.setup_sampler()\n",
    "\n",
    "    def set_validation(self):\n",
    "        self.training = False\n",
    "        self.setup_sampler()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.training_length\n",
    "    \n",
    "    def setup_sampler(self):\n",
    "        if self.training:\n",
    "            self._sample_to_consider = lambda idx: (idx // (len(self.list_of_axis) * len(self.anatomy_keys)))\n",
    "        else:\n",
    "            self._sample_to_consider = lambda idx: -(idx // (len(self.list_of_axis) * len(self.anatomy_keys))) - 1\n",
    "\n",
    "    def _read_image_and_gt(self, img_id, img_slice, anatomy, axis):\n",
    "        img = np.load(os.path.join(self.medsam_imgs, f'axis{axis}', f'CT_zzAMLART_{img_id:03d}-{img_slice:03d}.npy'))\n",
    "        gt = np.load(os.path.join(self.medsam_gts, anatomy, f'axis{axis}', f'CT_{anatomy}_zzAMLART_{img_id:03d}-{img_slice:03d}.npy'))\n",
    "        return img, gt\n",
    "\n",
    "    def _anatomy_to_consider(self, idx):\n",
    "        return self.anatomy_keys[(idx // len(self.list_of_axis)) % len(self.anatomy_keys)]\n",
    "    \n",
    "    def _axis_to_consider(self, idx):\n",
    "        return self.list_of_axis[idx % len(self.list_of_axis)]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anatomy_to_consider = self._anatomy_to_consider(idx)\n",
    "        axis_to_consider = self._axis_to_consider(idx)\n",
    "        sample_to_consider = self._sample_to_consider(idx)\n",
    "\n",
    "        ith_example = self.anatomies[anatomy_to_consider][axis_to_consider][sample_to_consider]\n",
    "\n",
    "        get_id_from_img = lambda img_name: int(img_name.split('_')[3].split('-')[0])\n",
    "        get_slice_from_img = lambda img_name: int(img_name.split('_')[3].split('-')[1].split('.')[0])\n",
    "\n",
    "        ith_id = get_id_from_img(ith_example)\n",
    "        ith_slice = get_slice_from_img(ith_example)\n",
    "\n",
    "        # get a support set that doesn't contain the same id as the ith example\n",
    "        support_set = random.sample([f for f in self.anatomies[anatomy_to_consider][axis_to_consider] if get_id_from_img(f) != ith_id], self.support_size)\n",
    "\n",
    "        # read in the images and gts for the ith example and the support set and resize them appropriately\n",
    "        ith_img, ith_gt = self._read_image_and_gt(ith_id, ith_slice, anatomy_to_consider, int(axis_to_consider[-1]))\n",
    "\n",
    "        support_imgs = []\n",
    "        support_gts = []\n",
    "\n",
    "        for support_example in support_set:\n",
    "            support_img, support_gt = self._read_image_and_gt(get_id_from_img(support_example), get_slice_from_img(support_example), anatomy_to_consider, int(axis_to_consider[-1]))\n",
    "\n",
    "            support_imgs.append(support_img)\n",
    "            support_gts.append(support_gt)\n",
    "\n",
    "        # resize the images and gts to 128x128 we need for universeg\n",
    "\n",
    "        ith_img = cv2.resize(ith_img, (128, 128), interpolation=cv2.INTER_LINEAR)\n",
    "        ith_gt = cv2.resize(ith_gt, (128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        support_imgs = [cv2.resize(support_img, (128, 128), interpolation=cv2.INTER_LINEAR) for support_img in support_imgs]\n",
    "        support_gts = [cv2.resize(support_gt, (128, 128), interpolation=cv2.INTER_NEAREST) for support_gt in support_gts]\n",
    "\n",
    "        # convert to torch tensors\n",
    "\n",
    "        ith_img = torch.from_numpy(ith_img).float().unsqueeze(0)\n",
    "        ith_gt = torch.from_numpy(ith_gt).float().unsqueeze(0)\n",
    "\n",
    "        support_imgs = [torch.from_numpy(support_img).float().unsqueeze(0) for support_img in support_imgs]\n",
    "        support_gts = [torch.from_numpy(support_gt).float().unsqueeze(0) for support_gt in support_gts]\n",
    "\n",
    "        # stack the support images and gts\n",
    "        support_imgs = torch.stack(support_imgs) # (S x 128 x 128)\n",
    "        support_gts = torch.stack(support_gts) # (S x 128 x 128)\n",
    "\n",
    "        assert support_imgs.shape == (self.support_size, 1, 128, 128), support_imgs.shape\n",
    "        assert support_gts.shape == (self.support_size, 1, 128, 128), support_gts.shape\n",
    "        assert ith_img.shape == (1, 128, 128), ith_img.shape\n",
    "        assert ith_gt.shape == (1, 128, 128), ith_gt.shape\n",
    "\n",
    "        return {\n",
    "            'query_name': ith_example,\n",
    "            'query_anatomy_axis': f'{anatomy_to_consider}_{axis_to_consider}', \n",
    "            'query': ith_img,\n",
    "            'query_gt': ith_gt,\n",
    "            'support_name': support_set,\n",
    "            'support_imgs': support_imgs,\n",
    "            'support_gts': support_gts\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class UniversegDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super(UniversegDataLoader, self).__init__(dataset, **kwargs)\n",
    "\n",
    "    def set_training(self):\n",
    "        self.dataset.set_training()\n",
    "\n",
    "    def set_validation(self):\n",
    "        self.dataset.set_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch a batch\n",
    "dataset = UniverSegDataSet(support_size)\n",
    "dataloader = UniversegDataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader.set_validation()\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch['query'].shape)\n",
    "    print(batch['query_gt'].shape)\n",
    "    print(batch['support_imgs'].shape)\n",
    "    print(batch['support_gts'].shape)\n",
    "    print('query name:', batch['query_name'])\n",
    "    print('support_name', batch['support_name'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universeg import universeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platipy.imaging.label.comparison import compute_metric_total_apl, compute_surface_dsc, compute_metric_hd\n",
    "import SimpleITK as sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import monai\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "df = pd.DataFrame(columns=['epoch', 'train_or_val', 'batch', 'anatomy_axis', 'loss', 'time'])\n",
    "\n",
    "save_dir = os.path.join('results', 'finetuning')\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# set up model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m universeg(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set up model\n",
    "model = universeg(pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# set up optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.00005,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "# set up loss function\n",
    "seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "dataset = UniverSegDataSet(support_size=support_size, training=True)\n",
    "dataloader = UniversegDataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# set up training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    dataloader.set_training()\n",
    "    for i, batch in tqdm(enumerate(dataloader), desc='Training', total=len(dataloader)):\n",
    "        new_record = pd.DataFrame([\n",
    "            {'epoch': epoch, 'train_or_val': 'train', 'batch': i, 'anatomy_axis': batch['query_anatomy_axis'], 'loss': 0, 'time': 0}\n",
    "        ])\n",
    "\n",
    "        # forward inference \n",
    "        start_time = time()\n",
    "        \n",
    "        names = batch['query_name']\n",
    "        image = batch['query'].to(device)\n",
    "        label = batch['query_gt']\n",
    "        support_images = batch['support_imgs'].to(device)\n",
    "        support_labels = batch['support_gts'].to(device)\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction_logits = model(image, support_images, support_labels)\n",
    "\n",
    "        loss = seg_loss(prediction_logits, label) + ce_loss(prediction_logits, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        new_record['loss'] = loss.item()\n",
    "        new_record['time'] = end_time - start_time\n",
    "\n",
    "        df = pd.concat([df, new_record], ignore_index=True)\n",
    "        df.to_csv(f'{save_dir}/training.csv', index=False)\n",
    "\n",
    "    epoch_loss_reduced = df[df['epoch'] == epoch]['loss'].mean()\n",
    "\n",
    "    model.eval()\n",
    "    dataloader.set_validation()\n",
    "    for i, batch in tqdm(enumerate(dataloader), desc='Validation', total=len(dataloader)):\n",
    "        new_record = pd.DataFrame([\n",
    "            {'epoch': epoch, 'train_or_val': 'val', 'batch': i, 'loss': 0, 'time': 0}\n",
    "        ])\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # calculate the validation loss\n",
    "        names = batch['query_name']\n",
    "        image = batch['query'].to(device)\n",
    "        label = batch['query_gt']\n",
    "        support_images = batch['support_imgs'].to(device)\n",
    "        support_labels = batch['support_gts'].to(device)\n",
    "\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "        support_images = support_images.to(device)\n",
    "        support_labels = support_labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction_logits = model(image, support_images, support_labels)\n",
    "            loss = seg_loss(prediction_logits, label) + ce_loss(prediction_logits, label)\n",
    "\n",
    "        new_record['loss'] = loss.item()\n",
    "\n",
    "        end_time = time()\n",
    "\n",
    "        new_record['time'] = end_time - start_time\n",
    "\n",
    "        df = pd.concat([df, new_record], ignore_index=True)\n",
    "        df.to_csv(f'{save_dir}/training.csv', index=False)\n",
    "    \n",
    "    # save a checkpoint of the model if the loss is lower than the previous best\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"loss\": epoch_loss_reduced,\n",
    "        \"best_loss\": best_loss\n",
    "    }\n",
    "\n",
    "    if epoch_loss_reduced < best_loss:\n",
    "        best_loss = epoch_loss_reduced\n",
    "        checkpoint['best_loss'] = best_loss\n",
    "        torch.save(checkpoint, f'{save_dir}/model_checkpoint_best.pth')\n",
    "    torch.save(checkpoint, f'{save_dir}/model_checkpoint_latest.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
