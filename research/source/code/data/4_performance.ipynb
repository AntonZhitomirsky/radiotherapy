{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.dirname(os.path.abspath(''))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_path(path: str, needs_num=True):\n",
    "    \"\"\"Given a path, assume that it is the full path that points to the file name. The\n",
    "    file nam ehsould contain a number indicating the id number. It should appear first.\n",
    "\n",
    "    Args:\n",
    "        path (str): A path to the file name or the file name itsself. \n",
    "        \n",
    "        needs_num (bool, optional): If the path needs a number. If it doesn't and no\n",
    "        number was found return 0, otherwise return the number found. Defaults to True.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If a number is required and no number was found in the path\n",
    "\n",
    "    Returns:\n",
    "        int: the number found in the path\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # Assume that it is the full path that points to the file name. The file name\n",
    "    # should contain a number indicating the id number. It should appear first\n",
    "    numbers = re.findall('\\d+', path.split('/')[-1])\n",
    "    if needs_num and len(numbers) == 0:\n",
    "        raise ValueError(f\"Could not find a number in {path}\")\n",
    "    if not needs_num and len(numbers) == 0:\n",
    "        return 0\n",
    "    return int(numbers[0])\n",
    "\n",
    "def stats_about_metrics(metrics_dictionary: dict):\n",
    "\n",
    "    return_metrics_dictionary = dict()\n",
    "    return_metrics_dictionary['mean'] = dict()\n",
    "\n",
    "    for model, metrics in metrics_dictionary.items():\n",
    "        return_metrics_dictionary['mean'][model] = dict()\n",
    "        for metric, data in metrics.items():\n",
    "            return_metrics_dictionary['mean'][model][metric] = np.mean(data)\n",
    "\n",
    "    return return_metrics_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from platipy.imaging.label.comparison import compute_metric_total_apl, compute_surface_dsc, compute_metric_hd\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_for_MedSAM(model_path):\n",
    "    dice = {0: [], 1: [], 2: []}\n",
    "    hd = {0: [], 1: [], 2: []}\n",
    "    volume_similarity = {0: [], 1: [], 2: []}\n",
    "    apl = {0: [], 1: [], 2: []}\n",
    "    surface_dsc = {0: [], 1: [], 2: []}\n",
    "\n",
    "    # we assume that we've already recorded and saved the statistics in a file validation.csv\n",
    "    # this will contain the validation data for that particular anatomy across all three axis\n",
    "\n",
    "    df = pd.DataFrame(columns=['name', 'axis', 'dice', 'jaccard', 'volume_similarity', 'apl', 'surface_distance', 'hausdorff_distance'])\n",
    "\n",
    "    # load in the processed data if it already exists\n",
    "    if os.path.exists(os.path.join(model_path, 'validation.csv')):\n",
    "        df = pd.read_csv(os.path.join(model_path, 'validation.csv'))\n",
    "    else:\n",
    "        raise ValueError(\"No validation data found for this model\")\n",
    "    \n",
    "    # go through the pandas dataframe and aggregate the metrics into each of the dictionaries keyed by the axis.\n",
    "     \n",
    "    for _, row in df.iterrows():\n",
    "        dice[row['axis']].append(row['dice'])\n",
    "        hd[row['axis']].append(row['hausdorff_distance'])\n",
    "        volume_similarity[row['axis']].append(row['volume_similarity'])\n",
    "        apl[row['axis']].append(row['apl'])\n",
    "        surface_dsc[row['axis']].append(row['surface_distance'])\n",
    "\n",
    "    return dice, hd, volume_similarity, surface_dsc, apl\n",
    "\n",
    "def calculate_for_UniverSeg(model_path):\n",
    "    dice = {0: [], 1: [], 2: []}\n",
    "    hd = {0: [], 1: [], 2: []}\n",
    "    volume_similarity = {0: [], 1: [], 2: []}\n",
    "    apl = {0: [], 1: [], 2: []}\n",
    "    surface_dsc = {0: [], 1: [], 2: []}\n",
    "\n",
    "    # we assume that we've already recorded and saved the statistics in a file validation.csv\n",
    "    # this will contain the validation data for that particular anatomy across all three axis\n",
    "\n",
    "    df = pd.DataFrame(columns=['name','dice','volume_similarity','apl','surface_distance','hausdorff_distance'])\n",
    "\n",
    "    # go through each axis in the model path and load in the validation data\n",
    "\n",
    "    axis_stats = sorted(os.listdir(model_path))\n",
    "    assert len(axis_stats) == 3, \"There should be 3 axis in the model path\"\n",
    "\n",
    "    for i, axis in enumerate(axis_stats):\n",
    "        if os.path.exists(os.path.join(model_path, axis, 'validation.csv')):\n",
    "            df = pd.read_csv(os.path.join(model_path, axis, 'validation.csv'))\n",
    "        else:\n",
    "            raise ValueError(\"No validation data found for this model\")\n",
    "    \n",
    "        # go through the pandas dataframe and aggregate the metrics into each of the dictionaries keyed by the axis.\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            dice[i].append(row['dice'])\n",
    "            hd[i].append(row['hausdorff_distance'])\n",
    "            volume_similarity[i].append(row['volume_similarity'])\n",
    "            apl[i].append(row['apl'])\n",
    "            surface_dsc[i].append(row['surface_distance'])\n",
    "\n",
    "    return dice, hd, volume_similarity, surface_dsc, apl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(input_path_gt: str, input_path_pred: dict):\n",
    "    \"\"\"Will return the metrics for each prediction path\n",
    "\n",
    "    Args:\n",
    "        input_path_gt (str): a string to the ground truth \n",
    "        input_path_pred (dict): a dictionary containing the paths to the predictions\n",
    "\n",
    "    Returns:\n",
    "        dict: returns dictionary of metrics for the main prediction paths\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of predictions for each input_path_pred\n",
    "    \n",
    "\n",
    "    ground_truth = [os.path.join(input_path_gt, file) for file in sorted(filter(lambda x: '.nii.gz' in x, os.listdir(input_path_gt)))]\n",
    "    assert len(ground_truth) == 100\n",
    "    assert get_id_from_path(ground_truth[0]) == 1\n",
    "\n",
    "    final_metrics = dict()\n",
    "    for k, _ in input_path_pred.items(): final_metrics[k] = dict()\n",
    "\n",
    "    overlap_measures_filter = sitk.LabelOverlapMeasuresImageFilter()\n",
    "\n",
    "    # Iterate over the models and their predictions\n",
    "    for prediction_type, prediction_path in input_path_pred.items():\n",
    "\n",
    "        dice = []\n",
    "        hd = []\n",
    "        volume_similarity = []\n",
    "        apl = []\n",
    "        surface_dsc = []\n",
    "\n",
    "        if 'MedSAM' in prediction_type:\n",
    "            # raise NotImplementedError(\"MedSAM is not decomissioned for the mean while\")\n",
    "            dice, hd, volume_similarity, surface_dsc, apl = calculate_for_MedSAM(prediction_path)\n",
    "        elif 'UniverSeg' in prediction_type:\n",
    "            dice, hd, volume_similarity, surface_dsc, apl = calculate_for_UniverSeg(prediction_path)\n",
    "        else:\n",
    "            # Extract all .nii.gz files for metric calculation\n",
    "            predictions = [os.path.join(prediction_path, file) for file in sorted(filter(lambda x: '.nii.gz' in x, os.listdir(prediction_path)))]\n",
    "\n",
    "            # Iterate over pairs of predictions and ground truth\n",
    "            print(f'Calculating metrics for  {prediction_type}')\n",
    "            for ypred in tqdm(predictions):\n",
    "                y_gt = ground_truth[get_id_from_path(ypred) - 1]\n",
    "                # If we wish to process the metrics as a 3D whole image, then this is\n",
    "                # trivially done without loading each slice separately\n",
    "                ypred_sitk = sitk.ReadImage(ypred)\n",
    "                y_gt_sitk = sitk.ReadImage(y_gt)\n",
    "\n",
    "                overlap_measures_filter.Execute(y_gt_sitk, ypred_sitk)\n",
    "\n",
    "                dice.append(overlap_measures_filter.GetDiceCoefficient())\n",
    "                hd.append(compute_metric_hd(y_gt_sitk, ypred_sitk))\n",
    "                volume_similarity.append(overlap_measures_filter.GetVolumeSimilarity())\n",
    "                surface_dsc.append(compute_surface_dsc(y_gt_sitk, ypred_sitk))\n",
    "                apl.append(compute_metric_total_apl(y_gt_sitk, ypred_sitk))\n",
    "\n",
    "\n",
    "        final_metrics[prediction_type]['DICE Similarity Coefficient'] = dice\n",
    "        final_metrics[prediction_type]['Haussdorf Distance'] = hd\n",
    "        final_metrics[prediction_type]['Relative Volume Difference'] = volume_similarity\n",
    "        final_metrics[prediction_type]['Added Path Length'] = apl\n",
    "        final_metrics[prediction_type]['Surface DSC'] = surface_dsc\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metric_for_class(class_id: int):\n",
    "    \"\"\"Prints a plot of the segmentations for the given class in the predefined format.\n",
    "    This method acts in a factory pattern to generate the plot for each class. \n",
    "\n",
    "    Args:\n",
    "        class_id (int): 1: Anorectum ... 5: Parametrium\n",
    "    \"\"\"\n",
    "\n",
    "    setup_data_vars()\n",
    "\n",
    "    classes = [os.environ.get('Anorectum')\n",
    "             , os.environ.get('Bladder')\n",
    "             , os.environ.get('CTVn')\n",
    "             , os.environ.get('CTVp')\n",
    "             , os.environ.get('Parametrium')\n",
    "             , os.environ.get('Uterus')\n",
    "             , os.environ.get('Vagina')]\n",
    "\n",
    "    gt_labels = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingLabels')) for x in classes]\n",
    "    # gt_labels = [os.path.join(os.environ.get('nnUNet_'), x, 'nnUNetPlans_3d_fullres') for x in classes]\n",
    "    # print('WARNING: using old nnUNet predictions. Change to new path when complete')\n",
    "\n",
    "    anorectum = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('Anorectum'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('Anorectum'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/Anorectum',\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'Anorectum'),\n",
    "    }\n",
    "\n",
    "    bladder = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('Bladder'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('Bladder'), 'nnUNetTrainer__nnUNetPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('Bladder'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'Bladder'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/Bladder',\n",
    "    }\n",
    "\n",
    "    ctvn = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('CTVn'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('CTVn'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'CTVn'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/CTVn',\n",
    "    }\n",
    "\n",
    "    ctvp = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('CTVp'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('CTVp'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'CTVp'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/CTVp',\n",
    "    }\n",
    "\n",
    "    parametrium = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('Parametrium'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('Parametrium'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'Parametrium'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/Parametrium',\n",
    "    }\n",
    "\n",
    "    uterus = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('Uterus'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('Uterus'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'Uterus'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/Uterus',\n",
    "    }\n",
    "\n",
    "    vagina = {\n",
    "        # 'nnUNet': os.path.join(os.environ.get('nnUNet_inference'), os.environ.get('Vagina'), 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres'),\n",
    "        # 'total segmentator (fine-tuned)': os.path.join(os.environ.get('TotalSegmentator_inference'), os.environ.get('Vagina'), 'nnUNetTrainer_250epochs__totseg_nnUNetPlans__3d_fullres'),\n",
    "        'MedSAM boxed': os.path.join(os.environ.get('MedSAM_finetuned'), 'boxed_lowres_2', 'Vagina'),\n",
    "        'UniverSeg out-of-the-box': '/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/research/source/code/UniverSeg/results/Vagina',\n",
    "    }\n",
    "\n",
    "    predictions = [anorectum, bladder, ctvn, ctvp, parametrium, uterus, vagina]\n",
    "\n",
    "    if 1 <= class_id <= len(predictions):\n",
    "        return calculate_metrics(gt_labels[class_id - 1], predictions[class_id - 1])\n",
    "    raise ValueError(\"Invalid class_id. Please choose a class between 1 and 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "tmp = []\n",
    "\n",
    "def plot_metrics(metrics_dictionary: dict, organ_class: str, separate: bool = False, save: bool = False, showfliers = True, table = True, additional_title_context = '', separate_axis_medsam = False):\n",
    "    \"\"\"Plots the metrics for the given dictionary of metrics and prints a table of mean\n",
    "    metrics to the right\n",
    "\n",
    "    Args:\n",
    "        metrics_dictionary (dict): A dictionary containing a key value pair of model type\n",
    "        and value of dictionary. This dictionary will have a key value pairing of metric\n",
    "        type and a list of values for that metric.\n",
    "        \n",
    "        organ_class (str): For saving the figure, supply the name of the organ class\n",
    "        \n",
    "        separate (bool, optional): Whether we print each type of model type separately or\n",
    "        together so that for each metric we plot the models side by side for better\n",
    "        comparison. Defaults to False.\n",
    "\n",
    "        save (bool, optional): Whether to save the figure or not. Defaults to False.\n",
    "\n",
    "        showfliers (bool, optional): Whether to show the outliers in the boxplot. Defaults\n",
    "        to True.\n",
    "\n",
    "        table (bool, optional): Whether to show the table of mean metrics or not. Defaults\n",
    "        to True.\n",
    "\n",
    "        additional_title_context (str, optional): Additional context to add to the title\n",
    "        of the plot. Defaults to ''.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def draw_table(metric_result, ax, model_type):\n",
    "        # Fetch statistics\n",
    "        mean = [np.mean(result) for result in metric_result]\n",
    "        std = [np.std(result) for result in metric_result]\n",
    "        median = [np.median(result) for result in metric_result]\n",
    "\n",
    "        # Format the table\n",
    "        cell_text = []\n",
    "        cell_text.append([f'{mean[i]:.2f}' for i in range(len(mean))])\n",
    "        cell_text.append([f'{std[i]:.2f}' for i in range(len(std))])\n",
    "        cell_text.append([f'{median[i]:.2f}' for i in range(len(std))])\n",
    "        rowLabels = [r'$\\hat{x}$', r'$\\sigma$', r'$med$']\n",
    "\n",
    "        import textwrap as twp\n",
    "        max_line_width = 18\n",
    "        colLabels = [twp.fill(label, max_line_width) for label in model_type]\n",
    "\n",
    "        # Plot the table\n",
    "        table_height = 0.05 * len(rowLabels)\n",
    "        ax_box = ax.get_position()\n",
    "        ax.set_position([ax_box.x0, ax_box.y0, ax_box.width, ax_box.height * (1 - table_height)])\n",
    "        #  bbox=[0., 1., 1., table_height / (1 - table_height)] the position before\n",
    "        table = ax.table(cell_text, cellLoc='center', rowLabels=rowLabels, colLabels=colLabels if not separate else None, fontsize=100) #, bbox=[0., 1., 1., table_height / (1 - table_height)] )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "\n",
    "    if separate and table:\n",
    "        print('[WARNING]: Bug with table printing in the separate mode. Set table=False to avoid this.')\n",
    "\n",
    "    augmented_metrics_dictionary = metrics_dictionary\n",
    "\n",
    "    # If we have medsam predictions, for now, plot the different axese separately\n",
    "    contains_medsam = ['MedSAM' in p or 'UniverSeg' in p for p in metrics_dictionary.keys()]\n",
    "    # heuristic to check if we've already done this shebang\n",
    "    # if sum([1 for p in contains_medsam if p]) <= 3:\n",
    "    if any(contains_medsam):\n",
    "        # go over each model type and extract the metrics for each axis\n",
    "\n",
    "        # extract the keys which are true out from the metrics_dictionary key set\n",
    "        extracted_values = [value for value, flag in zip(list(metrics_dictionary.keys()), contains_medsam) if flag]\n",
    "\n",
    "        for medsam_metric_type in extracted_values:\n",
    "\n",
    "            medsam_metrics = metrics_dictionary[medsam_metric_type]\n",
    "            if separate_axis_medsam:\n",
    "                # Extract the metrics for each axis\n",
    "                set_of_processed_axis = set()\n",
    "                for metric in medsam_metrics.keys():\n",
    "                    set_of_processed_axis.update(medsam_metrics[metric].keys())\n",
    "                    for axis in medsam_metrics[metric].keys():\n",
    "                        if f'{medsam_metric_type} (axis{axis})' not in augmented_metrics_dictionary.keys():\n",
    "                            augmented_metrics_dictionary[f'{medsam_metric_type} (axis{axis})'] = dict()\n",
    "                        augmented_metrics_dictionary[f'{medsam_metric_type} (axis{axis})'][metric] = medsam_metrics[metric][axis]\n",
    "\n",
    "                del augmented_metrics_dictionary[medsam_metric_type]\n",
    "\n",
    "            # now include a global aggregate for each axis\n",
    "            augmented_metrics_dictionary[medsam_metric_type] = dict()\n",
    "            for metric in medsam_metrics.keys():\n",
    "                for axis in medsam_metrics[metric].keys():\n",
    "                    if metric not in augmented_metrics_dictionary[medsam_metric_type].keys():\n",
    "                        augmented_metrics_dictionary[medsam_metric_type][metric] = []\n",
    "                    augmented_metrics_dictionary[medsam_metric_type][metric] += medsam_metrics[metric][axis]\n",
    "\n",
    "\n",
    "\n",
    "    num_models = len(augmented_metrics_dictionary)\n",
    "\n",
    "    nrows = num_models if separate == True else 1\n",
    "    metrics_names = [list(augmented_metrics_dictionary[m].keys()) for m in augmented_metrics_dictionary.keys()]\n",
    "    metrics_names = sorted(list(set([metric for model_metrics in metrics_names for metric in model_metrics])))\n",
    "    ncols = len(metrics_names)\n",
    "\n",
    "    plot_height = 5*1.6\n",
    "    plot_width = 0.4 * num_models *1.6\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=(plot_width * (ncols + num_models - 1), nrows * plot_height))\n",
    "\n",
    "    # Reshape axes to be a 2D array\n",
    "    axes = np.reshape(axes, (nrows, ncols))\n",
    "\n",
    "    for i, row in enumerate(axes):\n",
    "        for j, ax in enumerate(row):\n",
    "            # << Fetch Data Collections For Plotting >>\n",
    "\n",
    "            # Get the model type for printing. If we're not separating models, model_type\n",
    "            # is a list of all model names\n",
    "            model_type = list(augmented_metrics_dictionary.keys())[i] if separate else list(augmented_metrics_dictionary.keys())\n",
    "            # Get the metric type for the current column. Its possible that this metric\n",
    "            # doesn't exist for all models\n",
    "            metrics_type = metrics_names[j]\n",
    "            # Get the data for the current metric type while checking if it exists for the\n",
    "            # current model\n",
    "            getData = lambda m: augmented_metrics_dictionary[m][metrics_type] if metrics_type in augmented_metrics_dictionary[m].keys() else []\n",
    "            metric_result = getData(model_type) if separate else [getData(model) for model in augmented_metrics_dictionary.keys()]\n",
    "            \n",
    "            metric_result = [np.array(m) for m in metric_result]\n",
    "            metric_result = [m[~np.isnan(m)] for m in metric_result]\n",
    "            \n",
    "            # if len(metric_result.shape) == 1:\n",
    "            #     metric_result = metric_result[None, :]\n",
    "\n",
    "            # << Plot the Data >>\n",
    "\n",
    "            # ax.violinplot(metric_result)\n",
    "            ax.boxplot(metric_result, showfliers=showfliers, meanline=True, showmeans=True, patch_artist=True, widths=(.4))\n",
    "\n",
    "            # << Plot Table with Mean Metrics >>\n",
    "            \n",
    "            if table:\n",
    "                draw_table(np.reshape(metric_result, (1, -1)).tolist() if separate else metric_result, ax, model_type) \n",
    "\n",
    "            # << Axis Formatting >>\n",
    "\n",
    "            ax.set_title(metrics_type.capitalize(), fontsize='x-large') # y=1.02,\n",
    "            # TODO: Bug with separable mode. The x-axis labels are not being removed\n",
    "            ax.set_xticklabels([] if table else model_type, rotation=30) #  if not separate else model_type, rotation=30\n",
    "            ax.tick_params(axis='x', which='both', bottom=False)\n",
    "    \n",
    "    if separate:\n",
    "        for ax, title in zip(axes[:,0], augmented_metrics_dictionary.keys()):\n",
    "            ax.set_ylabel(title.capitalize(), fontsize='xx-large')\n",
    "    \n",
    "    # if organ class like Dataset000_classname, extract classname\n",
    "    import re\n",
    "    match = re.search(r'Dataset\\d+_(\\w+)', organ_class)\n",
    "    if match:\n",
    "        organ_class = match.group(1).lower()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # fig.set_facecolor('silver')\n",
    "    fig.suptitle(f'Segmentation metrics for the {organ_class.capitalize()} class {additional_title_context}', y=1, fontsize='xx-large', verticalalignment='center', horizontalalignment='center')  # Set the title of the whole plot\n",
    "    fig.subplots_adjust(top=0.88)  # Adjust the plot to make room for the title\n",
    "\n",
    "    # << Saving the Figure >>\n",
    "\n",
    "    if save: \n",
    "        number = max([0] + [get_id_from_path(fn, False) for fn in os.listdir('metrics/') if fn.startswith(f'metrics{organ_class}')])\n",
    "\n",
    "        file_name = lambda num: f'metrics{organ_class}_{f\"{num}_\"if num is not None else \"\"}{\"separated\" if separate else \"combined\"}_{\"_\".join(additional_title_context.split(\" \"))}.png'\n",
    "\n",
    "        try:\n",
    "            # Move the file\n",
    "            os.makedirs('metrics/old', exist_ok=True)\n",
    "            plt.savefig(f'metrics/{file_name(number + 1)}', bbox_inches='tight')\n",
    "            shutil.move(f'metrics/{file_name(number)}', f'metrics/old/')\n",
    "        except FileNotFoundError as e:\n",
    "            print(f'WARNING: {e}')\n",
    "    \n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metrics for  Dataset001_Anorectum  id  1\n"
     ]
    }
   ],
   "source": [
    "save = False\n",
    "separate = False\n",
    "table = True\n",
    "\n",
    "for c in [\n",
    "             os.environ.get('Anorectum'),\n",
    "             os.environ.get('Bladder'),\n",
    "             os.environ.get('CTVn'),\n",
    "             os.environ.get('CTVp'),\n",
    "             os.environ.get('Parametrium'),\n",
    "             os.environ.get('Uterus'),\n",
    "             os.environ.get('Vagina'),\n",
    "             ]:\n",
    "    id = int(c.split('_')[0][len('Dataset'):])\n",
    "    print('Fetching metrics for ', c, ' id ', id)\n",
    "    # check if c has already been processed\n",
    "    # if c not in metrics.keys():\n",
    "    metrics[c] = fetch_metric_for_class(id)\n",
    "    plot_metrics(metrics[c], c, save=save, separate=separate, table=table, showfliers=False, separate_axis_medsam=True)\n",
    "\n",
    "anorectum_metrics = None if os.environ.get('Anorectum') not in metrics.keys() else metrics[os.environ.get('Anorectum')]\n",
    "bladder_metrics = None if os.environ.get('Bladder') not in metrics.keys() else metrics[os.environ.get('Bladder')]\n",
    "ctvn_metrics = None if os.environ.get('CTVn') not in metrics.keys() else metrics[os.environ.get('CTVn')]\n",
    "ctvp_metrics = None if os.environ.get('CTVp') not in metrics.keys() else metrics[os.environ.get('CTVp')]\n",
    "parametrium_metrics = None if os.environ.get('Parametrium') not in metrics.keys() else metrics[os.environ.get('Parametrium')]\n",
    "uterus_metrics = None if os.environ.get('Uterus') not in metrics.keys() else metrics[os.environ.get('Uterus')]\n",
    "vagina_metrics = None if os.environ.get('Vagina') not in metrics.keys() else metrics[os.environ.get('Vagina')]\n",
    "\n",
    "# anorectum_metrics_ax0 = fetch_metric_for_class(1, axis=0)\n",
    "# bladder_metrics_ax0 = fetch_metric_for_class(2, axis=0)\n",
    "# ctvn_metrics_ax0 = fetch_metric_for_class(3, axis=0)  \n",
    "# ctvp_metrics_ax0 = fetch_metric_for_class(4, axis=0)\n",
    "# parametrium_metrics_ax0 = fetch_metric_for_class(5, axis=0)\n",
    "\n",
    "# anorectum_metrics_ax1 = fetch_metric_for_class(1, axis=1)\n",
    "# bladder_metrics_ax1 = fetch_metric_for_class(2, axis=1)\n",
    "# ctvn_metrics_ax1 = fetch_metric_for_class(3, axis=1)\n",
    "# ctvp_metrics_ax1 = fetch_metric_for_class(4, axis=1)\n",
    "# parametrium_metrics_ax1 = fetch_metric_for_class(5, axis=1)\n",
    "\n",
    "# anorectum_metrics_ax2 = fetch_metric_for_class(1, axis=2)\n",
    "# bladder_metrics_ax2 = fetch_metric_for_class(2, axis=2)\n",
    "# ctvn_metrics_ax2 = fetch_metric_for_class(3, axis=2)\n",
    "# ctvp_metrics_ax2 = fetch_metric_for_class(4, axis=2)\n",
    "# parametrium_metrics_ax2 = fetch_metric_for_class(5, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# new = tmp[0][~np.isnan(tmp[0])]\n",
    "\n",
    "new = np.array(tmp)\n",
    "print(np.any(np.isnan(new)))\n",
    "new = new[~np.isnan(new)]\n",
    "print(np.any(np.isnan(new)))\n",
    "\n",
    "new = list(tmp)\n",
    "\n",
    "print(new)\n",
    "\n",
    "\n",
    "\n",
    "# new = tmp[~np.isnan(tmp)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
