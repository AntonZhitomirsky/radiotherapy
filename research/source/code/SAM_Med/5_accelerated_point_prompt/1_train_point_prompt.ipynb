{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune a checkpoint of MedSAM on point prompted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path to a MedSAM checkpoint, we want to fine tune it on pre-processed data\n",
    "(subject to modifications specified by the paper and the transformation script). This will\n",
    "be done initially on an anatomy-specific level.\n",
    "\n",
    "## Improvement over previous\n",
    "\n",
    "The main limitation with the previous design was that the batches didn't fit neatly into\n",
    "memory, and a lot of overhead was required for loading upsampled slices. In addition to\n",
    "requiring a lot of memory and such, it is not feasible to expect fast performance. With\n",
    "annecdotal experience, some training datasets were around 300 GB, and with constant\n",
    "reading from disk etc. slows the entire system down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argparse Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from time import time, sleep\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from segment_anything import sam_model_registry\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add the setup_data_vars function as we will need it to find the directory for the training data.\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, required=False, help='Whether to resume training using the latest checkpoint in the save_dir', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Inspired by orginal code from the MedSAM/extensions/point_prompt\n",
    "\n",
    "# 1. Add the anatomy on which we will fine-tune\n",
    "parser.add_argument(\n",
    "    '--anatomy',\n",
    "    type=str,\n",
    "    help='Anatomy on which to fine-tune the model. Note: this is case sensitive, please capitalize the first letter and accronyms such as CTVn or CTVp.',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 1.2 Add the model training type\n",
    "parser.add_argument(\n",
    "    '--model_training',\n",
    "    type=str,\n",
    "    help='Determines the type of model that is being trained. For example, if the model uses only points, the argument should be \"point\". If the model uses both points and bounding boxes, the argument should be \"point_bbox\".',\n",
    "    required=True,\n",
    ")\n",
    "\n",
    "# 2. Path to the MedSAM checkpoint\n",
    "parser.add_argument(\n",
    "    '--checkpoint',\n",
    "    type=str,\n",
    "    help='Path to the checkpoint of the model to fine-tune',\n",
    "    default=os.path.join(os.environ['PROJECT_DIR'], 'models', 'MedSAM', 'work_dir', 'MedSAM', 'medsam_vit_b.pth'),\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 3. Path where we will be saving the checkpoints of the fine-tuned model\n",
    "parser.add_argument(\n",
    "    '--save_dir',\n",
    "    type=str,\n",
    "    help='Directory where the fine-tuned model will be saved',\n",
    "    required=False,\n",
    "    default=os.environ.get('MedSAM_finetuned')\n",
    ")\n",
    "\n",
    "# 4. Add the source directory for the data\n",
    "parser.add_argument(\n",
    "    '--img_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the images for the slices of the anatomy',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 5. Add the source directory for the gts\n",
    "parser.add_argument(\n",
    "    '--gt_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the ground truth masks for the slices of the anatomy',\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 6. Number of epochs for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--epochs',\n",
    "    type=int,\n",
    "    help='Number of epochs for the fine-tuning',\n",
    "    required=False,\n",
    "    default=300\n",
    ")\n",
    "\n",
    "# 7. Batch size for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    help='Batch size for the fine-tuning',\n",
    "    required=False,\n",
    "    default=8\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batches_per_epoch',\n",
    "    type=int,\n",
    "    help='Number of batches per epoch',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 8. Learning rate for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    help='Learning rate for the fine-tuning',\n",
    "    required=False,\n",
    "    default=0.00005\n",
    ")\n",
    "\n",
    "# 9. Number of workers for the data loader\n",
    "parser.add_argument(\n",
    "    '--num_workers',\n",
    "    type=int,\n",
    "    help='Number of workers for the data loader',\n",
    "    required=False,\n",
    "    default=16\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--weight_decay',\n",
    "    type=float,\n",
    "    help='Weight decay for the optimizer',\n",
    "    required=False,\n",
    "    default=0.01\n",
    ")\n",
    "\n",
    "# 11. Resume checkpoint\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    type=bool,\n",
    "    help='Whether to resume training using the latest checkpoint in the save_dir',\n",
    "    required=False,\n",
    "    default=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to parse args!\n"
     ]
    }
   ],
   "source": [
    "print('Preparing to parse args!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "# Suppose for now we get the following set of required arguments:\n",
    "args = parser.parse_args([\n",
    "    '--anatomy', 'Bladder',\n",
    "    '--model_training', 'point_general_checkpoint',\n",
    "#     # '--checkpoint', os.path.join(os.environ['PROJECT_DIR'], 'models', 'MedSAM', 'work_dir', 'MedSAM', 'medsam_vit_b.pth'),\n",
    "#     # '--save_dir', os.path.join(os.environ['MedSAM_finetuned']),\n",
    "    '--epochs', '100',\n",
    "    '--batch_size', '9',\n",
    "    '--batches_per_epoch', '100', \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy = args.anatomy\n",
    "checkpoint_path = args.checkpoint\n",
    "save_dir = args.save_dir\n",
    "img_dir = args.img_dir\n",
    "gt_dir = args.gt_dir\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "num_workers = args.num_workers\n",
    "weight_decay = args.weight_decay\n",
    "resume = args.resume\n",
    "batches_per_epoch = args.batches_per_epoch\n",
    "model_training = args.model_training\n",
    "\n",
    "if img_dir is None:\n",
    "    img_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'imgs')\n",
    "if gt_dir is None:\n",
    "    gt_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'gts', anatomy)\n",
    "\n",
    "save_dir = os.path.join(save_dir, model_training, anatomy)\n",
    "\n",
    "# print all the args\n",
    "print('Arguments:')\n",
    "for arg in vars(args):\n",
    "    print(f'{arg}: {getattr(args, arg)}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id_from_file_name_regex = r'.*_(\\d+).*'\n",
    "slice_id_from_file_name_regex = r'.*-(\\d+).*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stocaching import SharedCache\n",
    "\n",
    "# Adapted Dataset class from ../2_no_finetuning/MEDSAM_helper_functions.py\n",
    "class SAM_Dataset(Dataset):\n",
    "    \"\"\"A torch dataset for delivering slices of any axis to a medsam model.\"\"\"\n",
    "\n",
    "    def __init__(self, img_path, gt_path, id_split, data_aug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_path (string): Path to the directory containing the images\n",
    "            gt_path (string): Path to the directory containing the ground truth masks\n",
    "            id_split (list): List of image ids to include in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_img_path = img_path\n",
    "        self.root_gt_path = gt_path\n",
    "        self.id_split = id_split\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        # Assume that axese 0 1 and 2 have been processed.\n",
    "        filter_fn = lambda x : x.endswith('.npy') and int(re.search(image_id_from_file_name_regex, x).group(1)) in id_split\n",
    "        self.axis0_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis0'))))\n",
    "        self.axis1_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis1'))))\n",
    "        self.axis2_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis2'))))\n",
    "\n",
    "        self.cache = SharedCache(\n",
    "            size_limit_gib=50,\n",
    "            dataset_len=self.__len__(),\n",
    "            # dataset_len = self.__len__() if batches_per_epoch is None else min(self.__len__(), batches_per_epoch),\n",
    "            data_dims=(3,1024,1024),\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.axis0_imgs) + len(self.axis1_imgs) + len(self.axis2_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self.__len__(), f\"Index {idx} is out of range for dataset of size {self.__len__()}\"\n",
    "\n",
    "        # Fetch the image and ground truth mask. For safety, we index the items around the\n",
    "        # ground truth masks, so that if for some reason the images are misaligned we will\n",
    "        # guarantee that we will fetch the correct image\n",
    "\n",
    "        img_path, gt_path, img_name = self._get_image_and_gt_path(idx)\n",
    "\n",
    "        # retrieve data from cache if it's there\n",
    "        img = self.cache.get_slot(idx) # will be None if the cache slot was empty or OOB\n",
    "        if img is None:\n",
    "            img = np.load(img_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "            img = np.transpose(img, (2, 0, 1)) # (C, H, W)\n",
    "            assert np.max(img) <= 1. and np.min(img) >= 0., 'image should be normalized to [0, 1]'\n",
    "            \n",
    "            img = torch.tensor(img).float()\n",
    "\n",
    "            self.cache.set_slot(idx, img) # try to cache\n",
    "        \n",
    "        # Loading of ground truth shouldn't be the limiting factor\n",
    "        gt = np.load(gt_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "\n",
    "        # add data augmentation: random fliplr and random flipud\n",
    "        if self.data_aug:\n",
    "            if random.random() > 0.5:\n",
    "                # img = np.ascontiguousarray(np.flip(img, axis=-1))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-1))\n",
    "                img = img.flip(-1).contiguous()\n",
    "                # gt = gt.flip(-1).contiguous()\n",
    "            if random.random() > 0.5:\n",
    "                # img = np.ascontiguousarray(np.flip(img, axis=-2))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-2))\n",
    "                img = img.flip(-2).contiguous()\n",
    "                # gt = gt.flip(-2).contiguous()\n",
    "        \n",
    "        coords = self._get_random_coord(gt)\n",
    "\n",
    "        # The output of the model is 256x256, and it is easier to reason about\n",
    "        # constricting an image, rather than expanding the output back ot 1024x1024\n",
    "\n",
    "        gt = cv2.resize(\n",
    "            gt,\n",
    "            (256, 256),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image\": img, # 3x1024x1024\n",
    "            \"gt2D\": torch.tensor(gt[None, :,:]).long(), # 1x256x256\n",
    "            \"coords\": torch.tensor(coords[None, ...]).float(),\n",
    "            \"image_name\": img_name\n",
    "        }\n",
    "    \n",
    "    def _get_image_and_gt_path(self, idx):\n",
    "        if idx < len(self.axis0_imgs):\n",
    "            axis, gt_name = 0, self.axis0_imgs[idx]\n",
    "        elif idx < len(self.axis0_imgs) + len(self.axis1_imgs):\n",
    "            axis, gt_name = 1, self.axis1_imgs[idx - len(self.axis0_imgs)]\n",
    "        else:\n",
    "            axis, gt_name = 2, self.axis2_imgs[idx - len(self.axis0_imgs) - len(self.axis1_imgs)]\n",
    "\n",
    "        image_id = int(re.search(image_id_from_file_name_regex, gt_name).group(1))\n",
    "        slice_id = int(re.search(slice_id_from_file_name_regex, gt_name).group(1))\n",
    "\n",
    "        img_name = f'CT_zzAMLART_{image_id:03d}-{slice_id:03d}.npy'\n",
    "        \n",
    "        img_path = os.path.join(self.root_img_path, f'axis{axis}', img_name)\n",
    "        gt_path = os.path.join(self.root_gt_path, f'axis{axis}', gt_name)\n",
    "\n",
    "        return img_path, gt_path, img_name\n",
    "    \n",
    "    def _get_random_coord(self, gt):\n",
    "        # Select a random point. We will use this point to guide the model to segment the\n",
    "        # anatomy. The idea is that we want to select the center of this shape with\n",
    "        # greater probability than the outside of the shape.\n",
    "\n",
    "        gt = np.uint8(gt > 0)\n",
    "        y_indices, x_indices = np.where(gt > 0)\n",
    "\n",
    "        # Calculate the centroid of the segmentation\n",
    "        centroid_x = np.mean(x_indices)\n",
    "        centroid_y = np.mean(y_indices)\n",
    "\n",
    "        # Calculate distances of each point from the centroid\n",
    "        distances = np.sqrt((x_indices - centroid_x)**2 + (y_indices - centroid_y)**2)\n",
    "\n",
    "        # Invert the distances to get higher probabilities for points closer to the\n",
    "        # centroid\n",
    "        inverse_distances = 1 / (distances + 1e-6)  # adding a small value to avoid division by zero\n",
    "\n",
    "        # Normalize the probabilities\n",
    "        probabilities = inverse_distances / np.sum(inverse_distances)\n",
    "\n",
    "        # Sample a point based on the calculated probabilities\n",
    "        index = np.random.choice(len(x_indices), p=probabilities)\n",
    "        x_point = x_indices[index]\n",
    "        y_point = y_indices[index]\n",
    "\n",
    "        return np.array([x_point, y_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test to see if the points are being generated correctly and transformations are also ok\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from torchvision.utils import make_grid\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    experimental_datset = SAM_Dataset(img_dir, gt_dir, [1,2,3,4,5], data_aug=False)\n",
    "    dataloader = DataLoader(experimental_datset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Get a batch of examples\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    images = F.interpolate(batch['image'], size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "    grid_imgs = make_grid(images, nrow=4, padding=0)\n",
    "    grid_gts = make_grid(batch['gt2D'].float(), nrow=4, padding=0)\n",
    "    gts_mask = (grid_gts.sum(dim=0) > 0).float()\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(grid_imgs.permute(1, 2, 0))\n",
    "    plt.imshow(gts_mask, alpha=gts_mask, cmap='viridis')\n",
    "\n",
    "    shift_x = 0\n",
    "    shift_y = -256\n",
    "    for i in range(16):\n",
    "\n",
    "        shift_y = shift_y + 256 if i % 4 == 0 else shift_y\n",
    "        shift_x = shift_x + 256 if i % 4 != 0 else 0\n",
    "\n",
    "        coord = batch['coords'][i].squeeze().numpy()\n",
    "        x, y = coord[0], coord[1]\n",
    "        x, y = x * 256 / 1024 + shift_x, y * 256 / 1024 + shift_y\n",
    "        plt.scatter(x, y, c='r', s=60)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Fine-Tuning nn Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAM(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_encoder, \n",
    "                mask_decoder,\n",
    "                prompt_encoder,\n",
    "                freeze_image_encoder=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "        # freeze prompt encoder\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        if self.freeze_image_encoder:\n",
    "            for param in self.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, point_prompt):\n",
    "\n",
    "        # do not compute gradients for pretrained img encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
    "            # not need to convert box to 1024x1024 grid\n",
    "            # bbox is already in 1024x1024\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=point_prompt,\n",
    "                boxes=None,\n",
    "                masks=None,\n",
    "            )\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          ) # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointHandler():\n",
    "    def __init__(\n",
    "            self,\n",
    "            save_dir: str,\n",
    "            checkpoint_path: str,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        self.save_dir = save_dir\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        self.lr = kwargs['lr']\n",
    "        self.weight_decay = kwargs['weight_decay']\n",
    "\n",
    "    def save_checkpoint(self, model, optimizer, epoch, epoch_loss, best_loss, final):\n",
    "        \"\"\"\n",
    "        Will be guaranteed to save the checkpoint in the save_dir location. If the model\n",
    "        is at peak performance, it saves it under 'checkpoint_best' otherwise, by default\n",
    "        it is 'checkpoint_latest'. If specified, the checkpoint is saved under its final\n",
    "        form and thus replaces 'checkpoint_latest' -> 'checkpoint_final'.\n",
    "        \"\"\"\n",
    "\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"epochs\": epoch,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": epoch_loss,\n",
    "            \"best_loss\": best_loss\n",
    "        }\n",
    "\n",
    "        if epoch_loss <= best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_best.pth'))\n",
    "\n",
    "        if final:\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_final.pth'))\n",
    "            os.remove(os.path.join(self.checkpoint_path, 'checkpoint_latest.pth'))\n",
    "        else:\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        return best_loss\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Loads a checkpoint from the save_dir directory. Assumes this function will be called in the context of continuing training that hasn't finished yet.\n",
    "        \"\"\"\n",
    "        model, optimizer = self.load_base_checkpoint()\n",
    "        assert self.checkpoint_exists(), \"did you check that checkpoint_latest exists?\"\n",
    "        checkpoint = torch.load(os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch = checkpoint['epochs']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "\n",
    "        return model, optimizer, epoch, best_loss\n",
    "\n",
    "    def load_base_checkpoint(self):\n",
    "        sam_model = sam_model_registry[\"vit_b\"](checkpoint=self.checkpoint_path)\n",
    "\n",
    "        medsam_model = MedSAM(\n",
    "            image_encoder = sam_model.image_encoder,\n",
    "            mask_decoder = sam_model.mask_decoder,\n",
    "            prompt_encoder = sam_model.prompt_encoder,\n",
    "            freeze_image_encoder = True\n",
    "        )\n",
    "        medsam_model = medsam_model.to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            medsam_model.mask_decoder.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        return medsam_model, optimizer\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        return os.path.exists(os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "    def final_checkpoint_exists(self):\n",
    "        return os.path.exists(os.path.join(self.save_dir, 'checkpoint_final.pth')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderHandler():\n",
    "    def __init__(self, \n",
    "            save_dir, \n",
    "            image_dir, \n",
    "            gt_dir,\n",
    "            batch_size,\n",
    "            num_workers,\n",
    "            data_aug : bool,\n",
    "            training_split = 0.8, \n",
    "            validation_split = 0.2):\n",
    "\n",
    "        # Where to save the data splits\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Where to get image and ground truth info for training\n",
    "        self.image_dir = image_dir\n",
    "        self.gt_dir = gt_dir\n",
    "\n",
    "        # DataLoader parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        # Splits for validation and training\n",
    "        assert training_split + validation_split == 1\n",
    "        self.training_split = training_split\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        # Final dataLoaders\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "\n",
    "    def save_splits_to_json(self, training_image_ids, validation_image_ids):\n",
    "\n",
    "        data = {\n",
    "            \"training_image_ids\": list(training_image_ids),\n",
    "            \"validation_image_ids\": list(validation_image_ids)\n",
    "        }\n",
    "        with open(os.path.join(self.save_dir, 'data_splits.json'), 'w') as json_file:\n",
    "            json.dump(data, json_file)\n",
    "\n",
    "    def load_split_from_json(self):\n",
    "\n",
    "        with open(os.path.join(self.save_dir, 'data_splits.json'), 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        self.training_split = set(data[\"training_image_ids\"])\n",
    "        self.validation_split = set(data[\"validation_image_ids\"])\n",
    "\n",
    "    def try_setup_data_split_from_save_with_fallback(self):\n",
    "\n",
    "        # either load datasplits or setup anew\n",
    "        if os.path.exists(os.path.join(self.save_dir, 'data_Splits.json')):\n",
    "            self.load_split_from_json()\n",
    "        else:\n",
    "            self.setup_new_data_splits()\n",
    "\n",
    "    def setup_new_data_splits(self):\n",
    "        \"\"\"Setup the data splits from scratch and save\"\"\"\n",
    "\n",
    "        # get the image ids that have been processed. Use gt dir as reference\n",
    "        axis0_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis0'))))\n",
    "        axis1_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis1'))))\n",
    "        axis2_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis2'))))\n",
    "\n",
    "        if not axis0_slices == axis1_slices == axis2_slices:\n",
    "            print('[WARNING]: The slices for the anatomy are not consistent across the three axes. Some axese are missing data, please check')\n",
    "        \n",
    "        # Split the data into training and validation\n",
    "        self.training_image_ids = random.sample(list(axis0_slices), int(len(axis0_slices) * self.training_split))\n",
    "        self.validation_image_ids = list(set(axis0_slices) - set(self.training_image_ids))\n",
    "        assert set.intersection(set(self.training_image_ids), set(self.validation_image_ids)).__len__() == 0, 'Training and Validation sets are not disjoint'\n",
    "\n",
    "        # Save the splits in a json file\n",
    "        self.save_splits_to_json(self.training_image_ids, self.validation_image_ids)\n",
    "\n",
    "    def setup_dataloaders(self):\n",
    "        \n",
    "        self.training_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.training_image_ids, data_aug = self.data_aug)\n",
    "        self.validation_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.validation_image_ids, data_aug = self.data_aug)\n",
    "        \n",
    "        # Quick check\n",
    "        assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.validation_dataset.axis0_imgs)) == set(self.validation_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "        assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.training_dataset.axis0_imgs)) == set(self.training_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "\n",
    "        self.train_loader = DataLoader(self.training_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(self.validation_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingHandler():\n",
    "    def __init__(self, save_dir):\n",
    "        # idea, just have a dictionary with the stats that you save at the end of each\n",
    "        # epoch. You save it down to json at the end. On start, you create a new logging\n",
    "        # text file where you have info about the running script. IF there doesn't exist a\n",
    "        # file 'checkpoint_latest_stats.json' then create one. This is where you will load\n",
    "        # up the stats from the previous run. If there is no such file, then you start\n",
    "        # from scratch.\n",
    "\n",
    "        self.per_epoch_stats = dict()\n",
    "        self.save_dir = save_dir\n",
    "        self.curr_epoch = -1\n",
    "        self.curr_epoch_stats = dict()\n",
    "\n",
    "\n",
    "    def save_stats(self):\n",
    "        with open(os.path.join(self.save_dir, 'checkpoint_latest_stats.json'), 'w') as json_file:\n",
    "            json.dump(self.per_epoch_stats, json_file)\n",
    "\n",
    "    def load_stats(self):\n",
    "        with open(os.path.join(self.save_dir, 'checkpoint_latest_stats.json'), 'r') as json_file:\n",
    "            self.per_epoch_stats = json.load(json_file)\n",
    "\n",
    "    def log_metric(self, key, value, epoch):\n",
    "        assert self.curr_epoch == epoch\n",
    "        if key not in self.curr_epoch_stats:\n",
    "            self.curr_epoch_stats[key] = [value]\n",
    "        else:\n",
    "            self.curr_epoch_stats[key].append(value)\n",
    "\n",
    "    def log(self, line):\n",
    "        with open(self.loggerName, 'a') as f:\n",
    "            f.write(f'{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} - {line}\\n')\n",
    "\n",
    "    def start_new_epoch(self, epoch):\n",
    "        if self.curr_epoch >= 0:\n",
    "            self.save_stats()\n",
    "            self.per_epoch_stats[self.curr_epoch] = self.curr_epoch_stats\n",
    "        self.curr_epoch = epoch\n",
    "        self.curr_epoch_stats = {}\n",
    "\n",
    "        self.curr_epoch_stats['epoch_start'] = time()\n",
    "\n",
    "    def end_current_epoch(self, epoch):\n",
    "        assert epoch == self.curr_epoch\n",
    "        end_time = time()\n",
    "        self.curr_epoch_stats['epoch_end'] = end_time\n",
    "        self.curr_epoch_stats['epoch_time'] = end_time - self.curr_epoch_stats['epoch_start']\n",
    "\n",
    "        self.per_epoch_stats[self.curr_epoch] = self.curr_epoch_stats\n",
    "        self.save_stats()\n",
    "\n",
    "    def setup_logger(self):\n",
    "        # check if the file exists\n",
    "        if os.path.exists(os.path.join(save_dir, 'checkpoint_latest_stats.json')):\n",
    "            self.load_stats()\n",
    "        else:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            self.save_stats()\n",
    "\n",
    "        # create a logging file based on datetime\n",
    "        now = datetime.now()\n",
    "        self.loggerName = os.path.join(self.save_dir, f'training_{now.strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        \n",
    "        with open(self.loggerName, 'w') as f:\n",
    "            f.write(f\"==============================\\n\")\n",
    "            f.write(f\"Initialised Logger at {now.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Happy Logging!\\n\")\n",
    "            f.write(f\"==============================\\n\")\n",
    "    \n",
    "    def plot_stats(self):\n",
    "        self.load_stats()  # Make sure that the data is current\n",
    "\n",
    "        # Extract data for plotting\n",
    "        epochs = []\n",
    "        dice_loss = []\n",
    "        ce_loss = []\n",
    "        val_dice_loss = []\n",
    "        val_ce_loss = []\n",
    "        epoch_time = []\n",
    "\n",
    "        for key, value in self.per_epoch_stats.items():\n",
    "            epochs.append(int(key))\n",
    "            dice_loss.append(sum(value['dice_loss']) / len(value['dice_loss']))\n",
    "            ce_loss.append(sum(value['ce_loss']) / len(value['ce_loss']))\n",
    "            val_dice_loss.append(sum(value['val_dice_loss']) / len(value['val_dice_loss']))\n",
    "            val_ce_loss.append(sum(value['val_ce_loss']) / len(value['val_ce_loss']))\n",
    "            epoch_time.append(value['epoch_time'])\n",
    "\n",
    "        # Plotting\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "        # Loss plot\n",
    "        ax1.plot(epochs, dice_loss, label='Dice Loss (Train)', marker='o')\n",
    "        ax1.plot(epochs, ce_loss, label='CE Loss (Train)', marker='o')\n",
    "        ax1.plot(epochs, val_dice_loss, label='Dice Loss (Validation)', marker='o')\n",
    "        ax1.plot(epochs, val_ce_loss, label='CE Loss (Validation)', marker='o')\n",
    "\n",
    "        ax1.set_title('Losses Over Epochs')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Epoch time plot\n",
    "        ax2.plot(epochs, epoch_time, label='Epoch Time', color='orange', marker='o')\n",
    "\n",
    "        ax2.set_title('Epoch Time Over Epochs')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Epoch Time (seconds)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.save_dir, 'progress.png'))\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggingHandler = LoggingHandler(save_dir)\n",
    "dataloaderHandler = DataLoaderHandler(save_dir, img_dir, gt_dir, batch_size, num_workers, True)\n",
    "checkpointHandler = CheckpointHandler(save_dir, checkpoint_path, lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAMTrainer(object):\n",
    "    def __init__(self, loggingHandler, dataloaderHandler, checkpointHandler, *args, **kwargs):\n",
    "        self.loggingHandler = loggingHandler\n",
    "        self.dataloaderHandler = dataloaderHandler\n",
    "        self.checkpointHandler = checkpointHandler\n",
    "\n",
    "        self.epochs = kwargs['epochs']\n",
    "        self.batches_per_epoch = -1 if 'batches_per_epoch' not in kwargs.keys() or kwargs['batches_per_epoch'] is None else kwargs['batches_per_epoch']\n",
    "        self.resume = kwargs['resume']\n",
    "\n",
    "        self.dice_loss_fn = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "        self.ce_loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def run_training(self):\n",
    "        self.on_train_start()\n",
    "\n",
    "        for epoch in range(self.current_epoch, self.epochs):\n",
    "            self.on_epoch_start(epoch)\n",
    "\n",
    "            pbar = tqdm(range(self.batches_per_epoch))\n",
    "            for batch_id in pbar:\n",
    "\n",
    "                batch = next(iter(self.dataloaderHandler.train_loader))\n",
    "                dice_loss, ce_loss = self.train_step(batch_id, batch)            \n",
    "                \n",
    "                pbar.set_description(f\"Epoch {epoch}, loss: {dice_loss + ce_loss:.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.on_validation_epoch_start()\n",
    "                val_len = min(len(self.dataloaderHandler.val_loader), self.batches_per_epoch) # in debugging batches might be small, so we go with this.\n",
    "                pbar = tqdm(range(val_len))\n",
    "                for batch_id in pbar:\n",
    "\n",
    "                    batch = next(iter(self.dataloaderHandler.val_loader))\n",
    "                    dice_loss, ce_loss = self.validation_step(batch_id, batch)\n",
    "                    \n",
    "                    pbar.set_description(f\"Validating epoch {epoch}, loss: {dice_loss + ce_loss:.4f}\")\n",
    "\n",
    "                self.on_validation_epoch_end()\n",
    "\n",
    "            self.on_epoch_end(epoch)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.loggingHandler.setup_logger()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.loggingHandler.log('Setting up dataloaders')\n",
    "        self.dataloaderHandler.try_setup_data_split_from_save_with_fallback()\n",
    "        self.dataloaderHandler.setup_dataloaders()\n",
    "\n",
    "        self.batches_per_epoch = len(self.dataloaderHandler.train_loader) if self.batches_per_epoch == -1 else self.batches_per_epoch\n",
    "\n",
    "        self.loggingHandler.log('Setting up models')\n",
    "        self.model, self.optimizer, self.current_epoch, self.best_loss = None, None, 0, 1e10\n",
    "        if self.checkpointHandler.final_checkpoint_exists() and self.resume:\n",
    "            self.loggingHandler.log('We have already trained this model to completion')\n",
    "            exit(1)\n",
    "        elif self.checkpointHandler.checkpoint_exists() and self.resume:\n",
    "            self.loggingHandler.log('Resume is true and a checkpoint exists, we resume')\n",
    "            self.model, self.optimizer, self.current_epoch, self.best_loss = self.checkpointHandler.load_checkpoint()\n",
    "            self.current_epoch += 1\n",
    "            self.loggingHandler.log(f'Resuming at epoch: {self.current_epoch}')\n",
    "        else:\n",
    "            self.loggingHandler.log('Setting up a fresh start model')\n",
    "            self.model, self.optimizer = self.checkpointHandler.load_base_checkpoint()\n",
    "\n",
    "    def on_epoch_start(self, epoch):\n",
    "        self.loggingHandler.log('=====================================')\n",
    "        self.loggingHandler.log('Setting up a new epoch for the logger')\n",
    "        self.loggingHandler.start_new_epoch(epoch)\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def train_step(self, step, batch):\n",
    "        self.loggingHandler.log(f'Starting epoch {self.current_epoch} and step {step} out of {self.batches_per_epoch}')\n",
    "\n",
    "        # Get data\n",
    "        image = batch[\"image\"].to(device)\n",
    "        gt2D = batch[\"gt2D\"].to(device)\n",
    "        coords_torch = batch[\"coords\"].to(device) # (B, 2)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        labels_torch = torch.ones(coords_torch.shape[0]).long() # (B,)\n",
    "        labels_torch = labels_torch.unsqueeze(1) # (B, 1)\n",
    "        coords_torch, labels_torch = coords_torch.to(device), labels_torch.to(device)\n",
    "        point_prompt = (coords_torch, labels_torch)\n",
    "        medsam_lite_pred = self.model(image, point_prompt)\n",
    "\n",
    "        dice_loss = self.dice_loss_fn(medsam_lite_pred, gt2D)\n",
    "        ce_loss = self.ce_loss_fn(medsam_lite_pred, gt2D.float())\n",
    "\n",
    "        loss = dice_loss + ce_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        self.loggingHandler.log(f'[TRAINING]:   Received dice loss: {dice_loss.item()} with cross entropy loss: {ce_loss.item()}')\n",
    "        self.loggingHandler.log_metric('dice_loss', dice_loss.item(), self.current_epoch)\n",
    "        self.loggingHandler.log_metric('ce_loss', ce_loss.item(), self.current_epoch)\n",
    "\n",
    "        return dice_loss.item(), ce_loss.item()\n",
    "\n",
    "    def on_epoch_end(self, epoch):\n",
    "        self.loggingHandler.end_current_epoch(epoch)\n",
    "\n",
    "        # Get reduced loss\n",
    "        dice_loss = self.loggingHandler.curr_epoch_stats['dice_loss']\n",
    "        ce_loss = self.loggingHandler.curr_epoch_stats['ce_loss']\n",
    "        epoch_loss_reduced = (sum(dice_loss) + sum(ce_loss)) / (len(dice_loss) + len(ce_loss))\n",
    "\n",
    "        if epoch_loss_reduced < self.best_loss:\n",
    "            self.best_loss = epoch_loss_reduced\n",
    "        \n",
    "        self.checkpointHandler.save_checkpoint(self.model, self.optimizer, epoch, epoch_loss_reduced, self.best_loss, final=False)\n",
    "\n",
    "        self.loggingHandler.plot_stats()\n",
    "\n",
    "        self.loggingHandler.log('=====================================')\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.loggingHandler.log('Starting validation epoch')\n",
    "\n",
    "    def validation_step(self, batch_id, batch):\n",
    "        self.loggingHandler.log(f'Validation step {batch_id} out of {min(len(self.dataloaderHandler.val_loader), self.batches_per_epoch)}')\n",
    "\n",
    "        image = batch[\"image\"].to(device)\n",
    "        gt2D = batch[\"gt2D\"].to(device)\n",
    "        coords_torch = batch[\"coords\"].to(device)\n",
    "\n",
    "        labels_torch = torch.ones(coords_torch.shape[0]).long().to(device)\n",
    "        labels_torch = labels_torch.unsqueeze(1)\n",
    "\n",
    "        point_prompt = (coords_torch, labels_torch)\n",
    "        medsam_lite_pred = self.model(image, point_prompt)\n",
    "\n",
    "        dice_loss = self.dice_loss_fn(medsam_lite_pred, gt2D)\n",
    "        ce_loss = self.ce_loss_fn(medsam_lite_pred, gt2D.float())\n",
    "\n",
    "        self.loggingHandler.log(f'[VALIDATION]: Received dice loss: {dice_loss.item()} with cross entropy loss: {ce_loss.item()}')\n",
    "        self.loggingHandler.log_metric('val_dice_loss', dice_loss.item(), self.current_epoch)\n",
    "        self.loggingHandler.log_metric('val_ce_loss', ce_loss.item(), self.current_epoch)\n",
    "\n",
    "        return dice_loss.item(), ce_loss.item()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # Get reduced loss\n",
    "        dice_loss = self.loggingHandler.curr_epoch_stats['val_dice_loss']\n",
    "        ce_loss = self.loggingHandler.curr_epoch_stats['val_ce_loss']\n",
    "        epoch_loss_reduced = (sum(dice_loss) + sum(ce_loss)) / (len(dice_loss) + len(ce_loss))\n",
    "\n",
    "        self.loggingHandler.log('Validation epoch ended')\n",
    "        self.loggingHandler.log(f'Average loss: {epoch_loss_reduced}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTrainer = MedSAMTrainer(\n",
    "    loggingHandler, \n",
    "    dataloaderHandler, \n",
    "    checkpointHandler,\n",
    "    epochs=epochs,\n",
    "    resume=resume,\n",
    "    batches_per_epoch=batches_per_epoch\n",
    ")\n",
    "myTrainer.run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
