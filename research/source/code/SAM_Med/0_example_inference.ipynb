{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% environment and functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "join = os.path.join\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from skimage import io, transform\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# visualization functions\n",
    "# source: https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb\n",
    "# change color to avoid red and green\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='blue', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "@torch.no_grad()\n",
    "def medsam_inference(medsam_model, img_embed, box_1024, H, W):\n",
    "    box_torch = torch.as_tensor(box_1024, dtype=torch.float, device=img_embed.device)\n",
    "    if len(box_torch.shape) == 2:\n",
    "        box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "\n",
    "    sparse_embeddings, dense_embeddings = medsam_model.prompt_encoder(\n",
    "        points=None,\n",
    "        boxes=box_torch,\n",
    "        masks=None,\n",
    "    )\n",
    "    low_res_logits, _ = medsam_model.mask_decoder(\n",
    "        image_embeddings=img_embed, # (B, 256, 64, 64)\n",
    "        image_pe=medsam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "        sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "        dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "        multimask_output=False,\n",
    "        )\n",
    "\n",
    "    low_res_pred = torch.sigmoid(low_res_logits)  # (1, 1, 256, 256)\n",
    "\n",
    "    low_res_pred = F.interpolate(\n",
    "        low_res_pred,\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )  # (1, 1, gt.shape)\n",
    "    low_res_pred = low_res_pred.squeeze().cpu().numpy()  # (256, 256)\n",
    "    medsam_seg = (low_res_pred > 0.5).astype(np.uint8)\n",
    "    return medsam_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-02 12:53:30--  https://raw.githubusercontent.com/bowang-lab/MedSAM/main/assets/img_demo.png\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 87865 (86K) [image/png]\n",
      "Saving to: ‘img_demo.png’\n",
      "\n",
      "img_demo.png        100%[===================>]  85.81K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2024-05-02 12:53:30 (27.0 MB/s) - ‘img_demo.png’ saved [87865/87865]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download model and data\n",
    "!wget -O img_demo.png https://raw.githubusercontent.com/bowang-lab/MedSAM/main/assets/img_demo.png\n",
    "# !wget -O medsam_vit_b.pth https://zenodo.org/records/10689643/files/medsam_vit_b.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/vol/bitbucket/az620/radiothearpy/models/checkpoints/medsam_vit_b.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m MedSAM_CKPT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/vol/bitbucket/az620/radiothearpy/models/checkpoints/medsam_vit_b.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m medsam_model \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMedSAM_CKPT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m medsam_model \u001b[38;5;241m=\u001b[39m medsam_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m medsam_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/vol/bitbucket/az620/radiotherapy/models/MedSAM/segment_anything/build_sam.py:45\u001b[0m, in \u001b[0;36mbuild_sam_vit_b\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sam_vit_b\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/az620/radiotherapy/models/MedSAM/segment_anything/build_sam.py:143\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28mprint\u001b[39m(checkpoint\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is downloaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    144\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    145\u001b[0m     sam\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/vol/bitbucket/az620/radiothearpy/models/checkpoints/medsam_vit_b.pth'"
     ]
    }
   ],
   "source": [
    "#%% load model and image\n",
    "MedSAM_CKPT_PATH = \"/vol/bitbucket/az620/radiotherapy/models/MedSAM/checkpoints/medsam_vit_b.pth\"\n",
    "device = \"cuda:0\"\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_CKPT_PATH)\n",
    "medsam_model = medsam_model.to(device)\n",
    "medsam_model.eval()\n",
    "\n",
    "img_np = io.imread('img_demo.png')\n",
    "if len(img_np.shape) == 2:\n",
    "    img_3c = np.repeat(img_np[:, :, None], 3, axis=-1)\n",
    "else:\n",
    "    img_3c = img_np\n",
    "H, W, _ = img_3c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% image preprocessing and model inference\n",
    "img_1024 = transform.resize(img_3c, (1024, 1024), order=3, preserve_range=True, anti_aliasing=True).astype(np.uint8)\n",
    "img_1024 = (img_1024 - img_1024.min()) / np.clip(\n",
    "    img_1024.max() - img_1024.min(), a_min=1e-8, a_max=None\n",
    ")  # normalize to [0, 1], (H, W, 3)\n",
    "# convert the shape to (3, H, W)\n",
    "img_1024_tensor = torch.tensor(img_1024).float().permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "box_np = np.array([[95,255, 190, 350]])\n",
    "# transfer box_np t0 1024x1024 scale\n",
    "box_1024 = box_np / np.array([W, H, W, H]) * 1024\n",
    "with torch.no_grad():\n",
    "    image_embedding = medsam_model.image_encoder(img_1024_tensor) # (1, 256, 64, 64)\n",
    "\n",
    "medsam_seg = medsam_inference(medsam_model, image_embedding, box_1024, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% visualize results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(img_3c)\n",
    "show_box(box_np[0], ax[0])\n",
    "ax[0].set_title(\"Input Image and Bounding Box\")\n",
    "ax[1].imshow(img_3c)\n",
    "show_mask(medsam_seg, ax[1])\n",
    "show_box(box_np[0], ax[1])\n",
    "ax[1].set_title(\"MedSAM Segmentation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
