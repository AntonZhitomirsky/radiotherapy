{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune a checkpoint of MedSAM on point prompted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path to a MedSAM checkpoint, we want to fine tune it on pre-processed data\n",
    "(subject to modifications specified by the paper and the transformation script). This will\n",
    "be done initially on an anatomy-specific level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argparse Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add the setup_data_vars function as we will need it to find the directory for the training data.\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()\n",
    "\n",
    "# Add utility classes\n",
    "dir2 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '0_utils'))\n",
    "if not dir2 in sys.path: sys.path.append(dir2)\n",
    "\n",
    "from dataset import SAM_Dataset\n",
    "from view_MedSAM_batch import display_batch\n",
    "from medsam_model import MedSAM\n",
    "from checkpoint_handler import CheckpointHandler\n",
    "from dataload_handler import DataLoaderHandler\n",
    "from logging_handler import LoggingHandler\n",
    "from trainer import MedSAMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Inspired by orginal code from the MedSAM/extensions/point_prompt\n",
    "\n",
    "# 1. Add the anatomy on which we will fine-tune\n",
    "parser.add_argument(\n",
    "    '--anatomy',\n",
    "    type=str,\n",
    "    help='Anatomy on which to fine-tune the model. Note: this is case sensitive, please capitalize the first letter and accronyms such as CTVn or CTVp.',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 1.2 Add the model training type\n",
    "parser.add_argument(\n",
    "    '--model_training',\n",
    "    type=str,\n",
    "    help='Determines the type of model that is being trained. For example, if the model uses only points, the argument should be \"point\". If the model uses both points and bounding boxes, the argument should be \"point_bbox\".',\n",
    "    required=True,\n",
    ")\n",
    "\n",
    "# 2. Path to the MedSAM checkpoint\n",
    "parser.add_argument(\n",
    "    '--checkpoint',\n",
    "    type=str,\n",
    "    help='Path to the checkpoint of the model to fine-tune',\n",
    "    default=os.path.join(os.environ['PROJECT_DIR'], 'models', 'MedSAM', 'work_dir', 'MedSAM', 'medsam_vit_b.pth'),\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 3. Path where we will be saving the checkpoints of the fine-tuned model\n",
    "parser.add_argument(\n",
    "    '--save_dir',\n",
    "    type=str,\n",
    "    help='Directory where the fine-tuned model will be saved',\n",
    "    required=False,\n",
    "    default=os.environ.get('MedSAM_finetuned')\n",
    ")\n",
    "\n",
    "# 4. Add the source directory for the data\n",
    "parser.add_argument(\n",
    "    '--img_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the images for the slices of the anatomy',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 5. Add the source directory for the gts\n",
    "parser.add_argument(\n",
    "    '--gt_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the ground truth masks for the slices of the anatomy',\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 6. Number of epochs for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--epochs',\n",
    "    type=int,\n",
    "    help='Number of epochs for the fine-tuning',\n",
    "    required=False,\n",
    "    default=300\n",
    ")\n",
    "\n",
    "# 7. Batch size for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    help='Batch size for the fine-tuning',\n",
    "    required=False,\n",
    "    default=8\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--batches_per_epoch',\n",
    "    type=int,\n",
    "    help='Number of batches per epoch',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 8. Learning rate for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    help='Learning rate for the fine-tuning',\n",
    "    required=False,\n",
    "    default=0.00005\n",
    ")\n",
    "\n",
    "# 9. Number of workers for the data loader\n",
    "parser.add_argument(\n",
    "    '--num_workers',\n",
    "    type=int,\n",
    "    help='Number of workers for the data loader',\n",
    "    required=False,\n",
    "    default=16\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--weight_decay',\n",
    "    type=float,\n",
    "    help='Weight decay for the optimizer',\n",
    "    required=False,\n",
    "    default=0.01\n",
    ")\n",
    "\n",
    "# 11. Resume checkpoint\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    type=bool,\n",
    "    help='Whether to resume training using the latest checkpoint in the save_dir',\n",
    "    required=False,\n",
    "    default=True\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--lowres',\n",
    "    type=bool,\n",
    "    help='A flag for setting the source of the data. For now, if the flag is set to True, the data will be loaded from the lowres directory. Otherwise, we load it from the pure pre-processed directory.',\n",
    "    required=False,\n",
    "    default=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing to parse args!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose for now we get the following set of required arguments:\n",
    "args = parser.parse_args([\n",
    "    '--anatomy', 'CTVn',\n",
    "    '--model_training', 'boxed_lowres',\n",
    "#     # '--checkpoint', os.path.join(os.environ['PROJECT_DIR'], 'models', 'MedSAM', 'work_dir', 'MedSAM', 'medsam_vit_b.pth'),\n",
    "#     # '--save_dir', os.path.join(os.environ['MedSAM_finetuned']),\n",
    "    '--epochs', '100',\n",
    "    '--batch_size', '4',\n",
    "    '--batches_per_epoch', '200', \n",
    "    '--lowres', 'True',\n",
    "])\n",
    "\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy = args.anatomy\n",
    "checkpoint_path = args.checkpoint\n",
    "save_dir = args.save_dir\n",
    "img_dir = args.img_dir\n",
    "gt_dir = args.gt_dir\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "num_workers = args.num_workers\n",
    "weight_decay = args.weight_decay\n",
    "resume = args.resume\n",
    "batches_per_epoch = args.batches_per_epoch\n",
    "model_training = args.model_training\n",
    "lowres = args.lowres\n",
    "\n",
    "if img_dir is None:\n",
    "    img_dir = os.environ['MedSAM_preprocessed_lowres'] if lowres else os.environ['MedSAM_preprocessed']\n",
    "    img_dir = os.path.join(img_dir, 'imgs')\n",
    "if gt_dir is None:\n",
    "    gt_dir = os.environ['MedSAM_preprocessed_lowres'] if lowres else os.environ['MedSAM_preprocessed']\n",
    "    gt_dir = os.path.join(gt_dir, 'gts', anatomy)\n",
    "\n",
    "assert not lowres or ('lowres' in img_dir and 'lowres' in gt_dir) , 'Please make sure that the lowres flag is set correctly!'\n",
    "\n",
    "save_dir = os.path.join(save_dir, model_training, anatomy)\n",
    "\n",
    "# print all the args\n",
    "print('Arguments:')\n",
    "print(f'anatomy {anatomy}')\n",
    "print(f'checkpoint {checkpoint_path}')\n",
    "print(f'save_dir {save_dir}')\n",
    "print(f'img_dir {img_dir}')\n",
    "print(f'gt_dir {gt_dir}')\n",
    "print(f'epochs {epochs}')\n",
    "print(f'batch_size {batch_size}')\n",
    "print(f'lr {lr}')\n",
    "print(f'num_workers {num_workers}')\n",
    "print(f'weight_decay {weight_decay}')\n",
    "print(f'resume {resume}')\n",
    "print(f'batches_per_epoch {batches_per_epoch}')\n",
    "print(f'model_training {model_training}')\n",
    "print(f'lowres {lowres}')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick test to see if the points are being generated correctly and transformations are also ok\n",
    "# experimental_datset = SAM_Dataset(img_dir, gt_dir, [i for i in range(1, 2)], data_aug=False, max_box_points=1)\n",
    "# dataloader = DataLoader(experimental_datset, batch_size=25, shuffle=True)\n",
    "# # Get a batch of examples\n",
    "# batch = next(iter(dataloader))\n",
    "# # Display the batch\n",
    "# display_batch(batch, show_points = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggingHandler = LoggingHandler(save_dir)\n",
    "dataloaderHandler = DataLoaderHandler(save_dir, img_dir, gt_dir, batch_size, num_workers, True, 0, 5, 1)\n",
    "checkpointHandler = CheckpointHandler(save_dir, checkpoint_path, device, lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myTrainer = MedSAMTrainer(\n",
    "    loggingHandler, \n",
    "    dataloaderHandler, \n",
    "    checkpointHandler,\n",
    "    device,\n",
    "    epochs=epochs,\n",
    "    resume=resume,\n",
    "    batches_per_epoch=batches_per_epoch,\n",
    "    use_boxes = True,\n",
    "    use_positive_points = False,\n",
    ")\n",
    "myTrainer.run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
