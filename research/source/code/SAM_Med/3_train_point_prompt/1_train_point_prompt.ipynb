{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune a checkpoint of MedSAM on point prompted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path to a MedSAM checkpoint, we want to fine tune it on pre-processed data\n",
    "(subject to modifications specified by the paper and the transformation script). This will\n",
    "be done initially on an anatomy-specific level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argparse Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from time import time, sleep\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from segment_anything import sam_model_registry\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add the setup_data_vars function as we will need it to find the directory for the training data.\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, required=False, help='Whether to resume training using the latest checkpoint in the save_dir', metavar=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Inspired by orginal code from the MedSAM/extensions/point_prompt\n",
    "\n",
    "# 1. Add the anatomy on which we will fine-tune\n",
    "parser.add_argument(\n",
    "    '--anatomy',\n",
    "    type=str,\n",
    "    help='Anatomy on which to fine-tune the model. Note: this is case sensitive, please capitalize the first letter and accronyms such as CTVn or CTVp.',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 2. Path to the MedSAM checkpoint\n",
    "parser.add_argument(\n",
    "    '--checkpoint',\n",
    "    type=str,\n",
    "    help='Path to the checkpoint of the model to fine-tune',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 3. Path where we will be saving the checkpoints of the fine-tuned model\n",
    "parser.add_argument(\n",
    "    '--save_dir',\n",
    "    type=str,\n",
    "    help='Directory where the fine-tuned model will be saved',\n",
    "    required=True,\n",
    "    default=os.environ.get('MedSAM_finetuned')\n",
    ")\n",
    "\n",
    "# 4. Add the source directory for the data\n",
    "parser.add_argument(\n",
    "    '--img_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the images for the slices of the anatomy',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 5. Add the source directory for the gts\n",
    "parser.add_argument(\n",
    "    '--gt_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the ground truth masks for the slices of the anatomy',\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 6. Number of epochs for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--epochs',\n",
    "    type=int,\n",
    "    help='Number of epochs for the fine-tuning',\n",
    "    required=False,\n",
    "    default=300\n",
    ")\n",
    "\n",
    "# 7. Batch size for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    help='Batch size for the fine-tuning',\n",
    "    required=False,\n",
    "    default=8\n",
    ")\n",
    "\n",
    "# 8. Learning rate for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    help='Learning rate for the fine-tuning',\n",
    "    required=False,\n",
    "    default=0.00005\n",
    ")\n",
    "\n",
    "# 9. Number of workers for the data loader\n",
    "parser.add_argument(\n",
    "    '--num_workers',\n",
    "    type=int,\n",
    "    help='Number of workers for the data loader',\n",
    "    required=False,\n",
    "    default=16\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--weight_decay',\n",
    "    type=float,\n",
    "    help='Weight decay for the optimizer',\n",
    "    required=False,\n",
    "    default=0.01\n",
    ")\n",
    "\n",
    "# 11. Resume checkpoint\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    type=bool,\n",
    "    help='Whether to resume training using the latest checkpoint in the save_dir',\n",
    "    required=False,\n",
    "    default=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "# Suppose for now we get the following set of required arguments:\n",
    "args = parser.parse_args([\n",
    "    '--anatomy', 'Bladder',\n",
    "    '--checkpoint', os.path.join(os.environ['PROJECT_DIR'], 'models', 'MedSAM', 'work_dir', 'MedSAM', 'medsam_vit_b.pth'),\n",
    "    '--save_dir', os.path.join(os.environ['MedSAM_finetuned']),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy = args.anatomy\n",
    "checkpoint_path = args.checkpoint\n",
    "save_dir = args.save_dir\n",
    "img_dir = args.img_dir\n",
    "gt_dir = args.gt_dir\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "num_workers = args.num_workers\n",
    "weight_decay = args.weight_decay\n",
    "resume = args.resume\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_dir is None:\n",
    "    img_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'imgs')\n",
    "if gt_dir is None:\n",
    "    gt_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'gts', anatomy)\n",
    "\n",
    "save_dir = os.path.join(save_dir, anatomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id_from_file_name_regex = r'.*_(\\d+).*'\n",
    "slice_id_from_file_name_regex = r'.*-(\\d+).*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted Dataset class from ../2_no_finetuning/MEDSAM_helper_functions.py\n",
    "class SAM_Dataset(Dataset):\n",
    "    \"\"\"A torch dataset for delivering slices of any axis to a medsam model.\"\"\"\n",
    "\n",
    "    def __init__(self, img_path, gt_path, id_split, data_aug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_path (string): Path to the directory containing the images\n",
    "            gt_path (string): Path to the directory containing the ground truth masks\n",
    "            id_split (list): List of image ids to include in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_img_path = img_path\n",
    "        self.root_gt_path = gt_path\n",
    "        self.id_split = id_split\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        # Assume that axese 0 1 and 2 have been processed.\n",
    "        filter_fn = lambda x : x.endswith('.npy') and int(re.search(image_id_from_file_name_regex, x).group(1)) in id_split\n",
    "        self.axis0_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis0'))))\n",
    "        self.axis1_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis1'))))\n",
    "        self.axis2_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis2'))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.axis0_imgs) + len(self.axis1_imgs) + len(self.axis2_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self.__len__(), f\"Index {idx} is out of range for dataset of size {self.__len__()}\"\n",
    "\n",
    "        # Fetch the image and ground truth mask. For safety, we index the items around the\n",
    "        # ground truth masks, so that if for some reason the images are misaligned we will\n",
    "        # guarantee that we will fetch the correct image\n",
    "\n",
    "        if idx < len(self.axis0_imgs):\n",
    "            axis, gt_name = 0, self.axis0_imgs[idx]\n",
    "        elif idx < len(self.axis0_imgs) + len(self.axis1_imgs):\n",
    "            axis, gt_name = 1, self.axis1_imgs[idx - len(self.axis0_imgs)]\n",
    "        else:\n",
    "            axis, gt_name = 2, self.axis2_imgs[idx - len(self.axis0_imgs) - len(self.axis1_imgs)]\n",
    "\n",
    "        image_id = int(re.search(image_id_from_file_name_regex, gt_name).group(1))\n",
    "        slice_id = int(re.search(slice_id_from_file_name_regex, gt_name).group(1))\n",
    "\n",
    "        img_name = f'CT_zzAMLART_{image_id:03d}-{slice_id:03d}.npy'\n",
    "        \n",
    "        # Load the image and ground truth mask\n",
    "\n",
    "        img_path = os.path.join(self.root_img_path, f'axis{axis}', img_name)\n",
    "        gt_path = os.path.join(self.root_gt_path, f'axis{axis}', gt_name)\n",
    "\n",
    "        img = np.load(img_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "        gt = np.load(gt_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "\n",
    "        # Pre-process where necessary\n",
    "\n",
    "        img = np.transpose(img, (2, 0, 1)) # (C, H, W)\n",
    "        assert np.max(img) <= 1. and np.min(img) >= 0., 'image should be normalized to [0, 1]'\n",
    "\n",
    "        # add data augmentation: random fliplr and random flipud\n",
    "        if self.data_aug:\n",
    "            if random.random() > 0.5:\n",
    "                img = np.ascontiguousarray(np.flip(img, axis=-1))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-1))\n",
    "            if random.random() > 0.5:\n",
    "                img = np.ascontiguousarray(np.flip(img, axis=-2))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-2))\n",
    "        \n",
    "        # Select a random point. We will use this point to guide the model to segment the\n",
    "        # anatomy. The idea is that we want to select the center of this shape with\n",
    "        # greater probability than the outside of the shape.\n",
    "        gt = np.uint8(gt > 0)\n",
    "        y_indices, x_indices = np.where(gt > 0)\n",
    "\n",
    "        # Calculate the centroid of the segmentation\n",
    "        centroid_x = np.mean(x_indices)\n",
    "        centroid_y = np.mean(y_indices)\n",
    "\n",
    "        # Calculate distances of each point from the centroid\n",
    "        distances = np.sqrt((x_indices - centroid_x)**2 + (y_indices - centroid_y)**2)\n",
    "\n",
    "        # Invert the distances to get higher probabilities for points closer to the\n",
    "        # centroid\n",
    "        inverse_distances = 1 / (distances + 1e-6)  # adding a small value to avoid division by zero\n",
    "\n",
    "        # Normalize the probabilities\n",
    "        probabilities = inverse_distances / np.sum(inverse_distances)\n",
    "\n",
    "        # Sample a point based on the calculated probabilities\n",
    "        index = np.random.choice(len(x_indices), p=probabilities)\n",
    "        x_point = x_indices[index]\n",
    "        y_point = y_indices[index]\n",
    "\n",
    "        coords = np.array([x_point, y_point])\n",
    "\n",
    "        # The output of the model is 256x256, and it is easier to reason about\n",
    "        # constricting an image, rather than expanding the output back ot 1024x1024\n",
    "\n",
    "        gt = cv2.resize(\n",
    "            gt,\n",
    "            (256, 256),\n",
    "            interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(img).float(),\n",
    "            \"gt2D\": torch.tensor(gt[None, :,:]).long(),\n",
    "            \"coords\": torch.tensor(coords[None, ...]).float(),\n",
    "            \"image_name\": img_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test to see if the points are being generated correctly and transformations are also ok\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     from torchvision.utils import make_grid\n",
    "#     import torch.nn.functional as F\n",
    "\n",
    "#     experimental_datset = SAM_Dataset(img_dir, gt_dir, [1,2,3,4,5], data_aug=False)\n",
    "#     dataloader = torch.utils.data.DataLoader(experimental_datset, batch_size=16, shuffle=True)\n",
    "\n",
    "#     # Get a batch of examples\n",
    "#     batch = next(iter(dataloader))\n",
    "\n",
    "#     images = F.interpolate(batch['image'], size=(256, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "#     grid_imgs = make_grid(images, nrow=4, padding=0)\n",
    "#     grid_gts = make_grid(batch['gt2D'].float(), nrow=4, padding=0)\n",
    "#     gts_mask = (grid_gts.sum(dim=0) > 0).float()\n",
    "\n",
    "#     plt.figure(figsize=(30, 30))\n",
    "#     plt.imshow(grid_imgs.permute(1, 2, 0))\n",
    "#     plt.imshow(gts_mask, alpha=gts_mask, cmap='viridis')\n",
    "\n",
    "#     shift_x = 0\n",
    "#     shift_y = -256\n",
    "#     for i in range(16):\n",
    "\n",
    "#         shift_y = shift_y + 256 if i % 4 == 0 else shift_y\n",
    "#         shift_x = shift_x + 256 if i % 4 != 0 else 0\n",
    "\n",
    "#         coord = batch['coords'][i].squeeze().numpy()\n",
    "#         x, y = coord[0], coord[1]\n",
    "#         x, y = x * 256 / 1024 + shift_x, y * 256 / 1024 + shift_y\n",
    "#         plt.scatter(x, y, c='r', s=60)\n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Fine-Tuning nn Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAM(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_encoder, \n",
    "                mask_decoder,\n",
    "                prompt_encoder,\n",
    "                freeze_image_encoder=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "        # freeze prompt encoder\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        if self.freeze_image_encoder:\n",
    "            for param in self.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, point_prompt):\n",
    "\n",
    "        # do not compute gradients for pretrained img encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
    "            # not need to convert box to 1024x1024 grid\n",
    "            # bbox is already in 1024x1024\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=point_prompt,\n",
    "                boxes=None,\n",
    "                masks=None,\n",
    "            )\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          ) # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointHandler():\n",
    "    def __init__(\n",
    "            self,\n",
    "            save_dir: str,\n",
    "            checkpoint_path: str,\n",
    "            *args,\n",
    "            **kwargs\n",
    "    ):\n",
    "        self.save_dir = save_dir\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        \n",
    "        self.lr = kwargs['lr']\n",
    "        self.weight_decay = kwargs['weight_decay']\n",
    "\n",
    "    def save_checkpoint(self, model, optimizer, epoch, epoch_loss, best_loss, final):\n",
    "        \"\"\"\n",
    "        Will be guaranteed to save the checkpoint in the save_dir location. If the model\n",
    "        is at peak performance, it saves it under 'checkpoint_best' otherwise, by default\n",
    "        it is 'checkpoint_latest'. If specified, the checkpoint is saved under its final\n",
    "        form and thus replaces 'checkpoint_latest' -> 'checkpoint_final'.\n",
    "        \"\"\"\n",
    "\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"epochs\": epoch,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"loss\": epoch_loss,\n",
    "            \"best_loss\": best_loss\n",
    "        }\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_best.pth'))\n",
    "\n",
    "        if final:\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_final.pth'))\n",
    "            os.remove(os.path.join(self.checkpoint_path, 'checkpoint_latest.pth'))\n",
    "        else:\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        return best_loss\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Loads a checkpoint from the save_dir directory. Assumes this function will be called in the context of continuing training that hasn't finished yet.\n",
    "        \"\"\"\n",
    "        model, optimizer = self.load_base_checkpoint()\n",
    "        assert self.checkpoint_exists(), \"did you check that checkpoint_latest exists?\"\n",
    "        checkpoint = torch.load(os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        epoch = checkpoint['epochs']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "\n",
    "        return model, optimizer, epoch, best_loss\n",
    "\n",
    "    def load_base_checkpoint(self):\n",
    "        sam_model = sam_model_registry[\"vit_b\"](checkpoint=self.checkpoint_path)\n",
    "\n",
    "        medsam_model = MedSAM(\n",
    "            image_encoder = sam_model.image_encoder,\n",
    "            mask_decoder = sam_model.mask_decoder,\n",
    "            prompt_encoder = sam_model.prompt_encoder,\n",
    "            freeze_image_encoder = True\n",
    "        )\n",
    "        medsam_model = medsam_model.to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            medsam_model.mask_decoder.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        return medsam_model, optimizer\n",
    "\n",
    "    def checkpoint_exists(self):\n",
    "        return os.path.exists(os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "    def final_checkpoint_exists(self):\n",
    "        return os.path.exists(os.path.join(self.save_dir, 'checkpoint_final.pth')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderHandler():\n",
    "    def __init__(self, \n",
    "            save_dir, \n",
    "            image_dir, \n",
    "            gt_dir,\n",
    "            batch_size,\n",
    "            num_workers,\n",
    "            data_aug : bool,\n",
    "            training_split = 0.8, \n",
    "            validation_split = 0.2):\n",
    "\n",
    "        # Where to save the data splits\n",
    "        self.save_dir = save_dir\n",
    "        \n",
    "        # Where to get image and ground truth info for training\n",
    "        self.image_dir = image_dir\n",
    "        self.gt_dir = gt_dir\n",
    "\n",
    "        # DataLoader parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        # Splits for validation and training\n",
    "        assert training_split + validation_split == 1\n",
    "        self.training_split = training_split\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        # Final dataLoaders\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "\n",
    "    def save_splits_to_json(self, training_image_ids, validation_image_ids):\n",
    "\n",
    "        data = {\n",
    "            \"training_image_ids\": list(training_image_ids),\n",
    "            \"validation_image_ids\": list(validation_image_ids)\n",
    "        }\n",
    "        with open(os.path.join(self.save_dir, 'data_splits.json'), 'w') as json_file:\n",
    "            json.dump(data, json_file)\n",
    "\n",
    "    def load_split_from_json(self):\n",
    "\n",
    "        with open(os.path.join(self.save_dir, 'data_splits.json'), 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        self.training_split = set(data[\"training_image_ids\"])\n",
    "        self.validation_split = set(data[\"validation_image_ids\"])\n",
    "\n",
    "    def try_setup_data_split_from_save_with_fallback(self):\n",
    "\n",
    "        # either load datasplits or setup anew\n",
    "        if os.path.exists(os.path.join(self.save_dir, 'data_Splits.json')):\n",
    "            self.load_split_from_json()\n",
    "        else:\n",
    "            self.setup_new_data_splits()\n",
    "\n",
    "    def setup_new_data_splits(self):\n",
    "        \"\"\"Setup the data splits from scratch and save\"\"\"\n",
    "\n",
    "        # get the image ids that have been processed. Use gt dir as reference\n",
    "        axis0_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis0'))))\n",
    "        axis1_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis1'))))\n",
    "        axis2_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis2'))))\n",
    "\n",
    "        if not axis0_slices == axis1_slices == axis2_slices:\n",
    "            print('[WARNING]: The slices for the anatomy are not consistent across the three axes. Some axese are missing data, please check')\n",
    "        \n",
    "        # Split the data into training and validation\n",
    "        self.training_image_ids = random.sample(list(axis0_slices), int(len(axis0_slices) * self.training_split))\n",
    "        self.validation_image_ids = list(set(axis0_slices) - set(self.training_image_ids))\n",
    "        assert set.intersection(set(self.training_image_ids), set(self.validation_image_ids)).__len__() == 0, 'Training and Validation sets are not disjoint'\n",
    "\n",
    "        # Save the splits in a json file\n",
    "        self.save_splits_to_json(self.training_image_ids, self.validation_image_ids)\n",
    "\n",
    "    def setup_dataloaders(self):\n",
    "        \n",
    "        self.training_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.training_image_ids, data_aug = self.data_aug)\n",
    "        self.validation_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.validation_image_ids, data_aug = self.data_aug)\n",
    "        \n",
    "        # Quick check\n",
    "        assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.validation_dataset.axis0_imgs)) == set(self.validation_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "        assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.training_dataset.axis0_imgs)) == set(self.training_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "\n",
    "        self.train_loader = DataLoader(self.training_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(self.validation_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggingHandler():\n",
    "    def __init__(self, save_dir):\n",
    "        # idea, just have a dictionary with the stats that you save at the end of each\n",
    "        # epoch. You save it down to json at the end. On start, you create a new logging\n",
    "        # text file where you have info about the running script. IF there doesn't exist a\n",
    "        # file 'checkpoint_latest_stats.json' then create one. This is where you will load\n",
    "        # up the stats from the previous run. If there is no such file, then you start\n",
    "        # from scratch.\n",
    "\n",
    "        self.per_epoch_stats = dict()\n",
    "        self.save_dir = save_dir\n",
    "        self.curr_epoch = -1\n",
    "        self.curr_epoch_stats = dict()\n",
    "\n",
    "\n",
    "    def save_stats(self):\n",
    "        with open(os.path.join(self.save_dir, 'checkpoint_latest_stats.json'), 'w') as json_file:\n",
    "            json.dump(self.per_epoch_stats, json_file)\n",
    "\n",
    "    def load_stats(self):\n",
    "        with open(os.path.join(self.save_dir, 'checkpoint_latest_stats.json'), 'r') as json_file:\n",
    "            self.per_epoch_stats = json.load(json_file)\n",
    "\n",
    "    def log_metric(self, key, value, epoch):\n",
    "        assert self.curr_epoch == epoch\n",
    "        if key not in self.curr_epoch_stats:\n",
    "            self.curr_epoch_stats[key] = [value]\n",
    "        else:\n",
    "            self.curr_epoch_stats[key].append(value)\n",
    "\n",
    "    def log(self, line):\n",
    "        with open(self.loggerName, 'a') as f:\n",
    "            f.write(f'{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} - {line}\\n')\n",
    "\n",
    "    def start_new_epoch(self, epoch):\n",
    "        if self.curr_epoch >= 0:\n",
    "            self.save_stats()\n",
    "            self.per_epoch_stats[self.curr_epoch] = self.curr_epoch_stats\n",
    "        self.curr_epoch = epoch\n",
    "        self.curr_epoch_stats = {}\n",
    "\n",
    "        self.curr_epoch_stats['epoch_start'] = time()\n",
    "\n",
    "    def end_current_epoch(self, epoch):\n",
    "        assert epoch == self.curr_epoch\n",
    "        end_time = time()\n",
    "        self.curr_epoch_stats['epoch_end'] = end_time\n",
    "        self.curr_epoch_stats['epoch_time'] = self.curr_epoch_stats['epoch_start'] - end_time\n",
    "        self.save_stats()\n",
    "        self.per_epoch_stats[self.curr_epoch] = self.curr_epoch_stats\n",
    "\n",
    "    def setup_logger(self):\n",
    "        # check if the file exists\n",
    "        if os.path.exists(os.path.join(save_dir, 'checkpoint_latest_stats.json')):\n",
    "            self.load_stats()\n",
    "        else:\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            self.save_stats()\n",
    "\n",
    "        # create a logging file based on datetime\n",
    "        now = datetime.now()\n",
    "        self.loggerName = os.path.join(self.save_dir, f'training_{now.strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
    "        \n",
    "        with open(self.loggerName, 'w') as f:\n",
    "            f.write(f\"Initialised Logger at {now.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loggingHandler = LoggingHandler(save_dir)\n",
    "dataloaderHandler = DataLoaderHandler(save_dir, img_dir, gt_dir, batch_size, num_workers, True)\n",
    "checkpointHandler = CheckpointHandler(save_dir, checkpoint_path, lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAMTrainer(object):\n",
    "    def __init__(self, loggingHandler, dataloaderHandler, checkpointHandler, *args, **kwargs):\n",
    "        self.loggingHandler = loggingHandler\n",
    "        self.dataloaderHandler = dataloaderHandler\n",
    "        self.checkpointHandler = checkpointHandler\n",
    "\n",
    "        self.epochs = kwargs['epochs']\n",
    "        self.resume = kwargs['resume']\n",
    "\n",
    "        self.dice_loss_fn = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "        self.ce_loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def run_training(self):\n",
    "        self.on_train_start()\n",
    "\n",
    "        for epoch in range(self.current_epoch, self.epochs):\n",
    "            self.on_epoch_start(epoch)\n",
    "\n",
    "            pbar = tqdm(self.dataloaderHandler.train_loader)\n",
    "            for batch_id, batch in enumerate(pbar):\n",
    "                # batch = next(iter(self.dataloaderHandler.train_loader))\n",
    "                dice_loss, ce_loss = self.train_step(batch_id, batch)            \n",
    "                pbar.set_description(f\"Epoch {epoch} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, loss: {dice_loss + ce_loss:.4f}\")\n",
    "\n",
    "            self.on_epoch_end(epoch)\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     self.on_validation_epoch_start()\n",
    "            #     val_outputs = []\n",
    "            #     for batch_id in range(self.num_val_iterations_per_epoch):\n",
    "            #         val_outputs.append(self.validation_step(next(self.dataloader_val)))\n",
    "            #     self.on_validation_epoch_end(val_outputs)\n",
    "\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.loggingHandler.setup_logger()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        self.loggingHandler.log('Setting up dataloaders')\n",
    "        self.dataloaderHandler.try_setup_data_split_from_save_with_fallback()\n",
    "        self.dataloaderHandler.setup_dataloaders()\n",
    "\n",
    "        self.loggingHandler.log('Setting up models')\n",
    "        self.model, self.optimizer, self.current_epoch, self.best_loss = None, None, 0, 1e10\n",
    "        if self.checkpointHandler.final_checkpoint_exists() and self.resume:\n",
    "            self.loggingHandler.log('We have already trained this model to completion')\n",
    "            exit(1)\n",
    "        elif self.checkpointHandler.checkpoint_exists() and self.resume:\n",
    "            self.loggingHandler.log('Resume is true and a checkpoint exists, we resume')\n",
    "            self.model, self.optimizer, self.current_epoch, self.best_loss = self.checkpointHandler.load_checkpoint()\n",
    "            self.loggingHandler.log('Resuming at epoch: ', self.current_epoch)\n",
    "        else:\n",
    "            self.loggingHandler.log('Setting up a fresh start model')\n",
    "            self.model, self.optimizer = self.checkpointHandler.load_base_checkpoint()\n",
    "\n",
    "    def on_epoch_start(self, epoch):\n",
    "        self.loggingHandler.log('Setting up a new epoch for the logger')\n",
    "        self.loggingHandler.start_new_epoch(epoch)\n",
    "        self.current_epoch = epoch\n",
    "\n",
    "    def train_step(self, step, batch):\n",
    "        self.loggingHandler.log(f'Starting epoch {self.current_epoch} and step {step} out of {len(self.dataloaderHandler.train_loader)}')\n",
    "\n",
    "        # Get data\n",
    "        image = batch[\"image\"].to(device)\n",
    "        gt2D = batch[\"gt2D\"].to(device)\n",
    "        coords_torch = batch[\"coords\"].to(device) # (B, 2)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        labels_torch = torch.ones(coords_torch.shape[0]).long() # (B,)\n",
    "        labels_torch = labels_torch.unsqueeze(1) # (B, 1)\n",
    "        coords_torch, labels_torch = coords_torch.to(device), labels_torch.to(device)\n",
    "        point_prompt = (coords_torch, labels_torch)\n",
    "        medsam_lite_pred = self.model(image, point_prompt)\n",
    "\n",
    "        dice_loss = self.dice_loss_fn(medsam_lite_pred, gt2D)\n",
    "        ce_loss = self.ce_loss_fn(medsam_lite_pred, gt2D.float())\n",
    "\n",
    "        loss = dice_loss + ce_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        self.loggingHandler.log(f'Received dice: {dice_loss.item()} with cross entropy loss: {ce_loss.item()}')\n",
    "        self.loggingHandler.log_metric('dice_loss', dice_loss.item(), self.current_epoch)\n",
    "        self.loggingHandler.log_metric('ce_loss', ce_loss.item(), self.current_epoch)\n",
    "\n",
    "        return dice_loss.item(), ce_loss.item()\n",
    "\n",
    "    def on_epoch_end(self, epoch):\n",
    "        self.loggingHandler.end_current_epoch(epoch)\n",
    "\n",
    "    def on_validation_epoch_start(self, start):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self, val_outputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 at 2024-05-18 15:46:01, loss: 0.0538:  69%|██████▊   | 1445/2103 [41:05<11:59,  1.09s/it] "
     ]
    }
   ],
   "source": [
    "myTrainer = MedSAMTrainer(\n",
    "    loggingHandler, \n",
    "    dataloaderHandler, \n",
    "    checkpointHandler,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    num_workers=num_workers,\n",
    "    resume=resume\n",
    ").run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MedSAMTrainer(object):\n",
    "#     def __init__(self\n",
    "#                 , anatomy\n",
    "#                 , checkpoint_path\n",
    "#                 , save_dir\n",
    "#                 , image_dir\n",
    "#                 , gt_dir\n",
    "#                 , epochs\n",
    "#                 , batch_size\n",
    "#                 , lr\n",
    "#                 , num_workers\n",
    "#                 , weight_decay\n",
    "#                 , resume\n",
    "#                 , device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#                 , ):\n",
    "        \n",
    "#         self.anatomy = anatomy\n",
    "#         assert self.anatomy in ['CTVn', 'CTVp', 'Bladder', 'Anorectum', 'Uterus', 'Vagina']\n",
    "        \n",
    "#         self.image_dir = image_dir\n",
    "#         self.gt_dir = gt_dir\n",
    "#         assert os.path.exists(self.image_dir), 'Image Directory doesn\\'t exist.'\n",
    "#         assert os.path.exists(self.gt_dir), 'Ground Truth Directory doesn\\'t exist for the requested anatomy.'\n",
    "\n",
    "#         self.save_dir = save_dir\n",
    "#         self.checkpoint_path = checkpoint_path\n",
    "#         self.epochs = epochs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.lr = lr\n",
    "#         self.num_workers = num_workers\n",
    "#         self.weight_decay = weight_decay\n",
    "#         self.resume = resume\n",
    "#         device = device\n",
    "\n",
    "        \n",
    "\n",
    "#         # self.stat_logger = MedSAMLogger(self.save_dir)\n",
    "\n",
    "#     def run_training(self):\n",
    "#         \"\"\"Main Training Loop\"\"\"\n",
    "\n",
    "#         self.on_train_start()\n",
    "\n",
    "#         for epoch in range(self.start_epoch, self.epochs):\n",
    "#             # <Training>\n",
    "#             self.on_epoch_start(epoch)\n",
    "\n",
    "#             pbar = tqdm(self.train_loader)\n",
    "#             for step, batch in enumerate(pbar):\n",
    "#                 dice_loss, ce_loss = self.train_step(step, batch)\n",
    "\n",
    "#                 # self.stat_logger.log_train(dice_loss, ce_loss)\n",
    "\n",
    "#                 pbar.set_description(f\"Epoch {epoch} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, loss: {dice_loss + ce_loss:.4f}\")\n",
    "\n",
    "#             self.on_epoch_end()\n",
    "#             # </Training>\n",
    "\n",
    "#             # # <Validation>\n",
    "#             # with torch.no_grad():\n",
    "#             #     self.medsam_model.eval()\n",
    "#             #     for step, batch in enumerate(self.val_loader):\n",
    "#             #         image = batch[\"image\"].to(device)\n",
    "#             #         gt2D = batch[\"gt2D\"].to(device)\n",
    "#             #         coords_torch = batch[\"coords\"].to(device)\n",
    "#             # # </Validation>\n",
    "\n",
    "#         self.on_train_end()\n",
    "\n",
    "#     def train_step(self, step, batch):\n",
    "#         logging.debug(f'Starting epoch {self.current_epoch} step {step} at time {datetime.now()}')\n",
    "\n",
    "#         # Get data\n",
    "#         logging.debug('getting the data and setting to device')\n",
    "#         image = batch[\"image\"].to(device)\n",
    "#         gt2D = batch[\"gt2D\"].to(device)\n",
    "#         coords_torch = batch[\"coords\"].to(device) # (B, 2)\n",
    "\n",
    "#         logging.debug('Performing forward pass')\n",
    "#         self.optimizer.zero_grad()\n",
    "#         labels_torch = torch.ones(coords_torch.shape[0]).long() # (B,)\n",
    "#         labels_torch = labels_torch.unsqueeze(1) # (B, 1)\n",
    "#         coords_torch, labels_torch = coords_torch.to(device), labels_torch.to(device)\n",
    "#         point_prompt = (coords_torch, labels_torch)\n",
    "#         medsam_lite_pred = self.medsam_model(image, point_prompt)\n",
    "\n",
    "#         logging.debug('Calculating loss')\n",
    "#         dice_loss = self.dice_loss(medsam_lite_pred, gt2D)\n",
    "#         ce_loss = self.ce_loss(medsam_lite_pred, gt2D.float())\n",
    "\n",
    "#         loss = dice_loss + ce_loss\n",
    "#         logging.debug(f'Loss calculated: {loss.item()}')\n",
    "        \n",
    "#         logging.debug('Performing step in optimizer and propagating loss backgwards')\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         self.optimizer.zero_grad()\n",
    "\n",
    "#         # self.stat_logger.log_train(dice_loss.item(), ce_loss.item())\n",
    "\n",
    "#         return dice_loss.item(), ce_loss.item()\n",
    "    \n",
    "#     def on_train_end(self):\n",
    "#         logging.debug(f'Reached the end of training! Epochs: {self.epochs}')\n",
    "#         self.save_checkpoint(self.current_epoch, self.epoch_loss, final=True)\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         # epoch_time, epoch_los_reduced = self.stat_logger.end_of_epoch(self.current_epoch)\n",
    "\n",
    "#         self.epoch_end_time = time()\n",
    "#         self.epoch_time.append(self.epoch_end_time - self.epoch_start_time)\n",
    "#         logging.debug(f'Reached the end of epoch {self.current_epoch}, it took {self.epoch_end_time - self.epoch_start_time} seconds')\n",
    "\n",
    "#         epoch_loss_reduced = sum(self.epoch_loss) / len(self.epoch_loss)\n",
    "\n",
    "#         self.save_checkpoint(self.current_epoch, epoch_loss_reduced)\n",
    "\n",
    "#         # Plot the progress\n",
    "\n",
    "#         # self.print_to_log_file('train_loss', np.round(logging.my_fantastic_logging['train_losses'][-1], decimals=4))\n",
    "#         # self.print_to_log_file('val_loss', np.round(logging.my_fantastic_logging['val_losses'][-1], decimals=4))\n",
    "#         # self.print_to_log_file('Pseudo dice', [np.round(i, decimals=4) for i in\n",
    "#         #                                        logging.my_fantastic_logging['dice_per_class_or_region'][-1]])\n",
    "#         # self.print_to_log_file(\n",
    "#         #     f\"Epoch time: {np.round(logging.my_fantastic_logging['epoch_end_timestamps'][-1] - logging.my_fantastic_logging['epoch_start_timestamps'][-1], decimals=2)} s\")\n",
    "\n",
    "#         # logging.plot_progress_png(self.save_dir)\n",
    "\n",
    "#     def on_train_start(self):\n",
    "#         \"\"\"Sets up the training environment\"\"\"\n",
    "#         logging.debug('starting training environment')\n",
    "\n",
    "#         # empty cuda cache\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         # create save_dir if it doesn't exist yet\n",
    "#         os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "#         # set up logger to a new instance\n",
    "#         # logging = MedSAMLogger()\n",
    "\n",
    "#         # load the previous checkpoint if it exists and resume is True. Otherwise, load\n",
    "#         # the medSAM model from which we train. \n",
    "#         self._first_run_setup()\n",
    "#         if (not self._maybe_load_checkpoint()):\n",
    "#             self._setup_data_splits()\n",
    "\n",
    "#         # set up the dataloaders\n",
    "#         self._get_dataloaders()\n",
    "\n",
    "#         # set up loss functions\n",
    "#         logging.debug('Setting up loss functions')\n",
    "#         self.dice_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "#         self.ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "#     def on_epoch_start(self, epoch):        \n",
    "#         self.medsam_model.train()\n",
    "#         self.current_epoch = epoch\n",
    "#         self.epoch_start_time = datetime.now()\n",
    "#         self.epoch_loss = [1e10 for _ in range(len(self.train_loader))]\n",
    "\n",
    "#         # self.stat_logger.start_epoch(epoch, epoch_start_time)\n",
    "\n",
    "#         logging.debug(f'>> Starting epoch {epoch} at time {self.epoch_start_time}')\n",
    "\n",
    "#     def _maybe_load_checkpoint(self):\n",
    "#         \"\"\"\n",
    "#         Populates the variables IF a checkpoint has been found\n",
    "#         - checkpoint\n",
    "#         - medsam_model\n",
    "#         - optimizer\n",
    "#         - start_epoch\n",
    "#         - best_loss\n",
    "#         - training_split\n",
    "#         - validation_split\n",
    "#         \"\"\"\n",
    "#         logging.debug('Attempting to find and load checkpoint from save directory')\n",
    "#         if not self.resume:\n",
    "#             logging.debug('Resume was set to false, skipping checkpoint loading.')\n",
    "#             return False\n",
    "\n",
    "#         if len([f for f in os.listdir(save_dir) if f == 'checkpoint_latest.pth']) == 0:\n",
    "#             logging.debug('No checkpoint found in the save directory')\n",
    "#             return False\n",
    "        \n",
    "#         # Load model details\n",
    "#         logging.debug('found and loading checkpoint from checkpoint_latest.pth')\n",
    "#         self.checkpoint = torch.load(os.path.join(save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "#         self.medsam_model.load_state_dict(self.checkpoint[\"model\"])\n",
    "#         self.optimizer.load_state_dict(self.checkpoint[\"optimizer\"])\n",
    "#         self.start_epoch = self.checkpoint[\"epoch\"] + 1\n",
    "#         self.best_loss = self.checkpoint[\"best_loss\"]\n",
    "#         logging.debug(f\"Loaded checkpoint from epoch {self.start_epoch}, best loss: {self.best_loss:.4f}\")\n",
    "\n",
    "#         # Get Data Split\n",
    "#         try:\n",
    "#             self._load_split_from_json()\n",
    "#         except FileNotFoundError as e:\n",
    "#             logging.debug('[WARNING] a checkpoint was found, but a split file was not.')\n",
    "#             logging.debug('          a new data split will be generated with potential training bias towards the validaiton split.')\n",
    "#             self._setup_data_splits()\n",
    "\n",
    "#         return True  \n",
    "\n",
    "#     def _first_run_setup(self):\n",
    "#         logging.debug('running setup for sam model at base checkpoint')\n",
    "#         sam_model = sam_model_registry[\"vit_b\"](checkpoint=self.checkpoint_path)\n",
    "#         self.medsam_model = MedSAM(\n",
    "#             image_encoder = sam_model.image_encoder,\n",
    "#             mask_decoder = sam_model.mask_decoder,\n",
    "#             prompt_encoder = sam_model.prompt_encoder,\n",
    "#             freeze_image_encoder = True\n",
    "#         )\n",
    "#         self.medsam_model = self.medsam_model.to(device)\n",
    "\n",
    "#         logging.debug('setting up optimizer for sam model at base checkpoint')\n",
    "#         self.optimizer = optim.AdamW(\n",
    "#             self.medsam_model.mask_decoder.parameters(),\n",
    "#             lr=self.lr,\n",
    "#             betas=(0.9, 0.999),\n",
    "#             eps=1e-08,\n",
    "#             weight_decay=self.weight_decay\n",
    "#         )\n",
    "\n",
    "#         self.start_epoch = 0\n",
    "#         self.best_loss = float('inf')\n",
    "\n",
    "#     def _save_splits_to_json(self, training_image_ids, validation_image_ids):\n",
    "#         logging.debug('Saving data splits to json')\n",
    "#         data = {\n",
    "#             \"training_image_ids\": list(training_image_ids),\n",
    "#             \"validation_image_ids\": list(validation_image_ids)\n",
    "#         }\n",
    "#         with open(os.path.join(self.save_dir, 'data_splits.json'), 'w') as json_file:\n",
    "#             json.dump(data, json_file)\n",
    "\n",
    "#     def _load_split_from_json(self):\n",
    "#         logging.debug('Loading the data split from save directory')\n",
    "#         with open(os.path.join(self.save_dir, 'data_splits.json'), 'r') as json_file:\n",
    "#             data = json.load(json_file)\n",
    "#         self.training_split = set(data[\"training_image_ids\"])\n",
    "#         self.validation_split = set(data[\"validation_image_ids\"])\n",
    "\n",
    "#     def _setup_data_splits(self, training_split=None, validation_split=None):\n",
    "#         \"\"\"Setup the data splits either from a known source or a new setup.\"\"\"\n",
    "#         logging.debug('Setting up the data splits for training and validation sets.')\n",
    "#         if training_split is not None and validation_split is not None:\n",
    "#             logging.debug('Existing splits found, setting them up.')\n",
    "#             self.training_split = training_split\n",
    "#             self.validation_split = validation_split\n",
    "#             return\n",
    "\n",
    "#         # get the image ids that have been processed. Use gt dir as reference\n",
    "#         axis0_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis0'))))\n",
    "#         axis1_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis1'))))\n",
    "#         axis2_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis2'))))\n",
    "\n",
    "#         if not axis0_slices == axis1_slices == axis2_slices:\n",
    "#             print('[WARNING]: The slices for the anatomy are not consistent across the three axes. Some axese are missing data, please check')\n",
    "\n",
    "#         training_split = 0.8\n",
    "#         validation_split = 1 - training_split\n",
    "\n",
    "#         # Split the data into training and validation\n",
    "#         self.training_image_ids = random.sample(list(axis0_slices), int(len(axis0_slices) * training_split))\n",
    "#         self.validation_image_ids = list(set(axis0_slices) - set(self.training_image_ids))\n",
    "#         assert set.intersection(set(self.training_image_ids), set(self.validation_image_ids)).__len__() == 0, 'Training and Validation sets are not disjoint'\n",
    "\n",
    "#         # Save the splits in a json file\n",
    "#         self._save_splits_to_json(self.training_image_ids, self.validation_image_ids)\n",
    "\n",
    "#     def _get_dataloaders(self):\n",
    "#         logging.debug('Setting up Datasets and DataLoaders using the splits.')\n",
    "#         self.training_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.training_image_ids)\n",
    "#         self.validation_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.validation_image_ids)\n",
    "        \n",
    "#         # Quick check\n",
    "#         assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.validation_dataset.axis0_imgs)) == set(self.validation_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "#         assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.training_dataset.axis0_imgs)) == set(self.training_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "\n",
    "#         self.train_loader = DataLoader(self.training_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "#         self.val_loader = DataLoader(self.validation_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MedSAMTrainer(\n",
    "    anatomy\n",
    "    , checkpoint_path\n",
    "    , save_dir\n",
    "    , img_dir\n",
    "    , gt_dir\n",
    "    , epochs\n",
    "    , batch_size\n",
    "    , lr\n",
    "    , num_workers\n",
    "    , weight_decay\n",
    "    , resume\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1052 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[241], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[239], line 51\u001b[0m, in \u001b[0;36mMedSAMTrainer.run_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m---> 51\u001b[0m     dice_loss, ce_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# self.stat_logger.log_train(dice_loss, ce_loss)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdice_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mce_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[239], line 86\u001b[0m, in \u001b[0;36mMedSAMTrainer.train_step\u001b[0;34m(self, step, batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m coords_torch, labels_torch \u001b[38;5;241m=\u001b[39m coords_torch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), labels_torch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     85\u001b[0m point_prompt \u001b[38;5;241m=\u001b[39m (coords_torch, labels_torch)\n\u001b[0;32m---> 86\u001b[0m medsam_lite_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmedsam_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalculating loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m dice_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdice_loss(medsam_lite_pred, gt2D)\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[235], line 26\u001b[0m, in \u001b[0;36mMedSAM.forward\u001b[0;34m(self, image, point_prompt)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, point_prompt):\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# do not compute gradients for pretrained img encoder and prompt encoder\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m         image_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, 256, 64, 64)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# not need to convert box to 1024x1024 grid\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# bbox is already in 1024x1024\u001b[39;00m\n\u001b[1;32m     29\u001b[0m         sparse_embeddings, dense_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_encoder(\n\u001b[1;32m     30\u001b[0m             points\u001b[38;5;241m=\u001b[39mpoint_prompt,\n\u001b[1;32m     31\u001b[0m             boxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m             masks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m         )\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/models/MedSAM/segment_anything/modeling/image_encoder.py:115\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    112\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 115\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/models/MedSAM/segment_anything/modeling/image_encoder.py:179\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    176\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    177\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 179\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/models/MedSAM/segment_anything/modeling/image_encoder.py:238\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# q, k, v with shape (B * nHead, H * W, C)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m, B \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H \u001b[38;5;241m*\u001b[39m W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 238\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[1;32m    241\u001b[0m     attn \u001b[38;5;241m=\u001b[39m add_decomposed_rel_pos(\n\u001b[1;32m    242\u001b[0m         attn, q, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w, (H, W), (H, W)\n\u001b[1;32m    243\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 GiB. GPU "
     ]
    }
   ],
   "source": [
    "trainer.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
