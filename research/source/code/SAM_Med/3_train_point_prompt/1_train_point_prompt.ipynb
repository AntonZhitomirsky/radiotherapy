{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune a checkpoint of MedSAM on point prompted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path to a MedSAM checkpoint, we want to fine tune it on pre-processed data\n",
    "(subject to modifications specified by the paper and the transformation script). This will\n",
    "be done initially on an anatomy-specific level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from time import time, sleep\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from segment_anything import sam_model_registry\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Add the setup_data_vars function as we will need it to find the directory for the training data.\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, required=False, help='Whether to resume training using the latest checkpoint in the save_dir', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Inspired by orginal code from the MedSAM/extensions/point_prompt\n",
    "\n",
    "# 1. Add the anatomy on which we will fine-tune\n",
    "parser.add_argument(\n",
    "    '--anatomy',\n",
    "    type=str,\n",
    "    help='Anatomy on which to fine-tune the model. Note: this is case sensitive, please capitalize the first letter and accronyms such as CTVn or CTVp.',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 2. Path to the MedSAM checkpoint\n",
    "parser.add_argument(\n",
    "    '--checkpoint',\n",
    "    type=str,\n",
    "    help='Path to the checkpoint of the model to fine-tune',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 3. Path where we will be saving the checkpoints of the fine-tuned model\n",
    "parser.add_argument(\n",
    "    '--save_dir',\n",
    "    type=str,\n",
    "    help='Directory where the fine-tuned model will be saved',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 4. Add the source directory for the data\n",
    "parser.add_argument(\n",
    "    '--img_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the images for the slices of the anatomy',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 5. Add the source directory for the gts\n",
    "parser.add_argument(\n",
    "    '--gt_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the ground truth masks for the slices of the anatomy',\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 6. Number of epochs for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--epochs',\n",
    "    type=int,\n",
    "    help='Number of epochs for the fine-tuning',\n",
    "    required=False,\n",
    "    default=300\n",
    ")\n",
    "\n",
    "# 7. Batch size for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    help='Batch size for the fine-tuning',\n",
    "    required=False,\n",
    "    default=4\n",
    ")\n",
    "\n",
    "# 8. Learning rate for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    help='Learning rate for the fine-tuning',\n",
    "    required=False,\n",
    "    default=0.00005\n",
    ")\n",
    "\n",
    "# 9. Number of workers for the data loader\n",
    "parser.add_argument(\n",
    "    '--num_workers',\n",
    "    type=int,\n",
    "    help='Number of workers for the data loader',\n",
    "    required=False,\n",
    "    default=4\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    '--weight_decay',\n",
    "    type=float,\n",
    "    help='Weight decay for the optimizer',\n",
    "    required=False,\n",
    "    default=0.01\n",
    ")\n",
    "\n",
    "# 11. Resume checkpoint\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    type=bool,\n",
    "    help='Whether to resume training using the latest checkpoint in the save_dir',\n",
    "    required=False,\n",
    "    default=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "# Suppose for now we get the following set of required arguments:\n",
    "args = parser.parse_args([\n",
    "    '--anatomy', 'CTVn',\n",
    "    '--checkpoint', os.path.join(os.environ['PROJECT_DIR'], 'models', 'MedSAM', 'work_dir', 'MedSAM', 'medsam_vit_b.pth'),\n",
    "    '--save_dir', os.path.join(os.environ['MedSAM_finetuned'], 'CTVn')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy = args.anatomy\n",
    "checkpoint_path = args.checkpoint\n",
    "save_dir = args.save_dir\n",
    "img_dir = args.img_dir\n",
    "gt_dir = args.gt_dir\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "num_workers = args.num_workers\n",
    "weight_decay = args.weight_decay\n",
    "resume = args.resume\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img_dir is None:\n",
    "    img_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'imgs')\n",
    "if gt_dir is None:\n",
    "    gt_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'gts', anatomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id_from_file_name_regex = r'.*_(\\d+).*'\n",
    "slice_id_from_file_name_regex = r'.*-(\\d+).*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted Dataset class from ../2_no_finetuning/MEDSAM_helper_functions.py\n",
    "class SAM_Dataset(Dataset):\n",
    "    \"\"\"A torch dataset for delivering slices of any axis to a medsam model.\"\"\"\n",
    "\n",
    "    def __init__(self, img_path, gt_path, id_split, data_aug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_path (string): Path to the directory containing the images\n",
    "            gt_path (string): Path to the directory containing the ground truth masks\n",
    "            id_split (list): List of image ids to include in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_img_path = img_path\n",
    "        self.root_gt_path = gt_path\n",
    "        self.id_split = id_split\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        # Assume that axese 0 1 and 2 have been processed.\n",
    "        filter_fn = lambda x : x.endswith('.npy') and int(re.search(image_id_from_file_name_regex, x).group(1)) in id_split\n",
    "        self.axis0_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis0'))))\n",
    "        self.axis1_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis1'))))\n",
    "        self.axis2_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis2'))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.axis0_imgs) + len(self.axis1_imgs) + len(self.axis2_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self.__len__(), f\"Index {idx} is out of range for dataset of size {self.__len__()}\"\n",
    "\n",
    "        # Fetch the image and ground truth mask. For safety, we index the items around the\n",
    "        # ground truth masks, so that if for some reason the images are misaligned we will\n",
    "        # guarantee that we will fetch the correct image\n",
    "\n",
    "        if idx < len(self.axis0_imgs):\n",
    "            axis, gt_name = 0, self.axis0_imgs[idx]\n",
    "        elif idx < len(self.axis0_imgs) + len(self.axis1_imgs):\n",
    "            axis, gt_name = 1, self.axis1_imgs[idx - len(self.axis0_imgs)]\n",
    "        else:\n",
    "            axis, gt_name = 2, self.axis2_imgs[idx - len(self.axis0_imgs) - len(self.axis1_imgs)]\n",
    "\n",
    "        image_id = int(re.search(image_id_from_file_name_regex, gt_name).group(1))\n",
    "        slice_id = int(re.search(slice_id_from_file_name_regex, gt_name).group(1))\n",
    "\n",
    "        img_name = f'CT_zzAMLART_{image_id:03d}-{slice_id:03d}.npy'\n",
    "        \n",
    "        # Load the image and ground truth mask\n",
    "\n",
    "        img_path = os.path.join(self.root_img_path, f'axis{axis}', img_name)\n",
    "        gt_path = os.path.join(self.root_gt_path, f'axis{axis}', gt_name)\n",
    "\n",
    "        img = np.load(img_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "        gt = np.load(gt_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "\n",
    "        # Pre-process where necessary\n",
    "\n",
    "        img = np.transpose(img, (2, 0, 1)) # (C, H, W)\n",
    "        assert np.max(img) <= 1. and np.min(img) >= 0., 'image should be normalized to [0, 1]'\n",
    "\n",
    "        # add data augmentation: random fliplr and random flipud\n",
    "        if self.data_aug:\n",
    "            if random.random() > 0.5:\n",
    "                img = np.ascontiguousarray(np.flip(img, axis=-1))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-1))\n",
    "            if random.random() > 0.5:\n",
    "                img = np.ascontiguousarray(np.flip(img, axis=-2))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-2))\n",
    "        \n",
    "        gt = np.uint8(gt > 0)\n",
    "        y_indices, x_indices = np.where(gt > 0)\n",
    "        x_point = np.random.choice(x_indices)\n",
    "        y_point = np.random.choice(y_indices)\n",
    "        coords = np.array([x_point, y_point])\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(img).float(),\n",
    "            \"gt2D\": torch.tensor(gt[None, :,:]).long(),\n",
    "            \"coords\": torch.tensor(coords[None, ...]).float(),\n",
    "            \"image_name\": img_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Fine-Tuning nn Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAM(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_encoder, \n",
    "                mask_decoder,\n",
    "                prompt_encoder,\n",
    "                freeze_image_encoder=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "        # freeze prompt encoder\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        if self.freeze_image_encoder:\n",
    "            for param in self.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, point_prompt):\n",
    "\n",
    "        # do not compute gradients for pretrained img encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
    "            # not need to convert box to 1024x1024 grid\n",
    "            # bbox is already in 1024x1024\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=point_prompt,\n",
    "                boxes=None,\n",
    "                masks=None,\n",
    "            )\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          ) # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.training.logging.nnunet_logger import nnUNetLogger\n",
    "\n",
    "class MedSAMLogger(nnUNetLogger):\n",
    "    pass\n",
    "\n",
    "# self.logger.log('lrs', self.optimizer.param_groups[0]['lr'], self.current_epoch)\n",
    "# self.logger.log('train_losses', loss_here, self.current_epoch)\n",
    "# self.logger.log('mean_fg_dice', mean_fg_dice, self.current_epoch)\n",
    "# self.logger.log('dice_per_class_or_region', global_dc_per_class, self.current_epoch)\n",
    "# self.logger.log('val_losses', loss_here, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAMTrainer(object):\n",
    "    def __init__(self\n",
    "                , anatomy\n",
    "                , checkpoint_path\n",
    "                , save_dir\n",
    "                , image_dir\n",
    "                , gt_dir\n",
    "                , epochs\n",
    "                , batch_size\n",
    "                , lr\n",
    "                , num_workers\n",
    "                , weight_decay\n",
    "                , resume\n",
    "                , device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "                , ):\n",
    "        \n",
    "        self.anatomy = anatomy\n",
    "        assert self.anatomy in ['CTVn', 'CTVp', 'Bladder', 'Anorectum', 'Uterus', 'Vagina']\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        assert os.path.exists(self.image_dir), 'Image Directory doesn\\'t exist.'\n",
    "        assert os.path.exists(self.gt_dir), 'Ground Truth Directory doesn\\'t exist for the requested anatomy.'\n",
    "\n",
    "        self.save_dir = save_dir\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.num_workers = num_workers\n",
    "        self.weight_decay = weight_decay\n",
    "        self.resume = resume\n",
    "\n",
    "    def run_training(self):\n",
    "        \"\"\"Main Training Loop\"\"\"\n",
    "\n",
    "        self.on_train_start()\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "            # <Training>\n",
    "            self.on_epoch_start(epoch)\n",
    "\n",
    "            pbar = tqdm(self.train_loader)\n",
    "            for step, batch in enumerate(pbar):\n",
    "                loss = self.train_step(step, batch)\n",
    "\n",
    "                pbar.set_description(f\"Epoch {epoch} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, loss: {loss.item():.4f}\")\n",
    "\n",
    "            self.on_epoch_end()\n",
    "            # </Training>\n",
    "\n",
    "            # # <Validation>\n",
    "            # with torch.no_grad():\n",
    "            #     self.medsam_model.eval()\n",
    "            #     for step, batch in enumerate(self.val_loader):\n",
    "            #         image = batch[\"image\"].to(self.device)\n",
    "            #         gt2D = batch[\"gt2D\"].to(self.device)\n",
    "            #         coords_torch = batch[\"coords\"].to(self.device)\n",
    "            # # </Validation>\n",
    "\n",
    "        self.on_train_end()\n",
    "\n",
    "    def train_step(self, step, batch):\n",
    "        # Get data\n",
    "        image = batch[\"image\"].to(self.device)\n",
    "        gt2D = batch[\"gt2D\"].to(self.device)\n",
    "        coords_torch = batch[\"coords\"].to(self.device) # (B, 2)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        labels_torch = torch.ones(coords_torch.shape[0]).long() # (B,)\n",
    "        labels_torch = labels_torch.unsqueeze(1) # (B, 1)\n",
    "        coords_torch, labels_torch = coords_torch.to(device), labels_torch.to(device)\n",
    "        point_prompt = (coords_torch, labels_torch)\n",
    "        medsam_lite_pred = self.medsam_model(image, point_prompt)\n",
    "        loss = self.seg_loss(medsam_lite_pred, gt2D) + self.ce_loss(medsam_lite_pred, gt2D.float())\n",
    "        \n",
    "        self.epoch_loss[step] = loss.item()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_end(self):\n",
    "        self.save_checkpoint(self.current_epoch, self.epoch_loss, final=True)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # self.logger.log('epoch_end_timestamps', time(), self.current_epoch)\n",
    "\n",
    "        self.epoch_end_time = time()\n",
    "        self.epoch_time.append(self.epoch_end_time - self.epoch_start_time)\n",
    "\n",
    "        self.save_checkpoint(self.current_epoch, self.epoch_loss)\n",
    "\n",
    "        # Plot the progress\n",
    "\n",
    "        # self.print_to_log_file('train_loss', np.round(self.logger.my_fantastic_logging['train_losses'][-1], decimals=4))\n",
    "        # self.print_to_log_file('val_loss', np.round(self.logger.my_fantastic_logging['val_losses'][-1], decimals=4))\n",
    "        # self.print_to_log_file('Pseudo dice', [np.round(i, decimals=4) for i in\n",
    "        #                                        self.logger.my_fantastic_logging['dice_per_class_or_region'][-1]])\n",
    "        # self.print_to_log_file(\n",
    "        #     f\"Epoch time: {np.round(self.logger.my_fantastic_logging['epoch_end_timestamps'][-1] - self.logger.my_fantastic_logging['epoch_start_timestamps'][-1], decimals=2)} s\")\n",
    "\n",
    "        # self.logger.plot_progress_png(self.save_dir)\n",
    "\n",
    "    def save_checkpoint(self, epoch, epoch_loss, final=False):\n",
    "        # If the checkpoint is better than the last, save it as 'checkpoint_best.pth' otherwise, save it as checkpoint_latest.pth\n",
    "\n",
    "        epoch_loss_reduced = epoch_loss_reduced = sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "        checkpoint = {\n",
    "            \"model\": self.medsam_model.state_dict(),\n",
    "            \"epochs\": epoch,\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"loss\": epoch_loss_reduced,\n",
    "            \"best_loss\": self.best_loss\n",
    "        }\n",
    "\n",
    "        if epoch_loss_reduced < self.best_loss:\n",
    "            self.best_loss = epoch_loss_reduced\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_best.pth'))\n",
    "\n",
    "        if final:\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_final.pth'))\n",
    "            os.remove(os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "        else:\n",
    "            torch.save(checkpoint, os.path.join(self.save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "    def on_train_start(self):\n",
    "        \"\"\"Sets up the training environment\"\"\"\n",
    "\n",
    "        # empty cuda cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # create save_dir if it doesn't exist yet\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        # set up logger to a new instance\n",
    "        self.logger = MedSAMLogger()\n",
    "\n",
    "        # load the previous checkpoint if it exists and resume is True. Otherwise, load\n",
    "        # the medSAM model from which we train.\n",
    "        self._first_run_setup()\n",
    "        if (not self._maybe_load_checkpoint()):\n",
    "            self._setup_data_splits()\n",
    "\n",
    "        # set up the dataloaders\n",
    "        self._get_dataloaders()\n",
    "\n",
    "        # set up loss functions\n",
    "        self.seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "        self.ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "    def on_epoch_start(self, epoch):\n",
    "        self.medsam_model.train()\n",
    "        self.epoch_loss = [1e10 for _ in range(len(self.training_dataset))]\n",
    "        self.current_epoch = epoch\n",
    "        self.epoch_start_time = datetime.now()\n",
    "        \n",
    "        # self.logger.log('lrs', self.optimizer.param_groups[0]['lr'], self.current_epoch)\n",
    "\n",
    "    def _maybe_load_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Populates the variables IF a checkpoint has been found\n",
    "        - checkpoint\n",
    "        - medsam_model\n",
    "        - optimizer\n",
    "        - start_epoch\n",
    "        - best_loss\n",
    "        - training_split\n",
    "        - validation_split\n",
    "        \"\"\"\n",
    "\n",
    "        if self.resume:\n",
    "            if len([f for f in os.listdir(save_dir) if f == 'checkpoint_latest.pth']) == 0:\n",
    "                print(\"No checkpoint found in the save directory\")\n",
    "            else:\n",
    "                # Load model details\n",
    "                self.checkpoint = torch.load(os.path.join(save_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "                sam_model = sam_model_registry[\"vit_b\"](checkpoint=os.path.join(save_dir, 'checkpoint_latest.pth'))\n",
    "                \n",
    "                self.medsam_model = MedSAM(\n",
    "                    image_encoder = sam_model.image_encoder,\n",
    "                    mask_decoder = sam_model.mask_decoder,\n",
    "                    prompt_encoder = sam_model.prompt_encoder,\n",
    "                    freeze_image_encoder = True\n",
    "                ).to(device)\n",
    "\n",
    "                # self.medsam_model.load_state_dict(self.checkpoint[\"model\"])\n",
    "                self.optimizer.load_state_dict(self.checkpoint[\"optimizer\"])\n",
    "                self.start_epoch = self.checkpoint[\"epoch\"] + 1\n",
    "                self.best_loss = self.checkpoint[\"best_loss\"]\n",
    "                print(f\"Loaded checkpoint from epoch {self.start_epoch}, best loss: {self.best_loss:.4f}\")\n",
    "\n",
    "                # Get Data Split\n",
    "                try:\n",
    "                    self._load_split_from_json()\n",
    "                except FileNotFoundError as e:\n",
    "                    print('[WARNING] a checkpoint was found, but a split file was not.')\n",
    "                    print('          a new data split will be generated with potential training bias towards the validaiton split.')\n",
    "                    self._setup_data_splits()\n",
    "\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _first_run_setup(self):\n",
    "        sam_model = sam_model_registry[\"vit_b\"](checkpoint=self.checkpoint_path)\n",
    "        self.medsam_model = MedSAM(\n",
    "            image_encoder = sam_model.image_encoder,\n",
    "            mask_decoder = sam_model.mask_decoder,\n",
    "            prompt_encoder = sam_model.prompt_encoder,\n",
    "            freeze_image_encoder = True\n",
    "        )\n",
    "        self.medsam_model = self.medsam_model.to(device)\n",
    "\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.medsam_model.mask_decoder.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def _save_splits_to_json(self, training_image_ids, validation_image_ids):\n",
    "        data = {\n",
    "            \"training_image_ids\": list(training_image_ids),\n",
    "            \"validation_image_ids\": list(validation_image_ids)\n",
    "        }\n",
    "        with open(os.path.join(self.save_dir, 'data_splits.json'), 'w') as json_file:\n",
    "            json.dump(data, json_file)\n",
    "\n",
    "    def _load_split_from_json(self):\n",
    "        with open(os.path.join(self.save_dir, 'data_splits.json'), 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        self.training_split = set(data[\"training_image_ids\"])\n",
    "        self.validation_split = set(data[\"validation_image_ids\"])\n",
    "\n",
    "    def _setup_data_splits(self, training_split=None, validation_split=None):\n",
    "        \"\"\"Setup the data splits either from a known source or a new setup.\"\"\"\n",
    "        if training_split is not None and validation_split is not None:\n",
    "            self.training_split = training_split\n",
    "            self.validation_split = validation_split\n",
    "            return\n",
    "\n",
    "        # get the image ids that have been processed. Use gt dir as reference\n",
    "        axis0_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis0'))))\n",
    "        axis1_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis1'))))\n",
    "        axis2_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(self.gt_dir, 'axis2'))))\n",
    "\n",
    "        if not axis0_slices == axis1_slices == axis2_slices:\n",
    "            print('[WARNING]: The slices for the anatomy are not consistent across the three axes. Some axese are missing data, please check')\n",
    "\n",
    "        training_split = 0.8\n",
    "        validation_split = 1 - training_split\n",
    "\n",
    "        # Split the data into training and validation\n",
    "        self.training_image_ids = random.sample(list(axis0_slices), int(len(axis0_slices) * training_split))\n",
    "        self.validation_image_ids = list(set(axis0_slices) - set(self.training_image_ids))\n",
    "        assert set.intersection(set(self.training_image_ids), set(self.validation_image_ids)).__len__() == 0, 'Training and Validation sets are not disjoint'\n",
    "\n",
    "        # Save the splits in a json file\n",
    "        self._save_splits_to_json(self.training_image_ids, self.validation_image_ids)\n",
    "\n",
    "    def _get_dataloaders(self):\n",
    "        self.training_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.training_image_ids)\n",
    "        self.validation_dataset = SAM_Dataset(self.image_dir, self.gt_dir, self.validation_image_ids)\n",
    "        \n",
    "        # Quick check\n",
    "        assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.validation_dataset.axis0_imgs)) == set(self.validation_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "        assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), self.training_dataset.axis0_imgs)) == set(self.training_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "\n",
    "        self.train_loader = DataLoader(self.training_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        self.val_loader = DataLoader(self.validation_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MedSAMTrainer(\n",
    "    anatomy\n",
    "    , checkpoint_path\n",
    "    , save_dir\n",
    "    , img_dir\n",
    "    , gt_dir\n",
    "    , epochs\n",
    "    , batch_size\n",
    "    , lr\n",
    "    , num_workers\n",
    "    , weight_decay\n",
    "    , resume\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
