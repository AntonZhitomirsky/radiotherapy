{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune a checkpoint of MedSAM on point prompted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a path to a MedSAM checkpoint, we want to fine tune it on pre-processed data\n",
    "(subject to modifications specified by the paper and the transformation script). This will\n",
    "be done initially on an anatomy-specific level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Add the setup_data_vars function as we will need it to find the directory for the training data.\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, required=False, help='Whether to resume training using the latest checkpoint in the save_dir', metavar=None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Inspired by orginal code from the MedSAM/extensions/point_prompt\n",
    "\n",
    "# 1. Add the anatomy on which we will fine-tune\n",
    "parser.add_argument(\n",
    "    '--anatomy',\n",
    "    type=str,\n",
    "    help='Anatomy on which to fine-tune the model. Note: this is case sensitive, please capitalize the first letter and accronyms such as CTVn or CTVp.',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 2. Path to the MedSAM checkpoint\n",
    "parser.add_argument(\n",
    "    '--checkpoint',\n",
    "    type=str,\n",
    "    help='Path to the checkpoint of the model to fine-tune',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 3. Path where we will be saving the checkpoints of the fine-tuned model\n",
    "parser.add_argument(\n",
    "    '--save_dir',\n",
    "    type=str,\n",
    "    help='Directory where the fine-tuned model will be saved',\n",
    "    required=True\n",
    ")\n",
    "\n",
    "# 4. Add the source directory for the data\n",
    "parser.add_argument(\n",
    "    '--img_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the images for the slices of the anatomy',\n",
    "    required=False,\n",
    ")\n",
    "\n",
    "# 5. Add the source directory for the gts\n",
    "parser.add_argument(\n",
    "    '--gt_dir',\n",
    "    type=str,\n",
    "    help='Directory containing the ground truth masks for the slices of the anatomy',\n",
    "    required=False\n",
    ")\n",
    "\n",
    "# 6. Number of epochs for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--epochs',\n",
    "    type=int,\n",
    "    help='Number of epochs for the fine-tuning',\n",
    "    required=False,\n",
    "    default=300\n",
    ")\n",
    "\n",
    "# 7. Batch size for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--batch_size',\n",
    "    type=int,\n",
    "    help='Batch size for the fine-tuning',\n",
    "    required=False,\n",
    "    default=4\n",
    ")\n",
    "\n",
    "# 8. Learning rate for the fine-tuning\n",
    "parser.add_argument(\n",
    "    '--lr',\n",
    "    type=float,\n",
    "    help='Learning rate for the fine-tuning',\n",
    "    required=False,\n",
    "    default=0.00005\n",
    ")\n",
    "\n",
    "# 9. Number of workers for the data loader\n",
    "parser.add_argument(\n",
    "    '--num_workers',\n",
    "    type=int,\n",
    "    help='Number of workers for the data loader',\n",
    "    required=False,\n",
    "    default=4\n",
    ")\n",
    "\n",
    "# 10. Resume checkpoint\n",
    "parser.add_argument(\n",
    "    '--resume',\n",
    "    type=bool,\n",
    "    help='Whether to resume training using the latest checkpoint in the save_dir',\n",
    "    required=False,\n",
    "    default=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parser.parse_args()\n",
    "# Suppose for now we get the following set of required arguments:\n",
    "args = parser.parse_args([\n",
    "    '--anatomy', 'CTVn',\n",
    "    '--checkpoint', os.path.join(os.environ['PROJECT_DIR'], 'models', 'work_dir', 'medsam_vit_b.pth'),\n",
    "    '--save_dir', os.path.join(os.environ['MedSAM_finetuned'], 'CTVn')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "anatomy = args.anatomy\n",
    "checkpoint_path = args.checkpoint\n",
    "save_dir = args.save_dir\n",
    "img_dir = args.img_dir\n",
    "gt_dir = args.gt_dir\n",
    "epochs = args.epochs\n",
    "batch_size = args.batch_size\n",
    "lr = args.lr\n",
    "num_workers = args.num_workers\n",
    "resume = args.resume\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert anatomy in ['CTVn', 'CTVp', 'Bladder', 'Anorectum', 'Uterus', 'Vagina']\n",
    "\n",
    "if img_dir is None:\n",
    "    img_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'imgs')\n",
    "if gt_dir is None:\n",
    "    gt_dir = os.path.join(os.environ['MedSAM_preprocessed'], 'gts', anatomy)\n",
    "\n",
    "assert os.path.exists(img_dir), 'Image Directory doesn\\'t exist.'\n",
    "assert os.path.exists(gt_dir), 'Ground Truth Directory doesn\\'t exist for the requested anatomy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dataset and (Train, Validation) split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the image ids that have been processed. Use gt dir as reference\n",
    "image_id_from_file_name_regex = r'.*_(\\d+).*'\n",
    "slice_id_from_file_name_regex = r'.*-(\\d+).*'\n",
    "\n",
    "axis0_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(gt_dir, 'axis0'))))\n",
    "axis1_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(gt_dir, 'axis1'))))\n",
    "axis2_slices = set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), os.listdir(os.path.join(gt_dir, 'axis2'))))\n",
    "\n",
    "if not axis0_slices == axis1_slices == axis2_slices:\n",
    "    print('[WARNING]: The slices for the anatomy are not consistent across the three axes. Some axese are missing data, please check')\n",
    "\n",
    "training_split = 0.8\n",
    "validation_split = 1 - training_split\n",
    "\n",
    "# Split the data into training and validation\n",
    "training_image_ids = random.sample(list(axis0_slices), int(len(axis0_slices) * training_split))\n",
    "validation_image_ids = list(set(axis0_slices) - set(training_image_ids))\n",
    "assert set.intersection(set(training_image_ids), set(validation_image_ids)).__len__() == 0, 'Training and Validation sets are not disjoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted Dataset class from ../2_no_finetuning/MEDSAM_helper_functions.py\n",
    "class SAM_Dataset(Dataset):\n",
    "    \"\"\"A torch dataset for delivering slices of any axis to a medsam model.\"\"\"\n",
    "\n",
    "    def __init__(self, img_path, gt_path, id_split, data_aug=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_path (string): Path to the directory containing the images\n",
    "            gt_path (string): Path to the directory containing the ground truth masks\n",
    "            id_split (list): List of image ids to include in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        self.root_img_path = img_path\n",
    "        self.root_gt_path = gt_path\n",
    "        self.id_split = id_split\n",
    "        self.data_aug = data_aug\n",
    "        \n",
    "        # Assume that axese 0 1 and 2 have been processed.\n",
    "        filter_fn = lambda x : x.endswith('.npy') and int(re.search(image_id_from_file_name_regex, x).group(1)) in id_split\n",
    "        self.axis0_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis0'))))\n",
    "        self.axis1_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis1'))))\n",
    "        self.axis2_imgs = list(filter(filter_fn, os.listdir(os.path.join(gt_path, 'axis2'))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.axis0_imgs) + len(self.axis1_imgs) + len(self.axis2_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert 0 <= idx < self.__len__(), f\"Index {idx} is out of range for dataset of size {self.__len__()}\"\n",
    "\n",
    "        # Fetch the image and ground truth mask. For safety, we index the items around the\n",
    "        # ground truth masks, so that if for some reason the images are misaligned we will\n",
    "        # guarantee that we will fetch the correct image\n",
    "\n",
    "        if idx < len(self.axis0_imgs):\n",
    "            axis, gt_name = 0, self.axis0_imgs[idx]\n",
    "        elif idx < len(self.axis0_imgs) + len(self.axis1_imgs):\n",
    "            axis, gt_name = 1, self.axis1_imgs[idx - len(self.axis0_imgs)]\n",
    "        else:\n",
    "            axis, gt_name = 2, self.axis2_imgs[idx - len(self.axis0_imgs) - len(self.axis1_imgs)]\n",
    "\n",
    "        image_id = int(re.search(image_id_from_file_name_regex, gt_name).group(1))\n",
    "        slice_id = int(re.search(slice_id_from_file_name_regex, gt_name).group(1))\n",
    "\n",
    "        img_name = f'CT_zzAMLART_{image_id:03d}-{slice_id:03d}.npy'\n",
    "        \n",
    "        # Load the image and ground truth mask\n",
    "\n",
    "        img_path = os.path.join(self.root_img_path, f'axis{axis}', img_name)\n",
    "        gt_path = os.path.join(self.root_gt_path, f'axis{axis}', gt_name)\n",
    "\n",
    "        img = np.load(img_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "        gt = np.load(gt_path, 'r', allow_pickle=True) # (H, W, C)\n",
    "\n",
    "        # Pre-process where necessary\n",
    "\n",
    "        img = np.transpose(img, (2, 0, 1)) # (C, H, W)\n",
    "        assert np.max(img) <= 1. and np.min(img) >= 0., 'image should be normalized to [0, 1]'\n",
    "\n",
    "        # add data augmentation: random fliplr and random flipud\n",
    "        if self.data_aug:\n",
    "            if random.random() > 0.5:\n",
    "                img = np.ascontiguousarray(np.flip(img, axis=-1))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-1))\n",
    "            if random.random() > 0.5:\n",
    "                img = np.ascontiguousarray(np.flip(img, axis=-2))\n",
    "                gt = np.ascontiguousarray(np.flip(gt, axis=-2))\n",
    "        \n",
    "        gt = np.uint8(gt > 0)\n",
    "        y_indices, x_indices = np.where(gt > 0)\n",
    "        x_point = np.random.choice(x_indices)\n",
    "        y_point = np.random.choice(y_indices)\n",
    "        coords = np.array([x_point, y_point])\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(img).float(),\n",
    "            \"gt2D\": torch.tensor(gt[None, :,:]).long(),\n",
    "            \"coords\": torch.tensor(coords[None, ...]).float(),\n",
    "            \"image_name\": img_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = SAM_Dataset(img_dir, gt_dir, training_image_ids)\n",
    "validation_dataset = SAM_Dataset(img_dir, gt_dir, validation_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check\n",
    "assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), validation_dataset.axis0_imgs)) == set(validation_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'\n",
    "assert set(map(lambda x : int(re.search(image_id_from_file_name_regex, x).group(1)), training_dataset.axis0_imgs)) == set(training_image_ids), 'DataSet incorrectly loaded image ids that don\\'t match supplied validation set image ids'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Fine-Tuning nn Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAM(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_encoder, \n",
    "                mask_decoder,\n",
    "                prompt_encoder,\n",
    "                freeze_image_encoder=False,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "        # freeze prompt encoder\n",
    "        for param in self.prompt_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.freeze_image_encoder = freeze_image_encoder\n",
    "        if self.freeze_image_encoder:\n",
    "            for param in self.image_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, image, point_prompt):\n",
    "\n",
    "        # do not compute gradients for pretrained img encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
    "            # not need to convert box to 1024x1024 grid\n",
    "            # bbox is already in 1024x1024\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=point_prompt,\n",
    "                boxes=None,\n",
    "                masks=None,\n",
    "            )\n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          ) # (B, 1, 256, 256)\n",
    "\n",
    "        return low_res_masks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
