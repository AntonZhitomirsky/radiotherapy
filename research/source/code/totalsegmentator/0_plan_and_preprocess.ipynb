{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning and Preprocessing Data for Transfer Learning\n",
    "\n",
    "In order to fine tune the model on data that we have, the data must be transfored so that\n",
    "it matches the fingerprint of the data the original model was trained with. We can\n",
    "apparently fine-tune the model with the `plans.json` file that appears in each model's\n",
    "checkpoint once we download the weights for the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "\n",
    "def setup_data_vars(mine = True, overwrite = True):\n",
    "    \"\"\"\n",
    "    From within any directory related to radiotherapy with backtrack into the data folder\n",
    "    and execute the data_vars script. The assumption is that the datavars script will\n",
    "    output the list of environment variables that need to be set. This function will set\n",
    "    the environment variables for the current session.\n",
    "\n",
    "    For the mean while, my model hasn't completely finished training, therefore, to get\n",
    "    this task done, I will use Ben's pretrained nnUNet and then once mine has finished\n",
    "    training I will use my own. For the mean while, this means that we can choose between\n",
    "    using Ben's pretrained model or my own.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the environment variables are not set, assume that either a custom one has been\n",
    "    # provided or resetting them again is a redundant task\n",
    "    if os.environ.get('nnUNet_raw') is None or overwrite is True:\n",
    "        # run the script in the data folder for specifying the environment variables\n",
    "        if mine:\n",
    "            cwd = os.getcwd().split('/')\n",
    "            data_dir = os.path.join('/'.join(cwd[:cwd.index('radiotherapy') + 1]), 'data')\n",
    "\n",
    "            # Assuming the data_vars.sh script echoes the environment variables\n",
    "            script = os.path.join(data_dir, 'data_vars.sh')\n",
    "            output = subprocess.run([script], capture_output=True)\n",
    "            \n",
    "            assert len(output.stdout) != 0, f\"Please check {script} and make sure it echoes \\\n",
    "    the environment variables.\"\n",
    "\n",
    "            output = output.stdout.decode('utf-8')\n",
    "        else:\n",
    "            data_dir = '/vol/biomedic3/bglocker/nnUNet'\n",
    "\n",
    "            # Assuming this script won't change, it contains hard coded exports\n",
    "            script = os.path.join(data_dir, 'exports')\n",
    "\n",
    "            with open(script, 'r') as file:\n",
    "                output = file.read()\n",
    "        \n",
    "        for line in output.split('\\n'):\n",
    "            if line != '':\n",
    "                if mine:\n",
    "                    line = line.split(': ')\n",
    "                    os.environ[line[0]] = line[1]\n",
    "                else:\n",
    "                    line = line.split('=')\n",
    "                    os.environ[line[0].split(' ')[1]] = line[1]\n",
    "\n",
    "    assert os.environ.get('nnUNet_raw') is not None, \"Environemnt variables not set. \\\n",
    "Please run the data_vars.sh script in the data folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_and_gt_data_paths():\n",
    "    \n",
    "    setup_data_vars()\n",
    "\n",
    "    classes = [os.environ.get('data_Anorectum'), \n",
    "        os.environ.get('data_Bladder'), \n",
    "        os.environ.get('data_CTVn'), \n",
    "        os.environ.get('data_CTVp'), \n",
    "        os.environ.get('data_Parametrium'), \n",
    "        os.environ.get('data_Uterus'), \n",
    "        os.environ.get('data_Vagina')]\n",
    "\n",
    "    raw_data = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingImages')) for x in classes]\n",
    "    gt_labels = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingLabels')) for x in classes]\n",
    "\n",
    "    return classes, raw_data, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from totalsegmentator.config import setup_nnunet, setup_totalseg\n",
    "from totalsegmentator.libs import download_pretrained_weights\n",
    "\n",
    "def fetch_pretrained_totalsegmentator_model():\n",
    "    \"\"\"\n",
    "    Fetch the pretrained TotalSegmentator model.\n",
    "\n",
    "    The total segmentator model loads a separately trained nnUNet model for each new class\n",
    "    However, it is not trained on the parametrium case. Therefore, we load the general\n",
    "    model and attempt to finetune it on my case.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ['TOTALSEG_HOME_DIR'] = '/vol/bitbucket/az620/radiotherapy/models/TotalSegmentator/.weights'\n",
    "\n",
    "    setup_nnunet()\n",
    "    setup_totalseg()\n",
    "\n",
    "    # We assume that the model we are running will be finetuned with the 'total' task from\n",
    "    # the original TotalSegmentator model because this contains the most information about\n",
    "    # soft organ classification, most of which happens in the abdomen region, which\n",
    "    # intuitively seems like the most logical knowledge to transfer into this task\n",
    "\n",
    "    # From the totalsegmentator.python_api this task ID corresponds with the model trained\n",
    "    # for organ segmentation. Note there is also potential for cropping the image.\n",
    "    task_id = 291\n",
    "    download_pretrained_weights(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Download the weights from TotalSegmentator\n",
    "    fetch_pretrained_totalsegmentator_model()\n",
    "\n",
    "    # Something in the fetch_pretrained_totalsegmentator_model overwrites the global variables\n",
    "    setup_data_vars()\n",
    "    classes, raw_data, gt_labels = get_raw_and_gt_data_paths()\n",
    "\n",
    "    print('[DEBUG]: Obtained the environment variables. These are:')\n",
    "    print(f'nnUNet_raw: {os.environ.get(\"nnUNet_raw\")}')\n",
    "    print(f'nnUNet_preprocessed: {os.environ.get(\"nnUNet_preprocessed\")}')\n",
    "    print(f'nnUNet_results: {os.environ.get(\"nnUNet_results\")}')\n",
    "\n",
    "    # Set the data to pre-train on the fingerprint of the training data.\n",
    "    \n",
    "    \"\"\"\n",
    "    TARGET_DATASET = the one you wish to fine tune on, Radiotherapy data\n",
    "    SOURCE_DATASET = dataset you intend to run the pretraining on, TotalSegmentator\n",
    "\n",
    "    1. nnUNetv2_plan_and_preprocess -d TARGET_DATASET (this has been done already)\n",
    "    2. nnUNetv2_extract_fingerprint -d SOURCE_DATASET (this has been achieved by creating a dummy dataset id into which I copied the .json files obtained from the downloaded model)\n",
    "\n",
    "    Path to the plans.json for TotalSegmentator:\n",
    "    /vol/bitbucket/az620/radiotherapy/models/TotalSegmentator/.weights/nnunet/results/Dataset291_TotalSegmentator_part1_organs_1559subj/nnUNetTrainerNoMirroring__nnUNetPlans__3d_fullres/plans.json\n",
    "\n",
    "    3. Now I need to move the plans from the dummy dataset to the Radiotherapy dataset. Need to do this one at a time for each class\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print('RUNNING THE TRANSFER OF PLANS BETWEEN DATASETS......................')\n",
    "\n",
    "    from nnunetv2.experiment_planning.plans_for_pretraining.move_plans_between_datasets import move_plans_between_datasets\n",
    "\n",
    "    starting_class = 5\n",
    "    end_class = len(classes)\n",
    "\n",
    "    for i in range(starting_class, end_class + 1):\n",
    "        print(f'Transferring for {i}')\n",
    "\n",
    "        move_plans_between_datasets(source_dataset_name_or_id=8\n",
    "                                    , target_dataset_name_or_id=i\n",
    "                                    , source_plans_identifier='nnUNetPlans'\n",
    "                                    , target_plans_identifier=f'totseg_nnUNetPlans'\n",
    "        )\n",
    "\n",
    "    print('Now you can run the preprocessing on the source task:')\n",
    "\n",
    "    from nnunetv2.experiment_planning.plan_and_preprocess_entrypoints import preprocess_entry\n",
    "\n",
    "    original_sys_argv = sys.argv\n",
    "\n",
    "    for i in range(starting_class, end_class + 1):\n",
    "        print('Preprocessing for class:', str(i), ' (', classes[i - 1], ')')\n",
    "\n",
    "        sys.argv = [original_sys_argv[0], '-d', str(i), '-plans_name', 'totseg_nnUNetPlans', '-c', '3d_fullres', '--verbose', '-np', '4'] #, '-overwrite_plans_name', 'totseg_nnUNetPlans']\n",
    "        preprocess_entry()\n",
    "    \n",
    "    sys.argv = original_sys_argv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
