{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inference on the nnUNet Model we have fine-tuned\n",
    "\n",
    "Assuming that the pre-processed data is available, and the model has been trained for a\n",
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "\n",
    "def setup_data_vars(mine = True, overwrite = True):\n",
    "    \"\"\"\n",
    "    From within any directory related to radiotherapy with backtrack into the data folder\n",
    "    and execute the data_vars script. The assumption is that the datavars script will\n",
    "    output the list of environment variables that need to be set. This function will set\n",
    "    the environment variables for the current session.\n",
    "\n",
    "    For the mean while, my model hasn't completely finished training, therefore, to get\n",
    "    this task done, I will use Ben's pretrained nnUNet and then once mine has finished\n",
    "    training I will use my own. For the mean while, this means that we can choose between\n",
    "    using Ben's pretrained model or my own.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the environment variables are not set, assume that either a custom one has been\n",
    "    # provided or resetting them again is a redundant task\n",
    "    if os.environ.get('nnUNet_raw') is None or overwrite is True:\n",
    "        # run the script in the data folder for specifying the environment variables\n",
    "        if mine:\n",
    "            cwd = os.getcwd().split('/')\n",
    "            data_dir = os.path.join('/'.join(cwd[:cwd.index('radiotherapy') + 1]), 'data')\n",
    "\n",
    "            # Assuming the data_vars.sh script echoes the environment variables\n",
    "            script = os.path.join(data_dir, 'data_vars.sh')\n",
    "            output = subprocess.run([script], capture_output=True)\n",
    "            \n",
    "            assert len(output.stdout) != 0, f\"Please check {script} and make sure it echoes \\\n",
    "    the environment variables.\"\n",
    "\n",
    "            output = output.stdout.decode('utf-8')\n",
    "        else:\n",
    "            data_dir = '/vol/biomedic3/bglocker/nnUNet'\n",
    "\n",
    "            # Assuming this script won't change, it contains hard coded exports\n",
    "            script = os.path.join(data_dir, 'exports')\n",
    "\n",
    "            with open(script, 'r') as file:\n",
    "                output = file.read()\n",
    "        \n",
    "        for line in output.split('\\n'):\n",
    "            if line != '':\n",
    "                if mine:\n",
    "                    line = line.split(': ')\n",
    "                    os.environ[line[0]] = line[1]\n",
    "                else:\n",
    "                    line = line.split('=')\n",
    "                    os.environ[line[0].split(' ')[1]] = line[1]\n",
    "\n",
    "    assert os.environ.get('nnUNet_raw') is not None, \"Environemnt variables not set. \\\n",
    "Please run the data_vars.sh script in the data folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_and_gt_data_paths():\n",
    "    \n",
    "    setup_data_vars()\n",
    "\n",
    "    classes = [os.environ.get('data_Anorectum'), \n",
    "        os.environ.get('data_Bladder'), \n",
    "        os.environ.get('data_CTVn'), \n",
    "        os.environ.get('data_CTVp'), \n",
    "        os.environ.get('data_Parametrium'), \n",
    "        os.environ.get('data_Uterus'), \n",
    "        os.environ.get('data_Vagina')]\n",
    "\n",
    "    raw_data = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingImages')) for x in classes]\n",
    "    gt_labels = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingLabels')) for x in classes]\n",
    "\n",
    "    return classes, raw_data, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_predictor(model_path, fold, device):\n",
    "\n",
    "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "    import torch\n",
    "\n",
    "    predictor = nnUNetPredictor(\n",
    "            tile_step_size=0.5,\n",
    "            use_gaussian=True,\n",
    "            use_mirroring=True,\n",
    "            perform_everything_on_device=True,\n",
    "            device=device,\n",
    "            verbose=False,\n",
    "            verbose_preprocessing=False,\n",
    "            allow_tqdm=True\n",
    "        )\n",
    "\n",
    "    predictor.initialize_from_trained_model_folder(\n",
    "        model_path,\n",
    "        use_folds=fold,\n",
    "        checkpoint_name='checkpoint_final.pth',\n",
    "    )\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG]: Obtained the environment variables. These are:\n",
      "nnUNet_raw: /vol/bitbucket/az620/radiotherapy/data/nnUNet_raw\n",
      "nnUNet_preprocessed: /vol/bitbucket/az620/radiotherapy/data/nnUNet_preprocessed\n",
      "nnUNet_results: /vol/bitbucket/az620/radiotherapy/data/nnUNet_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/az620/radiotherapy/models/nnUNet/nnunetv2/utilities/plans_handling/plans_handler.py:37: UserWarning: Detected old nnU-Net plans format. Attempting to reconstruct network architecture parameters. If this fails, rerun nnUNetv2_plan_experiment for your dataset. If you use a custom architecture, please downgrade nnU-Net to the version you implemented this or update your implementation + plans.\n",
      "  warnings.warn(\"Detected old nnU-Net plans format. Attempting to reconstruct network architecture \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 100 cases that I would like to predict\n",
      "overwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 100 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnUNet_raw\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotalSegmentator_inference\u001b[39m\u001b[38;5;124m'\u001b[39m, classes[TARGET_DATASET \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnnUNetTrainer_50epochs__totseg_nnUNetPlans__3d_fullres\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m predictor \u001b[38;5;241m=\u001b[39m initialise_predictor(model_path, FOLD, device)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                             \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msave_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnum_processes_preprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes_segmentation_export\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfolder_with_segs_from_prev_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/az620/radiotherapy/models/nnUNet/nnunetv2/inference/predict_from_raw_data.py:256\u001b[0m, in \u001b[0;36mnnUNetPredictor.predict_from_files\u001b[0;34m(self, list_of_lists_or_source_folder, output_folder_or_list_of_truncated_output_files, save_probabilities, overwrite, num_processes_preprocessing, num_processes_segmentation_export, folder_with_segs_from_prev_stage, num_parts, part_id)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    252\u001b[0m data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_get_data_iterator_from_lists_of_filenames(list_of_lists_or_source_folder,\n\u001b[1;32m    253\u001b[0m                                                                          seg_from_prev_stage_files,\n\u001b[1;32m    254\u001b[0m                                                                          output_filename_truncated,\n\u001b[1;32m    255\u001b[0m                                                                          num_processes_preprocessing)\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_data_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes_segmentation_export\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/az620/radiotherapy/models/nnUNet/nnunetv2/inference/predict_from_raw_data.py:349\u001b[0m, in \u001b[0;36mnnUNetPredictor.predict_from_data_iterator\u001b[0;34m(self, data_iterator, save_probabilities, num_processes_segmentation_export)\u001b[0m\n\u001b[1;32m    347\u001b[0m worker_list \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m export_pool\u001b[38;5;241m.\u001b[39m_pool]\n\u001b[1;32m    348\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m preprocessed \u001b[38;5;129;01min\u001b[39;00m data_iterator:\n\u001b[1;32m    350\u001b[0m     data \u001b[38;5;241m=\u001b[39m preprocessed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/vol/bitbucket/az620/radiotherapy/models/nnUNet/nnunetv2/inference/data_iterators.py:71\u001b[0m, in \u001b[0;36mpreprocessing_iterator_fromfiles\u001b[0;34m(list_of_lists, list_of_segs_from_prev_stage_files, output_filenames_truncated, plans_manager, dataset_json, configuration_manager, num_processes, pin_memory, verbose)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessing_iterator_fromfiles\u001b[39m(list_of_lists: List[List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m     62\u001b[0m                                      list_of_segs_from_prev_stage_files: Union[\u001b[38;5;28;01mNone\u001b[39;00m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[1;32m     63\u001b[0m                                      output_filenames_truncated: Union[\u001b[38;5;28;01mNone\u001b[39;00m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      pin_memory: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m                                      verbose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     70\u001b[0m     context \u001b[38;5;241m=\u001b[39m multiprocessing\u001b[38;5;241m.\u001b[39mget_context(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 71\u001b[0m     manager \u001b[38;5;241m=\u001b[39m \u001b[43mManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     num_processes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list_of_lists), num_processes)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_processes \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py:57\u001b[0m, in \u001b[0;36mBaseContext.Manager\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanagers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SyncManager\n\u001b[1;32m     56\u001b[0m m \u001b[38;5;241m=\u001b[39m SyncManager(ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context())\n\u001b[0;32m---> 57\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/managers.py:566\u001b[0m, in \u001b[0;36mBaseManager.start\u001b[0;34m(self, initializer, initargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# get address of server\u001b[39;00m\n\u001b[1;32m    565\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 566\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_address \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m reader\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# register a finalizer\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')   \n",
    "\n",
    "    setup_data_vars()\n",
    "    classes, raw_data, gt_labels = get_raw_and_gt_data_paths()\n",
    "\n",
    "    print('[DEBUG]: Obtained the environment variables. These are:')\n",
    "    print(f'nnUNet_raw: {os.environ.get(\"nnUNet_raw\")}')\n",
    "    print(f'nnUNet_preprocessed: {os.environ.get(\"nnUNet_preprocessed\")}')\n",
    "    print(f'nnUNet_results: {os.environ.get(\"nnUNet_results\")}')\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('dataset', type=int, help='The dataset to run inference on')\n",
    "    parser.add_argument('fold', type=int, help='The max number of nodes that were trained')\n",
    "    sys.argv = ['2_inference.py', '1', '0']\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "    assert args.dataset is not None, \"Please provide the dataset to fine tune on\"\n",
    "    assert args.dataset in range(1, len(classes) + 1), \"Please provide a valid dataset to fine tune on\"\n",
    "\n",
    "    assert args.fold is not None, \"Please provide the fold to run inference on\"\n",
    "    assert args.fold in range(5), \"Please provide a valid fold to run inference on\"\n",
    "\n",
    "    TARGET_DATASET = args.dataset\n",
    "    FOLD = tuple(range(0, args.fold + 1))\n",
    "    CONFIG = '3d_fullres'\n",
    "\n",
    "    # Run inference\n",
    "    model_name = 'nnUNetTrainer_50epochs__totseg_nnUNetPlans__3d_fullres'\n",
    "    input_file = os.path.join(os.environ.get('nnUNet_raw'), classes[TARGET_DATASET - 1], os.environ.get('data_trainingImages'))\n",
    "    model_path = os.path.join(os.environ.get('nnUNet_results'), classes[TARGET_DATASET - 1], model_name) \n",
    "    output_file = os.path.join(os.environ.get('nnUNet_raw'), '..', 'TotalSegmentator_inference', classes[TARGET_DATASET - 1], model_name)\n",
    "\n",
    "    predictor = initialise_predictor(model_path, FOLD, device)\n",
    "    predictor.predict_from_files(input_file,\n",
    "                                 output_file,\n",
    "                                 save_probabilities=False, overwrite=False,\n",
    "                                 num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "                                 folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
