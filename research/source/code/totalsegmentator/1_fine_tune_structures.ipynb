{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Structures\n",
    "\n",
    "Assuming that the pre-processed data is available, run the fine-tuning process on the data\n",
    "we have for 50 epochs so that we may see it afterwards in due corse. We can increase the\n",
    "number of epochs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "\n",
    "def setup_data_vars(mine = True, overwrite = True):\n",
    "    \"\"\"\n",
    "    From within any directory related to radiotherapy with backtrack into the data folder\n",
    "    and execute the data_vars script. The assumption is that the datavars script will\n",
    "    output the list of environment variables that need to be set. This function will set\n",
    "    the environment variables for the current session.\n",
    "\n",
    "    For the mean while, my model hasn't completely finished training, therefore, to get\n",
    "    this task done, I will use Ben's pretrained nnUNet and then once mine has finished\n",
    "    training I will use my own. For the mean while, this means that we can choose between\n",
    "    using Ben's pretrained model or my own.\n",
    "    \"\"\"\n",
    "\n",
    "    # If the environment variables are not set, assume that either a custom one has been\n",
    "    # provided or resetting them again is a redundant task\n",
    "    if os.environ.get('nnUNet_raw') is None or overwrite is True:\n",
    "        # run the script in the data folder for specifying the environment variables\n",
    "        if mine:\n",
    "            cwd = os.getcwd().split('/')\n",
    "            data_dir = os.path.join('/'.join(cwd[:cwd.index('radiotherapy') + 1]), 'data')\n",
    "\n",
    "            # Assuming the data_vars.sh script echoes the environment variables\n",
    "            script = os.path.join(data_dir, 'data_vars.sh')\n",
    "            output = subprocess.run([script], capture_output=True)\n",
    "            \n",
    "            assert len(output.stdout) != 0, f\"Please check {script} and make sure it echoes \\\n",
    "    the environment variables.\"\n",
    "\n",
    "            output = output.stdout.decode('utf-8')\n",
    "        else:\n",
    "            data_dir = '/vol/biomedic3/bglocker/nnUNet'\n",
    "\n",
    "            # Assuming this script won't change, it contains hard coded exports\n",
    "            script = os.path.join(data_dir, 'exports')\n",
    "\n",
    "            with open(script, 'r') as file:\n",
    "                output = file.read()\n",
    "        \n",
    "        for line in output.split('\\n'):\n",
    "            if line != '':\n",
    "                if mine:\n",
    "                    line = line.split(': ')\n",
    "                    os.environ[line[0]] = line[1]\n",
    "                else:\n",
    "                    line = line.split('=')\n",
    "                    os.environ[line[0].split(' ')[1]] = line[1]\n",
    "\n",
    "    assert os.environ.get('nnUNet_raw') is not None, \"Environemnt variables not set. \\\n",
    "Please run the data_vars.sh script in the data folder.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_and_gt_data_paths():\n",
    "    \n",
    "    setup_data_vars()\n",
    "\n",
    "    classes = [os.environ.get('data_Anorectum'), \n",
    "        os.environ.get('data_Bladder'), \n",
    "        os.environ.get('data_CTVn'), \n",
    "        os.environ.get('data_CTVp'), \n",
    "        os.environ.get('data_Parametrium'), \n",
    "        os.environ.get('data_Uterus'), \n",
    "        os.environ.get('data_Vagina')]\n",
    "\n",
    "    raw_data = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingImages')) for x in classes]\n",
    "    gt_labels = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingLabels')) for x in classes]\n",
    "\n",
    "    return classes, raw_data, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from totalsegmentator.config import setup_nnunet, setup_totalseg\n",
    "from totalsegmentator.libs import download_pretrained_weights\n",
    "\n",
    "def fetch_pretrained_totalsegmentator_model():\n",
    "    \"\"\"\n",
    "    Fetch the pretrained TotalSegmentator model.\n",
    "\n",
    "    The total segmentator model loads a separately trained nnUNet model for each new class\n",
    "    However, it is not trained on the parametrium case. Therefore, we load the general\n",
    "    model and attempt to finetune it on my case.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ['TOTALSEG_HOME_DIR'] = '/vol/bitbucket/az620/radiotherapy/models/TotalSegmentator/.weights'\n",
    "\n",
    "    setup_nnunet()\n",
    "    setup_totalseg()\n",
    "\n",
    "    # We assume that the model we are running will be finetuned with the 'total' task from\n",
    "    # the original TotalSegmentator model because this contains the most information about\n",
    "    # soft organ classification, most of which happens in the abdomen region, which\n",
    "    # intuitively seems like the most logical knowledge to transfer into this task\n",
    "\n",
    "    # From the totalsegmentator.python_api this task ID corresponds with the model trained\n",
    "    # for organ segmentation. Note there is also potential for cropping the image.\n",
    "    task_id = 291\n",
    "    download_pretrained_weights(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Download the weights from TotalSegmentator\n",
    "    fetch_pretrained_totalsegmentator_model()\n",
    "\n",
    "    # Something in the fetch_pretrained_totalsegmentator_model overwrites the global variables\n",
    "    setup_data_vars()\n",
    "    classes, raw_data, gt_labels = get_raw_and_gt_data_paths()\n",
    "\n",
    "    print('[DEBUG]: Obtained the environment variables. These are:')\n",
    "    print(f'nnUNet_raw: {os.environ.get(\"nnUNet_raw\")}')\n",
    "    print(f'nnUNet_preprocessed: {os.environ.get(\"nnUNet_preprocessed\")}')\n",
    "    print(f'nnUNet_results: {os.environ.get(\"nnUNet_results\")}')\n",
    "\n",
    "    # Set the data to pre-train on the fingerprint of the training data.\n",
    "    \n",
    "    \"\"\"\n",
    "    TARGET_DATASET = the one you wish to fine tune on, Radiotherapy data\n",
    "    SOURCE_DATASET = dataset you intend to run the pretraining on, TotalSegmentator\n",
    "\n",
    "    1. nnUNetv2_plan_and_preprocess -d TARGET_DATASET (this has been done already)\n",
    "    2. nnUNetv2_extract_fingerprint -d SOURCE_DATASET (this has been achieved by creating a dummy dataset id into which I copied the .json files obtained from the downloaded model)\n",
    "\n",
    "    Path to the plans.json for TotalSegmentator:\n",
    "    /vol/bitbucket/az620/radiotherapy/models/TotalSegmentator/.weights/nnunet/results/Dataset291_TotalSegmentator_part1_organs_1559subj/nnUNetTrainerNoMirroring__nnUNetPlans__3d_fullres/plans.json\n",
    "\n",
    "    3. Now I need to move the plans from the dummy dataset to the Radiotherapy dataset. Need to do this one at a time for each class\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('dataset', type=int, help='The dataset to fine tune on')\n",
    "    parser.add_argument('fold', type=int, help='The fold to fine tune on')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    assert args.dataset is not None, \"Please provide the dataset to fine tune on\"\n",
    "    assert args.dataset in range(1, len(classes) + 1), \"Please provide a valid dataset to fine tune on\"\n",
    "\n",
    "    assert args.fold is not None, \"Please provide the fold to fine tune on\"\n",
    "    assert args.fold in range(5), \"Please provide a valid fold to fine tune on\"\n",
    "\n",
    "    TARGET_DATASET = args.dataset\n",
    "    PATH_TO_CHECKPOINT = '/vol/bitbucket/az620/radiotherapy/models/TotalSegmentator/.weights/nnunet/results/Dataset291_TotalSegmentator_part1_organs_1559subj/nnUNetTrainerNoMirroring__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth'\n",
    "    FOLD = args.fold\n",
    "    CONFIG = '3d_fullres'\n",
    "\n",
    "    # Run the training on the target dataset\n",
    "\n",
    "    from nnunetv2.run.run_training import run_training_entry\n",
    "\n",
    "    original_sys_argv = sys.argv\n",
    "\n",
    "    print('-----------')\n",
    "    print(f'FOLD: {FOLD}')\n",
    "    print('-----------')\n",
    "\n",
    "    # !nnUNetv2_train TARGET_DATASET CONFIG FOLD -pretrained_weights PATH_TO_CHECKPOINT\n",
    "    sys.argv = [original_sys_argv[0], str(TARGET_DATASET), CONFIG, str(FOLD), '-pretrained_weights', PATH_TO_CHECKPOINT, '-tr', 'nnUNetTrainer_50epochs', '--npz', '-p', 'totseg_nnUNetPlans']\n",
    "    run_training_entry()\n",
    "\n",
    "    sys.argv = original_sys_argv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
