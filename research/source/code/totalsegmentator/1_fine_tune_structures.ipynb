{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Structures\n",
    "\n",
    "Assuming that the pre-processed data is available, run the fine-tuning process on the data\n",
    "we have for 50 epochs so that we may see it afterwards in due corse. We can increase the\n",
    "number of epochs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)\n",
    "from utils.environment import setup_data_vars\n",
    "setup_data_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_and_gt_data_paths():\n",
    "    \n",
    "    setup_data_vars()\n",
    "\n",
    "    classes = [os.environ.get('Anorectum')\n",
    "             , os.environ.get('Bladder') \n",
    "             , os.environ.get('CTVn') \n",
    "             , os.environ.get('CTVp') \n",
    "             , os.environ.get('Parametrium') \n",
    "             , os.environ.get('Uterus') \n",
    "             , os.environ.get('Vagina')]\n",
    "\n",
    "    raw_data = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingImages')) for x in classes]\n",
    "    gt_labels = [os.path.join(os.environ.get('nnUNet_raw'), x, os.environ.get('data_trainingLabels')) for x in classes]\n",
    "\n",
    "    return classes, raw_data, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from totalsegmentator.config import setup_nnunet, setup_totalseg\n",
    "from totalsegmentator.libs import download_pretrained_weights\n",
    "\n",
    "def fetch_pretrained_totalsegmentator_model():\n",
    "    \"\"\"\n",
    "    Fetch the pretrained TotalSegmentator model.\n",
    "\n",
    "    The total segmentator model loads a separately trained nnUNet model for each new class\n",
    "    However, it is not trained on the parametrium case. Therefore, we load the general\n",
    "    model and attempt to finetune it on my case.\n",
    "    \"\"\"\n",
    "\n",
    "    os.environ['TOTALSEG_HOME_DIR'] = os.path.join(os.environ.get('PROJECT_DIR'), 'models', 'TotalSegmentator', '.weights')\n",
    "\n",
    "    setup_nnunet()\n",
    "    setup_totalseg()\n",
    "\n",
    "    # We assume that the model we are running will be finetuned with the 'total' task from\n",
    "    # the original TotalSegmentator model because this contains the most information about\n",
    "    # soft organ classification, most of which happens in the abdomen region, which\n",
    "    # intuitively seems like the most logical knowledge to transfer into this task\n",
    "\n",
    "    # From the totalsegmentator.python_api this task ID corresponds with the model trained\n",
    "    # for organ segmentation. Note there is also potential for cropping the image.\n",
    "    task_id = 291\n",
    "    download_pretrained_weights(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Download the weights from TotalSegmentator\n",
    "    fetch_pretrained_totalsegmentator_model()\n",
    "\n",
    "    # Something in the fetch_pretrained_totalsegmentator_model overwrites the global variables\n",
    "    setup_data_vars()\n",
    "    classes, raw_data, gt_labels = get_raw_and_gt_data_paths()\n",
    "\n",
    "    print('[DEBUG]: Obtained the environment variables. These are:')\n",
    "    print(f'nnUNet_raw: {os.environ.get(\"nnUNet_raw\")}')\n",
    "    print(f'nnUNet_preprocessed: {os.environ.get(\"nnUNet_preprocessed\")}')\n",
    "    print(f'nnUNet_results: {os.environ.get(\"nnUNet_results\")}')\n",
    "\n",
    "    # Set the data to pre-train on the fingerprint of the training data.\n",
    "    \n",
    "    \"\"\"\n",
    "    TARGET_DATASET = the one you wish to fine tune on, Radiotherapy data\n",
    "    SOURCE_DATASET = dataset you intend to run the pretraining on, TotalSegmentator\n",
    "\n",
    "    1. nnUNetv2_plan_and_preprocess -d TARGET_DATASET (this has been done already)\n",
    "    2. nnUNetv2_extract_fingerprint -d SOURCE_DATASET (this has been achieved by creating a dummy dataset id into which I copied the .json files obtained from the downloaded model)\n",
    "\n",
    "    Path to the plans.json for TotalSegmentator:\n",
    "    /vol/bitbucket/az620/radiotherapy/models/TotalSegmentator/.weights/nnunet/results/Dataset291_TotalSegmentator_part1_organs_1559subj/nnUNetTrainerNoMirroring__nnUNetPlans__3d_fullres/plans.json\n",
    "\n",
    "    3. Now I need to move the plans from the dummy dataset to the Radiotherapy dataset. Need to do this one at a time for each class\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('dataset', type=int, help='The dataset to fine tune on')\n",
    "    parser.add_argument('fold', type=int, help='The fold to fine tune on')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    assert args.dataset is not None, \"Please provide the dataset to fine tune on\"\n",
    "    assert args.dataset in range(1, len(classes) + 1), \"Please provide a valid dataset to fine tune on\"\n",
    "\n",
    "    assert args.fold is not None, \"Please provide the fold to fine tune on\"\n",
    "    assert args.fold in range(5), \"Please provide a valid fold to fine tune on\"\n",
    "\n",
    "    TARGET_DATASET = args.dataset\n",
    "    PATH_TO_CHECKPOINT = os.path.join(os.environ.get('PROJECT_DIR'), 'models', 'TotalSegmentator', '.weights', 'nnunet/results/Dataset291_TotalSegmentator_part1_organs_1559subj/nnUNetTrainerNoMirroring__nnUNetPlans__3d_fullres/fold_0/checkpoint_final.pth')\n",
    "    FOLD = args.fold\n",
    "    CONFIG = '3d_fullres'\n",
    "\n",
    "    # Run the training on the target dataset\n",
    "\n",
    "    from nnunetv2.run.run_training import run_training_entry\n",
    "\n",
    "    original_sys_argv = sys.argv\n",
    "\n",
    "    print('-----------')\n",
    "    print(f'FOLD: {FOLD}')\n",
    "    print('-----------')\n",
    "\n",
    "    # !nnUNetv2_train TARGET_DATASET CONFIG FOLD -pretrained_weights PATH_TO_CHECKPOINT\n",
    "    sys.argv = [original_sys_argv[0], str(TARGET_DATASET), CONFIG, str(FOLD), '-pretrained_weights', PATH_TO_CHECKPOINT, '-tr', 'nnUNetTrainer_50epochs', '--npz', '-p', 'totseg_nnUNetPlans']\n",
    "    run_training_entry()\n",
    "\n",
    "    sys.argv = original_sys_argv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
