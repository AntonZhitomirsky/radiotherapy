{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of nnUNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a checkpoint of a model for a given class, evaluate the different metrics of the \n",
    "model by performing forward inference on the model and extracting the different\n",
    "evaluation metrics on the data, such as DICE, Haussdorff distance, Jaccard etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.environment import setup_data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_predictor(model_path, fold):\n",
    "\n",
    "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "    import torch\n",
    "\n",
    "    predictor = nnUNetPredictor(\n",
    "            tile_step_size=0.5,\n",
    "            use_gaussian=True,\n",
    "            use_mirroring=True,\n",
    "            perform_everything_on_device=True,\n",
    "            device=device,\n",
    "            verbose=False,\n",
    "            verbose_preprocessing=False,\n",
    "            allow_tqdm=True\n",
    "        )\n",
    "\n",
    "    predictor.initialize_from_trained_model_folder(\n",
    "        os.path.join(os.environ.get('nnUNet_results'), model_path),\n",
    "        use_folds=fold,\n",
    "        checkpoint_name='checkpoint_best.pth',\n",
    "    )\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_file(input_file, output_file, predictor):\n",
    "    \"\"\"\n",
    "    Predict the segmentation of a single file and save it to the output location.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(input_file):\n",
    "        print(f\"{input_file} exists\")\n",
    "    else:\n",
    "        print(f\"{input_file} does not exist\")\n",
    "\n",
    "    predictor.predict_from_files(input_file,\n",
    "                                 output_file,\n",
    "                                 save_probabilities=False, overwrite=False,\n",
    "                                 num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "                                 folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)\n",
    "\n",
    "    # variant 2, use list of files as inputs. Note how we use nested lists!!!\n",
    "    # predictor.predict_from_files([[file_name]],\n",
    "    #                             [output_name],\n",
    "    #                             save_probabilities=False, overwrite=overwrite,\n",
    "    #                             num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "    #                             folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference on inputs: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunetv2/inference/readme.md\n",
    "\n",
    "Running on slurm requires freezing: https://stackoverflow.com/questions/24374288/where-to-put-freeze-support-in-a-python-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising predictor with class Dataset003_CTVn, and folds ('3', '2', '1', '0')\n",
      "predicting from /vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/data/nnUNet_raw/Dataset003_CTVn/imagesTr to /vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/data/nnUNet_inference/Dataset003_CTVn/nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "\n",
    "    setup_data_vars()\n",
    "\n",
    "    classes = [os.environ.get('Anorectum')\n",
    "             , os.environ.get('Bladder')\n",
    "             , os.environ.get('CTVn')\n",
    "             , os.environ.get('CTVp')\n",
    "             , os.environ.get('Parametrium')\n",
    "             , os.environ.get('Uterus')\n",
    "             , os.environ.get('Vagina')]\n",
    "\n",
    "    input_data_path = [os.path.join(os.environ.get('nnUNet_raw'), c, os.environ.get('data_trainingImages')) for c in classes]\n",
    "    output_path = [os.path.join(os.environ.get('nnUNet_inference'), c) for c in classes]\n",
    "    model_paths = [os.path.join(c, 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres') for c in classes]\n",
    "    \n",
    "    # for each model path, get the list of folds that contain checkpoints in them\n",
    "    folds_per_model = [set() for _ in model_paths]\n",
    "\n",
    "    for i,m in enumerate(model_paths):\n",
    "        full_path = os.path.join(os.environ.get('nnUNet_results'), m)\n",
    "        # print(f'searching for folds for {classes[i]},', end=' ')\n",
    "        _, subdirs, _ = next(os.walk(full_path))\n",
    "        for folds in sorted(subdirs):\n",
    "            for checkpoints in os.listdir(os.path.join(full_path, folds)):\n",
    "                if '.pth' in checkpoints:\n",
    "                    folds_per_model[i].add(folds.split('_')[1])\n",
    "        # print(f'found folds {folds_per_model[i]}')\n",
    "    \n",
    "    folds_per_model = [tuple(x) for x in folds_per_model]\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('dataset_id', type=int, help='The dataset id to fine tune on')\n",
    "    \n",
    "    # import sys\n",
    "    # original_args = sys.argv\n",
    "    # sys.argv = [original_args[0], '3']\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_class = classes[args.dataset_id - 1]\n",
    "    model_path = model_paths[args.dataset_id - 1]\n",
    "    fold = folds_per_model[args.dataset_id - 1]\n",
    "    \n",
    "    print(f'initialising predictor with class {input_class}, and folds {fold}')\n",
    "    predictor = initialise_predictor(model_path, fold)\n",
    "\n",
    "    setup_data_vars()\n",
    "\n",
    "    input_location = input_data_path[args.dataset_id - 1]\n",
    "    output_location = os.path.join(output_path[args.dataset_id - 1], 'nnUNetTrainer_500epochs__nnUNetResEncUNetLPlans__3d_fullres')\n",
    "    \n",
    "    print(f'predicting from {input_location} to {output_location}')\n",
    "    predict_file(input_location, output_location, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
