{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of nnUNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a checkpoint of a model for a given class, evaluate the different metrics of the \n",
    "model by performing forward inference on the model and extracting the different\n",
    "evaluation metrics on the data, such as DICE, Haussdorff distance, Jaccard etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "dir1 = os.path.abspath(os.path.join(os.path.abspath(''), '..', '..'))\n",
    "if not dir1 in sys.path: sys.path.append(dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.environment import setup_data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_predictor(model_path, fold):\n",
    "\n",
    "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "    import torch\n",
    "\n",
    "    predictor = nnUNetPredictor(\n",
    "            tile_step_size=0.5,\n",
    "            use_gaussian=True,\n",
    "            use_mirroring=True,\n",
    "            perform_everything_on_device=True,\n",
    "            device=device,\n",
    "            verbose=False,\n",
    "            verbose_preprocessing=False,\n",
    "            allow_tqdm=True\n",
    "        )\n",
    "\n",
    "    predictor.initialize_from_trained_model_folder(\n",
    "        os.path.join(os.environ.get('nnUNet_results'), model_path),\n",
    "        use_folds=fold,\n",
    "        checkpoint_name='checkpoint_best.pth',\n",
    "    )\n",
    "\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_file(input_file, output_file, predictor):\n",
    "    \"\"\"\n",
    "    Predict the segmentation of a single file and save it to the output location.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(input_file):\n",
    "        print(f\"{input_file} exists\")\n",
    "    else:\n",
    "        print(f\"{input_file} does not exist\")\n",
    "\n",
    "    predictor.predict_from_files(input_file,\n",
    "                                 output_file,\n",
    "                                 save_probabilities=False, overwrite=False,\n",
    "                                 num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "                                 folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)\n",
    "\n",
    "    # variant 2, use list of files as inputs. Note how we use nested lists!!!\n",
    "    # predictor.predict_from_files([[file_name]],\n",
    "    #                             [output_name],\n",
    "    #                             save_probabilities=False, overwrite=overwrite,\n",
    "    #                             num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "    #                             folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference on inputs: https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunetv2/inference/readme.md\n",
    "\n",
    "Running on slurm requires freezing: https://stackoverflow.com/questions/24374288/where-to-put-freeze-support-in-a-python-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising predictor with class Dataset008_TotalBinary, and folds ('0', '1')\n",
      "predicting from /vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/data/nnUNet_raw/Dataset008_TotalBinary/imagesTr to /vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/data/nnUNet_inference/Dataset008_TotalBinary/nnUNetTrainer_500epochs__nnUNetPlans__3d_fullres\n",
      "/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/data/nnUNet_raw/Dataset008_TotalBinary/imagesTr exists\n",
      "There are 100 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 100 cases that I would like to predict\n",
      "overwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 100 cases.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m setup_data_vars()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicting from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mpredict_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mpredict_file\u001b[0;34m(input_file, output_file, predictor)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                             \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                             \u001b[49m\u001b[43msave_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mnum_processes_preprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes_segmentation_export\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mfolder_with_segs_from_prev_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/models/nnUNet/nnunetv2/inference/predict_from_raw_data.py:257\u001b[0m, in \u001b[0;36mnnUNetPredictor.predict_from_files\u001b[0;34m(self, list_of_lists_or_source_folder, output_folder_or_list_of_truncated_output_files, save_probabilities, overwrite, num_processes_preprocessing, num_processes_segmentation_export, folder_with_segs_from_prev_stage, num_parts, part_id)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    252\u001b[0m data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_get_data_iterator_from_lists_of_filenames(list_of_lists_or_source_folder,\n\u001b[1;32m    253\u001b[0m                                                                          seg_from_prev_stage_files,\n\u001b[1;32m    254\u001b[0m                                                                          output_filename_truncated,\n\u001b[1;32m    255\u001b[0m                                                                          num_processes_preprocessing)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_data_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes_segmentation_export\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/models/nnUNet/nnunetv2/inference/predict_from_raw_data.py:350\u001b[0m, in \u001b[0;36mnnUNetPredictor.predict_from_data_iterator\u001b[0;34m(self, data_iterator, save_probabilities, num_processes_segmentation_export)\u001b[0m\n\u001b[1;32m    348\u001b[0m worker_list \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m export_pool\u001b[38;5;241m.\u001b[39m_pool]\n\u001b[1;32m    349\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m preprocessed \u001b[38;5;129;01min\u001b[39;00m data_iterator:\n\u001b[1;32m    351\u001b[0m     data \u001b[38;5;241m=\u001b[39m preprocessed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/az620/radiotherapy/models/nnUNet/nnunetv2/inference/data_iterators.py:114\u001b[0m, in \u001b[0;36mpreprocessing_iterator_fromfiles\u001b[0;34m(list_of_lists, list_of_segs_from_prev_stage_files, output_filenames_truncated, plans_manager, dataset_json, configuration_manager, num_processes, pin_memory, verbose)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_ok:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBackground workers died. Look for the error message further up! If there is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    112\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone then your RAM was full and the worker was killed by the OS. Use fewer \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    113\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkers or get more RAM in that case!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pin_memory:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import argparse\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "\n",
    "    setup_data_vars()\n",
    "\n",
    "    class_of_interest = os.environ.get('TotalBinary')\n",
    "    input_data_path = os.path.join(os.environ.get('nnUNet_raw'), class_of_interest, os.environ.get('data_trainingImages'))\n",
    "    output_path = os.path.join(os.environ.get('nnUNet_inference'), class_of_interest, 'nnUNetTrainer_500epochs__nnUNetPlans__3d_fullres')\n",
    "    model_path = os.path.join(class_of_interest, 'nnUNetTrainer_500epochs__nnUNetPlans__3d_fullres')\n",
    "\n",
    "    folds_for_model = set()\n",
    "    full_path = os.path.join(os.environ.get('nnUNet_results'), model_path)\n",
    "    _, subdirs, _ = next(os.walk(full_path))\n",
    "    for folds in sorted(subdirs):\n",
    "        for checkpoints in os.listdir(os.path.join(full_path, folds)):\n",
    "            if '.pth' in checkpoints:\n",
    "                folds_for_model.add(folds.split('_')[1])\n",
    "\n",
    "    fold = tuple(folds_for_model)\n",
    "    \n",
    "    print(f'initialising predictor with class {class_of_interest}, and folds {fold}')\n",
    "    predictor = initialise_predictor(model_path, fold)\n",
    "\n",
    "    setup_data_vars()\n",
    "    \n",
    "    print(f'predicting from {input_data_path} to {output_path}')\n",
    "    predict_file(input_data_path, output_path, predictor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
