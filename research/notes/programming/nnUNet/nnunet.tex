\documentclass[11pt]{article}

% some definitions for the title page
\newcommand{\reporttitle}{What is an nnU-Net}
\newcommand{\reportdescription}{This is a sumamry of the contents of the file~\cite{nnunet} and example of its implementation (from the github~\cite{nnunet-git-paper}).}

% load some definitions and default packages
\input{../../.latex-templates/includes}
\input{../../.latex-templates/notation}

\begin{document}

% Include the title page
\input{../../.latex-templates/titlepage}

\tableofcontents

\clearpage

\section{From~\cite{nnunet-git-paper}}

\begin{itemize}
    \item ``Especially in three-dimensional (3D) biomedical imaging, for which dataset properties such as imaging modality, image size, (anisotropic) voxel spacing and class ratio vary drastically, this process can be cumbersome and successful configurations from one dataset rarely translate to another''.
\end{itemize}

\subsection{Recipe for getting parameters}

Previously, applications have been influenced by ``expert-driven configurations''. nnU-Net is a ``primarily data-driven AutoML approache''. Specifically, the paper outlines a recipe that systematizes the configuration process on a task-agnostic level and drastically reduces the search space for empirical design choices when given a new task.

These are quoted below from the paper verbatim

\begin{enumerate}
    \item Collect design decisions that do not require adaptation between datasets and identify a robust common configuration ('fixed parameters').
    \item For as many of the remaining decisions as possible, formulate explicit dependencies between specific dataset properties ('dataset fingerprint') and design choices ('pipeline fingerprint') in the form of heuristic rules to allow for almost-instant adaptation on application ('rule-based parameters').
    \item Learn only the remaining decisions empirically from the data ('empirical parameters').
\end{enumerate}

It ``automatically configures itself, including preprocessing, network architecture, training and post-processing for any new task in the biomedical domain''.

We have a three step recipe to follow to allow for no intervention from the user when it comes to defining custom parameters based on the structure the u-net is analysing:

\begin{figure}[H]
    \includegraphics[width=\linewidth]{images/nnunet-recipe.png}
    \caption{Parameter choices and descriptions}
\end{figure}

\subsection{Fixed Parameters}

During development of nnU-Net we identified a robust configuration (that is, certain architecture and training properties) that can simply be used all the time. This includes, for example, nnU-Net's loss function, (most of the) data augmentation strategy and learning rate.

\begin{itemize}
    \item Architectural template: closely follows U-Net, uses Leaky ReLU instead of ReLU.
    \item Training schedule: 1000 epochs with 250 mini-batches
    \item Inference: sliding window
\end{itemize}

\subsection{Rule-based Parameters}

Uses the dataset fingerprint to adapt certain segmentation pipeline properties by following hard-coded heuristic rules. For example, the network topology (pooling behavior and depth of the network architecture) are adapted to the patch size; the patch size, network topology and batch size are optimized jointly given some GPU memory constraint.

\begin{itemize}
    \item intensity normalisation
    \item Resampling
    \item target spacing
    \item Adaptation of network topology, patch size and batch size.
    \item Initialization
    \item Architectural topology
    \item Adaptation to GPU memory budget.
    \item Batch size
    \item Configuration of the 3D U-Net cascade.
\end{itemize}

\subsection{Empirical Parameters}

trial-and-error. For example the selection of the best U-net configuration for the given dataset (2D, 3D full resolution, 3D low resolution, 3D cascade) and the optimization of the postprocessing strategy.

\begin{itemize}
    \item Ensembling and selection of U-Net configuration(s)
    \item Post-processing
\end{itemize}

\subsection{Data Fingerprint}

\begin{itemize}
    \item As a first processing step, nnU-Net crops the provided training cases to their non-zero region (improved computational efficiency)
    \item captures fingerprint like: \begin{itemize}
        \item image size (before and after cropping)
        \item image spacing (phyiscal size of voxels)
        \item modalities (x-ray, ultrasound, ct, etc...)
        \item as well as mean and std of all voxels
    \end{itemize}
\end{itemize}

\section{Summary of~\cite{nnunet}}

Similarly to the original U-Net architecture they don't change much, but since they are now dealing with 3D data, they consider a pool of basic U-Net architectures: 2D U-Net (\ref{sec:2d-u-net}), a 3D U-Net (\ref{sec:3d-u-net}) and a U-Net Cascade (\ref{sec:u-net-cascade}).

\subsection{2D U-Net}\label{sec:2d-u-net}

Contrary to belief that using a 2D network in the context of a 3D network may be counter-intuitive, the paper found that 3D segmentation methods deteriorate in performance if the dataset is anisotropic~\cite{nnunet} (when the property varies according to direction~\cite{anisotropy}).

\subsection{3D U-Net}\label{sec:3d-u-net}

The architecture would typically thrive on contextual information, but we are limited by GPU memory, which allows us to train this architecture only on image patches. This may pose a problem for generating PTV areas because of the large structures, which, in size, may be compared to a Liver which has been cited as `large images such as Liver, may impede training'~\cite{nnunet}.

\subsection{U-Net Cascade}\label{sec:u-net-cascade}

While the 2D and 3D U-Nets generate segmentations at full resolution, the cascade first generates low resolution segmentations and subsequently refines them.

This addresses the shortcoming in Section~\ref{sec:3d-u-net} because we first train the 3D U-Net on downsampled images (which compresses the amount of contextual information you need) (stage 1) and then upsamples to the original voxel spacing and passed as additional (one hot encoded) input channels to a second 3D U-Net, which is trained on patches at full resolution (stage 2).

\begin{figure}[H]
    \includegraphics[width=\linewidth]{images/nnunet-diagram.png}
\end{figure}

\printbibliography

\end{document}